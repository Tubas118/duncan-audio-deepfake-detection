{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.configuration import RunDetails\n",
    "\n",
    "runDetail = RunDetails('config.yml', 'GitLab-training-data')\n",
    "# runDetail = RunDetails('config.yml', 'ASVspoof-2019_training')\n",
    "# runDetail = RunDetails('config.yml', 'ASVspoof-2019_training_epoch-100')\n",
    "\n",
    "notebookName = 'audio-deepfake-detection-training'\n",
    "plot_title_suffix = \"(Training)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFilename = runDetail.configFilename\n",
    "runJobId = runDetail.jobId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.configuration as configuration\n",
    "import model_definitions.model_cnn_definition as model_cnn_definition\n",
    "from postprocessors.plot_confusion_matrix import PlotConfusionMatrix\n",
    "from postprocessors.plot_roc_curve import PlotRocCurve\n",
    "from preprocessors.abstract_preprocessor import AbstractPreprocessor\n",
    "from preprocessors.preprocessor_factory import PreprocessorFactory\n",
    "from notebook_utils import notebookToPython\n",
    "from processors.basic_model_training_processor import BasicModelTrainingProcessor\n",
    "from processors.basic_model_evaluation_processor import BasicModelEvaluationProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write python file\n",
      "Generating new model name: output/GitLab-training-data_2025-04-17T12-57-44.517459.libjob\n",
      "Assigned model name: output/GitLab-training-data_2025-04-17T12-57-44.517459.libjob\n"
     ]
    }
   ],
   "source": [
    "config = configuration.ConfigLoader(configFilename)\n",
    "\n",
    "notebookToPython(notebookName)\n",
    "job = config.getJobConfig(runJobId)\n",
    "\n",
    "if (job.newModelGenerated == False):\n",
    "    raise ValueError(\"This notebook is meant for training. Select a job without a value for 'persisted-model' set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MelSpectrogramPreprocessor\n"
     ]
    }
   ],
   "source": [
    "preproc_factory = PreprocessorFactory()\n",
    "preprocessor: AbstractPreprocessor = preproc_factory.newPreprocessor(job.preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./testvalues/sksmta.train.trn.txt...\n",
      "fullDataPath: ./testaudio-sksmta\n",
      "Loading audio files: 1\n",
      "Loading audio files: 2\n",
      "Loading audio files: 3\n",
      "Loading audio files: 4\n",
      "Loading audio files: 5\n",
      "Loading audio files: 6\n",
      "Loading audio files: 7\n",
      "Loading audio files: 8\n",
      "Loading audio files: 9\n",
      "Loading audio files: 10\n",
      "Number of audio files load: 10\n"
     ]
    }
   ],
   "source": [
    "X, y_encoded = preprocessor.extract_features_multipleSource(job, job.dataPathSuffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from config.configuration import Job\n",
    "from model_definitions.model_abstract_definition import ModelAbstractDefinition\n",
    "from processors.abstract_model_training_processor import AbstractModelTrainingProcessor\n",
    "\n",
    "# =============================================================================\n",
    "class BasicModelTrainingProcessor2(AbstractModelTrainingProcessor):\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def __init__(self, job: Job, modelDefType):\n",
    "        super().__init__(job)\n",
    "        self.resetStatistics()\n",
    "        self.modelDefType = modelDefType\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def resetStatistics(self):\n",
    "        self.jobStartTime = None\n",
    "        self.inputFileBatchCount = 0\n",
    "        self.inputFileCount = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def process(self, X, y_encoded, channels, test_size = 0.2, trainingSplitRandomState: int = None, \n",
    "                        scoring = 'accuracy'):\n",
    "        \n",
    "        if (self.jobStartTime == None):\n",
    "            self.jobStartTime = datetime.now(pytz.utc)\n",
    "\n",
    "        useTrainingSplitRandomState: int = self.__get_training_split_random_state__(trainingSplitRandomState)\n",
    "\n",
    "        print(f\"Selecting training and test data - traininSplitRandomState: {useTrainingSplitRandomState}\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=test_size, random_state=useTrainingSplitRandomState)\n",
    "        \n",
    "        print(f\"Training using {len(X_train)} files.\")\n",
    "        model = self.__train_model__(X_train, X_test, y_train, y_test, channels, scoring)\n",
    "\n",
    "        return model, X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    def reportSnapshot(self):\n",
    "        timestamp_utc = datetime.now(pytz.utc)\n",
    "        elapsed_time = timestamp_utc - self.jobStartTime\n",
    "        prettyJson = json.dumps(self.__job__.__dict__, indent=4)\n",
    "\n",
    "        report = f\"---- Training (start) ----\\n\"\n",
    "        report = report + f\"start time: {self.jobStartTime.isoformat()}\\n\"\n",
    "        report = report + f\"end time: {timestamp_utc.isoformat()}\\n\"\n",
    "        report = report + f\"elapsed: {elapsed_time}\\n\\n\"\n",
    "        report = report + f\"model file: {self.__job__.persistedModel}\\n\"\n",
    "        report = report + f\"batch count: {self.inputFileBatchCount}\\n\"\n",
    "        report = report + f\"file count: {self.inputFileCount}\\n\"\n",
    "        report = report + f\"job: {prettyJson}\\n\\n\"\n",
    "        report = report + f\"---- Training (end) ----\\n\"\n",
    "\n",
    "        return report\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def __train_model__(self, X_train, X_test, y_train, y_test, channels, scoring) -> Model:\n",
    "        \n",
    "        modelDef: ModelAbstractDefinition = self.modelDefType(self.__job__, X_train.shape[2], channels)\n",
    "\n",
    "        model = modelDef.buildModel()\n",
    "        model.compile(optimizer=self.__job__.optimizer, loss=self.__job__.loss, metrics=self.__job__.metrics)\n",
    "\n",
    "        print(\"Training the Model...\")\n",
    "        model.fit(X_train, y_train, batch_size=self.__job__.batchSize, epochs=self.__job__.numEpochs, validation_data=(X_test, y_test))\n",
    "\n",
    "        print(f\"Saving model: {self.__job__.persistedModel}\")\n",
    "        joblib.dump(model, self.__job__.persistedModel)\n",
    "\n",
    "        # scores = cross_val_score(model, X_train, y_train, cv=self.__job__.cv, scoring=scoring)\n",
    "        # print(f\"cross validation scores: {scores}\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting training and test data - traininSplitRandomState: 3\n",
      "Training using 8 files.\n",
      "__job__: <config.configuration.Job object at 0x00000245FACE3BC0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6250 - loss: 14.8835 - val_accuracy: 0.5000 - val_loss: 35.4379\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.3750 - loss: 92.3701 - val_accuracy: 0.5000 - val_loss: 197.0585\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6250 - loss: 20.9505 - val_accuracy: 0.5000 - val_loss: 207.0571\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7500 - loss: 164.2165 - val_accuracy: 0.5000 - val_loss: 87.4394\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6250 - loss: 62.6134 - val_accuracy: 0.5000 - val_loss: 14.3263\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.3750 - loss: 81.8597 - val_accuracy: 1.0000 - val_loss: 7.9274e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7500 - loss: 20.8784 - val_accuracy: 0.5000 - val_loss: 4.1736\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.6250 - loss: 100.9218 - val_accuracy: 0.5000 - val_loss: 45.8263\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5000 - loss: 50.6334 - val_accuracy: 0.5000 - val_loss: 100.0372\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7500 - loss: 17.2175 - val_accuracy: 0.5000 - val_loss: 110.1993\n",
      "Saving model: output/GitLab-training-data_2025-04-17T12-57-44.517459.libjob\n"
     ]
    }
   ],
   "source": [
    "trainingProc = BasicModelTrainingProcessor2(job, model_cnn_definition.ModelCnnDefinition)\n",
    "model, X_train, X_test, y_train, y_test = trainingProc.process(X, y_encoded, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      3\u001b[0m kerasModel \u001b[38;5;241m=\u001b[39m KerasClassifier(build_fn\u001b[38;5;241m=\u001b[39mmodel, batch_size\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39mbatchSize, epochs\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39mnumEpochs,\n\u001b[0;32m      4\u001b[0m                              optimizer\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m----> 6\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(kerasModel, X_train, y_train, cv\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39mcv)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross validation scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tubas\\anaconda3\\envs\\audio-deepfake-detection\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tubas\\anaconda3\\envs\\audio-deepfake-detection\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    685\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    686\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    687\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    688\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    689\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    690\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    691\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    692\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    693\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    694\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    695\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    696\u001b[0m )\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tubas\\anaconda3\\envs\\audio-deepfake-detection\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tubas\\anaconda3\\envs\\audio-deepfake-detection\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:347\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m indexable(X, y)\n\u001b[0;32m    346\u001b[0m params \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m params\n\u001b[1;32m--> 347\u001b[0m cv \u001b[38;5;241m=\u001b[39m check_cv(cv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    349\u001b[0m scorers \u001b[38;5;241m=\u001b[39m check_scoring(\n\u001b[0;32m    350\u001b[0m     estimator, scoring\u001b[38;5;241m=\u001b[39mscoring, raise_exc\u001b[38;5;241m=\u001b[39m(error_score \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    351\u001b[0m )\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;66;03m# For estimators, a MetadataRouter is created in get_metadata_routing\u001b[39;00m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;66;03m# methods. For these router methods, we create the router to use\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# `process_routing` on it.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tubas\\anaconda3\\envs\\audio-deepfake-detection\\Lib\\site-packages\\sklearn\\base.py:1237\u001b[0m, in \u001b[0;36mis_classifier\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mstack()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1234\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_estimator_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_tags(estimator)\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tubas\\anaconda3\\envs\\audio-deepfake-detection\\Lib\\site-packages\\sklearn\\utils\\_tags.py:430\u001b[0m, in \u001b[0;36mget_tags\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39mmro()):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[1;32m--> 430\u001b[0m         sklearn_tags_provider[klass] \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m__sklearn_tags__(estimator)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    431\u001b[0m         class_order\u001b[38;5;241m.\u001b[39mappend(klass)\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "File \u001b[1;32mc:\\Users\\tubas\\anaconda3\\envs\\audio-deepfake-detection\\Lib\\site-packages\\sklearn\\base.py:540\u001b[0m, in \u001b[0;36mClassifierMixin.__sklearn_tags__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__sklearn_tags__()\n\u001b[0;32m    541\u001b[0m     tags\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m     tags\u001b[38;5;241m.\u001b[39mclassifier_tags \u001b[38;5;241m=\u001b[39m ClassifierTags()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "kerasModel = KerasClassifier(build_fn=model, batch_size=job.batchSize, epochs=job.numEpochs,\n",
    "                             optimizer=job.optimizer, loss=job.loss, metrics=job.metrics)\n",
    "\n",
    "scores = cross_val_score(kerasModel, X_train, y_train, cv=job.cv)\n",
    "print(f\"cross validation scores: {scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluationProc = BasicModelEvaluationProcessor(job, model)\n",
    "results = evaluationProc.process(X_test, y_test)\n",
    "print(f\"{results.reportSnaphot()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_TITLE = f\"{PlotConfusionMatrix.DEFAULT_TITLE} {plot_title_suffix}\"\n",
    "cm_plot = PlotConfusionMatrix()\n",
    "cm_plot.plotFromResults(results, job, CM_TITLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC_TITLE = f\"{PlotRocCurve.DEFAULT_TITLE} {plot_title_suffix}\"\n",
    "roc_plot = PlotRocCurve()\n",
    "roc_plot.plotFromResults(results, RC_TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "report = evaluationProc.reportSnapshot(trainingProc)\n",
    "evaluationProc.writeReportToFile(job.persistedModelResults, report)\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-deepfake-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
