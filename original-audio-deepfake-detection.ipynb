{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookName = 'original-audio-deepfake-detection'\n",
    "runJobId = 'ASVspoof-2019_training'\n",
    "random_state = 186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import configuration.configuration as configuration\n",
    "from configuration.configuration import Job\n",
    "from notebook_utils import notebookToPython\n",
    "from readers.label_reader import readTrainingLabelsWithJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write python file\n",
      "Generating new model name: output/ASVspoof-2019_training_2025-03-27T20-07-45.848482.libjob\n",
      "Assigned model name: output/ASVspoof-2019_training_2025-03-27T20-07-45.848482.libjob\n"
     ]
    }
   ],
   "source": [
    "config = configuration.ConfigLoader('config.yml')\n",
    "\n",
    "notebookToPython(notebookName)\n",
    "job: Job = config.getJobConfig(runJobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:/Users/tubas/workspace/Deepfake/data/ASVspoof-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt...\n"
     ]
    }
   ],
   "source": [
    "trainingLabels = readTrainingLabelsWithJob(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "fullDataPath = job.fullJoinFilePath(job.dataPathRoot, job.dataPathSuffix)\n",
    "\n",
    "for filename, label in trainingLabels.items():\n",
    "    audioSourceFilename = job.fullJoinFilePath(fullDataPath, filename + job.dataExtension)\n",
    "    \n",
    "    audio, _ = librosa.load(audioSourceFilename, sr = job.sampleRate, duration = job.duration)\n",
    "\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y = audio, sr = job.sampleRate, n_mels = job.numMels)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    if (mel_spectrogram.shape[1] < job.maxTimeSteps):\n",
    "        padWidth = ((0, 0), (0, job.maxTimeSteps - mel_spectrogram.shape[1]))\n",
    "        mel_spectrogram = np.pad(array=mel_spectrogram, pad_width=padWidth, mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :job.maxTimeSteps]\n",
    "\n",
    "    X.append(mel_spectrogram)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y_encoded = to_categorical(y, job.numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)    # test data is 20% of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model architecture\n",
    "input_shape = (job.numMels, X_train.shape[2], 1)  # Input shape for CNN (height, width, channels)\n",
    "model_input = Input(shape=input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - why were these parameters selected? What purpose do they serve? Should they be configurable?\n",
    "x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "model_output = Dense(job.numClasses, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=job.optimizer, loss=job.loss, metrics=job.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.8859 - loss: 4.7077 - val_accuracy: 0.8983 - val_loss: 0.2594\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9203 - loss: 0.1870 - val_accuracy: 0.9060 - val_loss: 0.3390\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9424 - loss: 0.1407 - val_accuracy: 0.9817 - val_loss: 0.0483\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9748 - loss: 0.0657 - val_accuracy: 0.9819 - val_loss: 0.0497\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9723 - loss: 0.0735 - val_accuracy: 0.9641 - val_loss: 0.0907\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9661 - loss: 0.0832 - val_accuracy: 0.9787 - val_loss: 0.0572\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9778 - loss: 0.0575 - val_accuracy: 0.9874 - val_loss: 0.0351\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9836 - loss: 0.0450 - val_accuracy: 0.9821 - val_loss: 0.0467\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9795 - loss: 0.0550 - val_accuracy: 0.9872 - val_loss: 0.0355\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9804 - loss: 0.0506 - val_accuracy: 0.9909 - val_loss: 0.0283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a70766ed80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.fit(X_train, y_train, batch_size=job.batchSize, epochs=job.numEpochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/ASVspoof-2019_training_2025-03-27T20-07-45.848482.libjob']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, job.persistedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909377462568952"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = accuracy_score(y_test, y_pred)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-deepfake-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
