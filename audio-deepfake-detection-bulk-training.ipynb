{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.configuration import RunDetails\n",
    "\n",
    "# runDetail = RunDetails('config.yml', 'ASVspoof-2019_training')\n",
    "runDetail = RunDetails('config-mfcc.yml', 'ASVspoof-2019_training_mfcc')\n",
    "\n",
    "notebookName = 'audio-deepfake-detection-bulk-training'\n",
    "random_state_lowValue = 1\n",
    "random_state_highValue = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFilename = runDetail.configFilename\n",
    "runJobId = runDetail.jobId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.configuration as configuration\n",
    "import model_definitions.model_cnn_definition as model_cnn_definition\n",
    "from preprocessors.preprocessor_factory import PreprocessorFactory\n",
    "from notebook_utils import notebookToPython\n",
    "from processors.basic_model_training_processor import BasicModelTrainingProcessor\n",
    "from processors.basic_model_evaluation_processor import BasicModelEvaluationProcessor\n",
    "from processors.bulk_model_training_processor import BulkModelTrainingProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write python file\n",
      "Generating new model name: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353.libjob\n",
      "Assigned model name: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353.libjob\n"
     ]
    }
   ],
   "source": [
    "config = configuration.ConfigLoader(configFilename)\n",
    "\n",
    "notebookToPython(notebookName)\n",
    "job = config.getJobConfig(runJobId)\n",
    "\n",
    "if (job.newModelGenerated == False):\n",
    "    raise ValueError(\"This notebook is meant for training. Select a job without a value for 'persisted-model' set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MelFrequencyCepstralCoeffiecient\n"
     ]
    }
   ],
   "source": [
    "preproc_factory = PreprocessorFactory()\n",
    "preprocessor = preproc_factory.newPreprocessor(job.preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:/Users/tubas/workspace/Deepfake/data/ASVspoof-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt...\n",
      "fullDataPath: C:/Users/tubas/workspace/Deepfake/data/ASVspoof-2019/LA/ASVspoof2019_LA_train/flac\n",
      "Loading audio files: 1269\n",
      "Loading audio files: 2538\n",
      "Loading audio files: 3807\n",
      "Loading audio files: 5076\n",
      "Loading audio files: 6345\n",
      "Loading audio files: 7614\n",
      "Loading audio files: 8883\n",
      "Loading audio files: 10152\n",
      "Loading audio files: 11421\n",
      "Loading audio files: 12690\n",
      "Loading audio files: 13959\n",
      "Loading audio files: 15228\n",
      "Loading audio files: 16497\n",
      "Loading audio files: 17766\n",
      "Loading audio files: 19035\n",
      "Loading audio files: 20304\n",
      "Loading audio files: 21573\n",
      "Loading audio files: 22842\n",
      "Loading audio files: 24111\n",
      "Loading audio files: 25380\n",
      "Number of audio files load: 25380\n"
     ]
    }
   ],
   "source": [
    "X, y_encoded = preprocessor.extract_features_multipleSource(job, job.dataPathSuffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting training and test data - traininSplitRandomState: 1\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A68F942480>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - accuracy: 0.9160 - loss: 0.7632 - val_accuracy: 0.9856 - val_loss: 0.0540\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - accuracy: 0.9797 - loss: 0.0572 - val_accuracy: 0.9870 - val_loss: 0.0342\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9907 - loss: 0.0301 - val_accuracy: 0.9957 - val_loss: 0.0128\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - accuracy: 0.9940 - loss: 0.0172 - val_accuracy: 0.9970 - val_loss: 0.0100\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - accuracy: 0.9934 - loss: 0.0200 - val_accuracy: 0.9965 - val_loss: 0.0112\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - accuracy: 0.9930 - loss: 0.0191 - val_accuracy: 0.9974 - val_loss: 0.0070\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9961 - loss: 0.0102 - val_accuracy: 0.9974 - val_loss: 0.0109\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9969 - loss: 0.0098 - val_accuracy: 0.9970 - val_loss: 0.0100\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 0.9970 - val_loss: 0.0122\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9963 - loss: 0.0110 - val_accuracy: 0.9909 - val_loss: 0.0309\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000001.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9909377462568952 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 2\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A68F8E5BE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.9110 - loss: 1.0281 - val_accuracy: 0.9785 - val_loss: 0.0557\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9797 - loss: 0.0563 - val_accuracy: 0.9931 - val_loss: 0.0248\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9921 - loss: 0.0244 - val_accuracy: 0.9949 - val_loss: 0.0164\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9941 - loss: 0.0163 - val_accuracy: 0.9921 - val_loss: 0.0218\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9945 - loss: 0.0166 - val_accuracy: 0.9937 - val_loss: 0.0217\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9950 - loss: 0.0155 - val_accuracy: 0.9957 - val_loss: 0.0162\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9966 - loss: 0.0108 - val_accuracy: 0.9957 - val_loss: 0.0142\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9965 - loss: 0.0095 - val_accuracy: 0.9911 - val_loss: 0.0293\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9948 - loss: 0.0157 - val_accuracy: 0.9957 - val_loss: 0.0158\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9940 - loss: 0.0283 - val_accuracy: 0.9872 - val_loss: 0.0390\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000002.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9871946414499606 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 3\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79B930260>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9113 - loss: 1.7962 - val_accuracy: 0.9783 - val_loss: 0.0578\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9825 - loss: 0.0481 - val_accuracy: 0.9927 - val_loss: 0.0269\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9923 - loss: 0.0246 - val_accuracy: 0.9882 - val_loss: 0.0336\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9941 - loss: 0.0182 - val_accuracy: 0.9870 - val_loss: 0.0438\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9927 - loss: 0.0220 - val_accuracy: 0.9941 - val_loss: 0.0171\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9962 - loss: 0.0113 - val_accuracy: 0.9947 - val_loss: 0.0163\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9963 - loss: 0.0088 - val_accuracy: 0.9921 - val_loss: 0.0249\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9967 - loss: 0.0098 - val_accuracy: 0.9970 - val_loss: 0.0156\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9969 - loss: 0.0103 - val_accuracy: 0.9959 - val_loss: 0.0202\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9980 - loss: 0.0061 - val_accuracy: 0.9917 - val_loss: 0.0252\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000003.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.991725768321513 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 4\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A6D793F0B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9086 - loss: 1.8541 - val_accuracy: 0.9811 - val_loss: 0.0646\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9796 - loss: 0.0573 - val_accuracy: 0.9900 - val_loss: 0.0305\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9911 - loss: 0.0267 - val_accuracy: 0.9937 - val_loss: 0.0228\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9933 - loss: 0.0204 - val_accuracy: 0.9937 - val_loss: 0.0232\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9961 - loss: 0.0156 - val_accuracy: 0.9923 - val_loss: 0.0179\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9920 - loss: 0.0207 - val_accuracy: 0.9941 - val_loss: 0.0158\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9948 - loss: 0.0139 - val_accuracy: 0.9951 - val_loss: 0.0142\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9916 - loss: 0.0225 - val_accuracy: 0.9949 - val_loss: 0.0185\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9961 - loss: 0.0101 - val_accuracy: 0.9963 - val_loss: 0.0114\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9963 - loss: 0.0108 - val_accuracy: 0.9949 - val_loss: 0.0173\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000004.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9948778565799843 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 5\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A6F6375DC0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9111 - loss: 1.2550 - val_accuracy: 0.9584 - val_loss: 0.0926\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9747 - loss: 0.0690 - val_accuracy: 0.9925 - val_loss: 0.0257\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9885 - loss: 0.0326 - val_accuracy: 0.9931 - val_loss: 0.0206\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9927 - loss: 0.0247 - val_accuracy: 0.9949 - val_loss: 0.0190\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9937 - loss: 0.0175 - val_accuracy: 0.9951 - val_loss: 0.0151\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9960 - loss: 0.0117 - val_accuracy: 0.9894 - val_loss: 0.0396\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9925 - loss: 0.0193 - val_accuracy: 0.9963 - val_loss: 0.0188\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9946 - loss: 0.0152 - val_accuracy: 0.9943 - val_loss: 0.0335\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9957 - loss: 0.0119 - val_accuracy: 0.9957 - val_loss: 0.0186\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9940 - loss: 0.0172 - val_accuracy: 0.9974 - val_loss: 0.0152\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000005.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9974389282899921 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 6\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A68F92FAA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.8976 - loss: 3.8809 - val_accuracy: 0.9756 - val_loss: 0.0687\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9673 - loss: 0.0826 - val_accuracy: 0.9898 - val_loss: 0.0374\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9829 - loss: 0.0477 - val_accuracy: 0.9901 - val_loss: 0.0257\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9886 - loss: 0.0347 - val_accuracy: 0.9947 - val_loss: 0.0135\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9916 - loss: 0.0253 - val_accuracy: 0.9941 - val_loss: 0.0188\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9925 - loss: 0.0192 - val_accuracy: 0.9970 - val_loss: 0.0076\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9951 - loss: 0.0149 - val_accuracy: 0.9961 - val_loss: 0.0103\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9934 - loss: 0.0189 - val_accuracy: 0.9901 - val_loss: 0.0253\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9957 - loss: 0.0132 - val_accuracy: 0.9943 - val_loss: 0.0152\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9962 - loss: 0.0117 - val_accuracy: 0.9955 - val_loss: 0.0149\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000006.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9954688731284476 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 7\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79CFA9F70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9150 - loss: 1.3277 - val_accuracy: 0.9848 - val_loss: 0.0407\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9803 - loss: 0.0529 - val_accuracy: 0.9939 - val_loss: 0.0197\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9904 - loss: 0.0307 - val_accuracy: 0.9921 - val_loss: 0.0213\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9932 - loss: 0.0190 - val_accuracy: 0.9957 - val_loss: 0.0152\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9949 - loss: 0.0152 - val_accuracy: 0.9909 - val_loss: 0.0313\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9954 - loss: 0.0158 - val_accuracy: 0.9953 - val_loss: 0.0220\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9950 - loss: 0.0176 - val_accuracy: 0.9978 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9969 - loss: 0.0079 - val_accuracy: 0.9961 - val_loss: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9969 - loss: 0.0088 - val_accuracy: 0.9955 - val_loss: 0.0153\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9975 - loss: 0.0080 - val_accuracy: 0.9923 - val_loss: 0.0275\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000007.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9923167848699763 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 8\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A68F941430>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8963 - loss: 2.5050 - val_accuracy: 0.9608 - val_loss: 0.1059\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9619 - loss: 0.1035 - val_accuracy: 0.9738 - val_loss: 0.0699\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9771 - loss: 0.0672 - val_accuracy: 0.9900 - val_loss: 0.0401\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9831 - loss: 0.0529 - val_accuracy: 0.9888 - val_loss: 0.0391\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9906 - loss: 0.0362 - val_accuracy: 0.9943 - val_loss: 0.0204\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 68ms/step - accuracy: 0.9903 - loss: 0.0309 - val_accuracy: 0.9896 - val_loss: 0.0345\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9893 - loss: 0.0309 - val_accuracy: 0.9939 - val_loss: 0.0200\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9946 - loss: 0.0196 - val_accuracy: 0.9927 - val_loss: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9929 - loss: 0.0235 - val_accuracy: 0.9937 - val_loss: 0.0331\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9917 - loss: 0.0226 - val_accuracy: 0.9945 - val_loss: 0.0243\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000008.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 9\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A68E803E60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8949 - loss: 2.3790 - val_accuracy: 0.9748 - val_loss: 0.0763\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9704 - loss: 0.0754 - val_accuracy: 0.9921 - val_loss: 0.0317\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9856 - loss: 0.0443 - val_accuracy: 0.9949 - val_loss: 0.0176\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9924 - loss: 0.0269 - val_accuracy: 0.9965 - val_loss: 0.0142\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9943 - loss: 0.0171 - val_accuracy: 0.9959 - val_loss: 0.0113\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9929 - loss: 0.0231 - val_accuracy: 0.9937 - val_loss: 0.0227\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9960 - loss: 0.0138 - val_accuracy: 0.9955 - val_loss: 0.0133\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9953 - loss: 0.0136 - val_accuracy: 0.9949 - val_loss: 0.0135\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9961 - loss: 0.0101 - val_accuracy: 0.9967 - val_loss: 0.0124\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9963 - loss: 0.0129 - val_accuracy: 0.9978 - val_loss: 0.0065\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000009.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.997832939322301 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 10\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79D1A8AD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8975 - loss: 2.8534 - val_accuracy: 0.9714 - val_loss: 0.0729\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9651 - loss: 0.0908 - val_accuracy: 0.9758 - val_loss: 0.0646\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9780 - loss: 0.0573 - val_accuracy: 0.9846 - val_loss: 0.0406\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9854 - loss: 0.0397 - val_accuracy: 0.9878 - val_loss: 0.0382\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9896 - loss: 0.0331 - val_accuracy: 0.9886 - val_loss: 0.0334\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9910 - loss: 0.0266 - val_accuracy: 0.9911 - val_loss: 0.0244\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9901 - loss: 0.0262 - val_accuracy: 0.9915 - val_loss: 0.0276\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9941 - loss: 0.0176 - val_accuracy: 0.9917 - val_loss: 0.0258\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9934 - loss: 0.0177 - val_accuracy: 0.9931 - val_loss: 0.0211\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9946 - loss: 0.0151 - val_accuracy: 0.9903 - val_loss: 0.0315\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000010.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9903467297084318 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 11\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A16547A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9119 - loss: 1.2597 - val_accuracy: 0.9754 - val_loss: 0.0641\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9737 - loss: 0.0683 - val_accuracy: 0.9833 - val_loss: 0.0471\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9870 - loss: 0.0346 - val_accuracy: 0.9842 - val_loss: 0.0467\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9920 - loss: 0.0232 - val_accuracy: 0.9947 - val_loss: 0.0209\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9926 - loss: 0.0200 - val_accuracy: 0.9894 - val_loss: 0.0416\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9948 - loss: 0.0156 - val_accuracy: 0.9911 - val_loss: 0.0339\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9939 - loss: 0.0154 - val_accuracy: 0.9957 - val_loss: 0.0216\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9968 - loss: 0.0095 - val_accuracy: 0.9961 - val_loss: 0.0178\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9960 - loss: 0.0098 - val_accuracy: 0.9953 - val_loss: 0.0211\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9975 - loss: 0.0074 - val_accuracy: 0.9886 - val_loss: 0.0796\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000011.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9885736800630418 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 12\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79CC8EF60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9094 - loss: 1.2315 - val_accuracy: 0.9801 - val_loss: 0.0608\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9728 - loss: 0.0691 - val_accuracy: 0.9829 - val_loss: 0.0492\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9842 - loss: 0.0452 - val_accuracy: 0.9951 - val_loss: 0.0183\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9903 - loss: 0.0268 - val_accuracy: 0.9935 - val_loss: 0.0217\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9954 - loss: 0.0167 - val_accuracy: 0.9929 - val_loss: 0.0214\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9945 - loss: 0.0174 - val_accuracy: 0.9974 - val_loss: 0.0144\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9952 - loss: 0.0149 - val_accuracy: 0.9945 - val_loss: 0.0194\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9940 - loss: 0.0173 - val_accuracy: 0.9945 - val_loss: 0.0214\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9953 - loss: 0.0164 - val_accuracy: 0.9970 - val_loss: 0.0136\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9972 - loss: 0.0092 - val_accuracy: 0.9959 - val_loss: 0.0249\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000012.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 13\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A1447E60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9222 - loss: 0.5290 - val_accuracy: 0.9911 - val_loss: 0.0257\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9866 - loss: 0.0358 - val_accuracy: 0.9968 - val_loss: 0.0131\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9931 - loss: 0.0238 - val_accuracy: 0.9961 - val_loss: 0.0117\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9958 - loss: 0.0132 - val_accuracy: 0.9965 - val_loss: 0.0098\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9945 - loss: 0.0142 - val_accuracy: 0.9892 - val_loss: 0.0318\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9969 - loss: 0.0114 - val_accuracy: 0.9949 - val_loss: 0.0153\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9979 - loss: 0.0081 - val_accuracy: 0.9968 - val_loss: 0.0119\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9959 - loss: 0.0120 - val_accuracy: 0.9990 - val_loss: 0.0052\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 0.9984 - val_loss: 0.0057\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9954 - loss: 0.0128 - val_accuracy: 0.9986 - val_loss: 0.0043\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000013.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9986209613869188 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 14\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79DDF5610>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9070 - loss: 3.2310 - val_accuracy: 0.9677 - val_loss: 0.0901\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9631 - loss: 0.0913 - val_accuracy: 0.9744 - val_loss: 0.0755\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9733 - loss: 0.0667 - val_accuracy: 0.9860 - val_loss: 0.0480\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9860 - loss: 0.0414 - val_accuracy: 0.9882 - val_loss: 0.0349\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9911 - loss: 0.0268 - val_accuracy: 0.9858 - val_loss: 0.0618\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9940 - loss: 0.0191 - val_accuracy: 0.9911 - val_loss: 0.0313\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9964 - loss: 0.0126 - val_accuracy: 0.9870 - val_loss: 0.0548\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9961 - loss: 0.0119 - val_accuracy: 0.9884 - val_loss: 0.0580\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9951 - loss: 0.0144 - val_accuracy: 0.9943 - val_loss: 0.0343\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9955 - loss: 0.0143 - val_accuracy: 0.9941 - val_loss: 0.0411\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000014.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9940898345153665 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 15\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A156D790>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8976 - loss: 3.6859 - val_accuracy: 0.9801 - val_loss: 0.0632\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9716 - loss: 0.0758 - val_accuracy: 0.9844 - val_loss: 0.0536\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9841 - loss: 0.0403 - val_accuracy: 0.9909 - val_loss: 0.0318\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9905 - loss: 0.0278 - val_accuracy: 0.9945 - val_loss: 0.0186\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9919 - loss: 0.0203 - val_accuracy: 0.9965 - val_loss: 0.0229\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9952 - loss: 0.0146 - val_accuracy: 0.9941 - val_loss: 0.0215\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9946 - loss: 0.0138 - val_accuracy: 0.9925 - val_loss: 0.0462\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9954 - loss: 0.0146 - val_accuracy: 0.9959 - val_loss: 0.0257\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9935 - loss: 0.0189 - val_accuracy: 0.9949 - val_loss: 0.0222\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9944 - loss: 0.0161 - val_accuracy: 0.9961 - val_loss: 0.0261\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000015.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.996059889676911 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 16\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79F10EE40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.9049 - loss: 2.6195 - val_accuracy: 0.9744 - val_loss: 0.0620\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9738 - loss: 0.0708 - val_accuracy: 0.9870 - val_loss: 0.0416\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9874 - loss: 0.0364 - val_accuracy: 0.9913 - val_loss: 0.0281\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9936 - loss: 0.0197 - val_accuracy: 0.9915 - val_loss: 0.0297\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9921 - loss: 0.0200 - val_accuracy: 0.9931 - val_loss: 0.0305\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9948 - loss: 0.0185 - val_accuracy: 0.9968 - val_loss: 0.0180\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9944 - loss: 0.0188 - val_accuracy: 0.9945 - val_loss: 0.0217\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9955 - loss: 0.0121 - val_accuracy: 0.9957 - val_loss: 0.0154\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9974 - loss: 0.0086 - val_accuracy: 0.9947 - val_loss: 0.0169\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9959 - loss: 0.0120 - val_accuracy: 0.9965 - val_loss: 0.0169\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000016.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9964539007092199 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 17\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A2B66900>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9072 - loss: 2.4338 - val_accuracy: 0.9793 - val_loss: 0.0574\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9754 - loss: 0.0656 - val_accuracy: 0.9913 - val_loss: 0.0295\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9838 - loss: 0.0402 - val_accuracy: 0.9937 - val_loss: 0.0220\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9909 - loss: 0.0243 - val_accuracy: 0.9888 - val_loss: 0.0256\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9946 - loss: 0.0178 - val_accuracy: 0.9919 - val_loss: 0.0368\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9947 - loss: 0.0146 - val_accuracy: 0.9927 - val_loss: 0.0290\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9935 - loss: 0.0166 - val_accuracy: 0.9933 - val_loss: 0.0226\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9955 - loss: 0.0131 - val_accuracy: 0.9927 - val_loss: 0.0284\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9952 - loss: 0.0156 - val_accuracy: 0.9921 - val_loss: 0.0294\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9968 - loss: 0.0100 - val_accuracy: 0.9945 - val_loss: 0.0279\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000017.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 18\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79EE884A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.8966 - loss: 2.1170 - val_accuracy: 0.9703 - val_loss: 0.0885\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9614 - loss: 0.0954 - val_accuracy: 0.9770 - val_loss: 0.0571\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9757 - loss: 0.0643 - val_accuracy: 0.9836 - val_loss: 0.0468\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9841 - loss: 0.0433 - val_accuracy: 0.9900 - val_loss: 0.0307\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9905 - loss: 0.0310 - val_accuracy: 0.9931 - val_loss: 0.0198\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9928 - loss: 0.0203 - val_accuracy: 0.9947 - val_loss: 0.0178\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9910 - loss: 0.0253 - val_accuracy: 0.9937 - val_loss: 0.0232\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9934 - loss: 0.0178 - val_accuracy: 0.9945 - val_loss: 0.0185\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9938 - loss: 0.0182 - val_accuracy: 0.9913 - val_loss: 0.0260\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9922 - loss: 0.0220 - val_accuracy: 0.9943 - val_loss: 0.0233\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000018.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9942868400315209 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 19\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A30CD640>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.8958 - loss: 2.0243 - val_accuracy: 0.9833 - val_loss: 0.0579\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9724 - loss: 0.0751 - val_accuracy: 0.9874 - val_loss: 0.0389\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9867 - loss: 0.0448 - val_accuracy: 0.9870 - val_loss: 0.0398\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9897 - loss: 0.0336 - val_accuracy: 0.9913 - val_loss: 0.0339\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9937 - loss: 0.0208 - val_accuracy: 0.9943 - val_loss: 0.0243\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9957 - loss: 0.0146 - val_accuracy: 0.9894 - val_loss: 0.0442\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9955 - loss: 0.0141 - val_accuracy: 0.9951 - val_loss: 0.0227\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9975 - loss: 0.0095 - val_accuracy: 0.9965 - val_loss: 0.0245\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9954 - loss: 0.0134 - val_accuracy: 0.9970 - val_loss: 0.0209\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9976 - loss: 0.0072 - val_accuracy: 0.9957 - val_loss: 0.0235\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000019.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 20\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A2F5DAF0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 65ms/step - accuracy: 0.9208 - loss: 1.2151 - val_accuracy: 0.9813 - val_loss: 0.0467\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9812 - loss: 0.0539 - val_accuracy: 0.9886 - val_loss: 0.0316\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9887 - loss: 0.0321 - val_accuracy: 0.9929 - val_loss: 0.0186\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9957 - loss: 0.0145 - val_accuracy: 0.9923 - val_loss: 0.0284\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9950 - loss: 0.0136 - val_accuracy: 0.9949 - val_loss: 0.0186\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9936 - loss: 0.0174 - val_accuracy: 0.9951 - val_loss: 0.0176\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9963 - loss: 0.0108 - val_accuracy: 0.9953 - val_loss: 0.0160\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9963 - loss: 0.0124 - val_accuracy: 0.9963 - val_loss: 0.0159\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 0.9963 - val_loss: 0.0130\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9980 - loss: 0.0060 - val_accuracy: 0.9967 - val_loss: 0.0168\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000020.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9966509062253743 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 21\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79D46B3B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9061 - loss: 1.7553 - val_accuracy: 0.9750 - val_loss: 0.0827\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9675 - loss: 0.0869 - val_accuracy: 0.9882 - val_loss: 0.0380\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9874 - loss: 0.0485 - val_accuracy: 0.9760 - val_loss: 0.0644\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9889 - loss: 0.0354 - val_accuracy: 0.9959 - val_loss: 0.0149\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9931 - loss: 0.0257 - val_accuracy: 0.9957 - val_loss: 0.0113\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9934 - loss: 0.0219 - val_accuracy: 0.9878 - val_loss: 0.0452\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9949 - loss: 0.0176 - val_accuracy: 0.9968 - val_loss: 0.0163\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9960 - loss: 0.0133 - val_accuracy: 0.9929 - val_loss: 0.0316\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9971 - loss: 0.0108 - val_accuracy: 0.9949 - val_loss: 0.0240\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9950 - loss: 0.0154 - val_accuracy: 0.9951 - val_loss: 0.0176\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000021.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 22\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A68F996600>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9150 - loss: 1.4403 - val_accuracy: 0.9831 - val_loss: 0.0491\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9798 - loss: 0.0547 - val_accuracy: 0.9856 - val_loss: 0.0417\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9901 - loss: 0.0289 - val_accuracy: 0.9941 - val_loss: 0.0184\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9937 - loss: 0.0207 - val_accuracy: 0.9945 - val_loss: 0.0219\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9942 - loss: 0.0162 - val_accuracy: 0.9931 - val_loss: 0.0270\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9941 - loss: 0.0170 - val_accuracy: 0.9961 - val_loss: 0.0146\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9971 - loss: 0.0081 - val_accuracy: 0.9976 - val_loss: 0.0131\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9963 - loss: 0.0092 - val_accuracy: 0.9961 - val_loss: 0.0148\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9954 - loss: 0.0158 - val_accuracy: 0.9957 - val_loss: 0.0190\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9959 - loss: 0.0117 - val_accuracy: 0.9951 - val_loss: 0.0238\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000022.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 23\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A2729FA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.8990 - loss: 2.5012 - val_accuracy: 0.9628 - val_loss: 0.0964\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9630 - loss: 0.0921 - val_accuracy: 0.9714 - val_loss: 0.0721\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9816 - loss: 0.0581 - val_accuracy: 0.9900 - val_loss: 0.0355\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9914 - loss: 0.0348 - val_accuracy: 0.9919 - val_loss: 0.0313\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9921 - loss: 0.0289 - val_accuracy: 0.9896 - val_loss: 0.0361\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9894 - loss: 0.0315 - val_accuracy: 0.9951 - val_loss: 0.0207\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9964 - loss: 0.0141 - val_accuracy: 0.9931 - val_loss: 0.0316\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9932 - loss: 0.0187 - val_accuracy: 0.9933 - val_loss: 0.0213\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9964 - loss: 0.0101 - val_accuracy: 0.9939 - val_loss: 0.0317\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9976 - loss: 0.0089 - val_accuracy: 0.9943 - val_loss: 0.0397\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000023.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9942868400315209 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 24\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79DDF71D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9188 - loss: 0.8130 - val_accuracy: 0.9760 - val_loss: 0.0606\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9753 - loss: 0.0671 - val_accuracy: 0.9878 - val_loss: 0.0409\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9861 - loss: 0.0370 - val_accuracy: 0.9931 - val_loss: 0.0204\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9934 - loss: 0.0173 - val_accuracy: 0.9925 - val_loss: 0.0239\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9955 - loss: 0.0134 - val_accuracy: 0.9970 - val_loss: 0.0121\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9957 - loss: 0.0122 - val_accuracy: 0.9963 - val_loss: 0.0130\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9942 - loss: 0.0192 - val_accuracy: 0.9974 - val_loss: 0.0115\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9959 - loss: 0.0135 - val_accuracy: 0.9931 - val_loss: 0.0250\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9975 - loss: 0.0089 - val_accuracy: 0.9959 - val_loss: 0.0136\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9952 - loss: 0.0155 - val_accuracy: 0.9953 - val_loss: 0.0236\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000024.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 25\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A68E85DB50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8975 - loss: 1.7284 - val_accuracy: 0.9732 - val_loss: 0.0724\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9681 - loss: 0.0825 - val_accuracy: 0.9858 - val_loss: 0.0409\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9830 - loss: 0.0483 - val_accuracy: 0.9905 - val_loss: 0.0307\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9876 - loss: 0.0366 - val_accuracy: 0.9888 - val_loss: 0.0304\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9938 - loss: 0.0184 - val_accuracy: 0.9945 - val_loss: 0.0209\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9918 - loss: 0.0241 - val_accuracy: 0.9961 - val_loss: 0.0125\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9930 - loss: 0.0183 - val_accuracy: 0.9941 - val_loss: 0.0176\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9957 - loss: 0.0129 - val_accuracy: 0.9945 - val_loss: 0.0175\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9966 - loss: 0.0111 - val_accuracy: 0.9957 - val_loss: 0.0123\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9968 - loss: 0.0093 - val_accuracy: 0.9976 - val_loss: 0.0105\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000025.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9976359338061466 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 26\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A121E0C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9215 - loss: 1.0808 - val_accuracy: 0.9894 - val_loss: 0.0383\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9857 - loss: 0.0383 - val_accuracy: 0.9923 - val_loss: 0.0327\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9899 - loss: 0.0299 - val_accuracy: 0.9890 - val_loss: 0.0370\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9952 - loss: 0.0145 - val_accuracy: 0.9925 - val_loss: 0.0297\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9956 - loss: 0.0135 - val_accuracy: 0.9953 - val_loss: 0.0203\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9959 - loss: 0.0149 - val_accuracy: 0.9903 - val_loss: 0.0347\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9957 - loss: 0.0129 - val_accuracy: 0.9907 - val_loss: 0.0585\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9949 - loss: 0.0158 - val_accuracy: 0.9939 - val_loss: 0.0261\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9951 - loss: 0.0156 - val_accuracy: 0.9949 - val_loss: 0.0248\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9976 - loss: 0.0079 - val_accuracy: 0.9927 - val_loss: 0.0494\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000026.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9927107959022853 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 27\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A757AB7710>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.9016 - loss: 1.7610 - val_accuracy: 0.9742 - val_loss: 0.0732\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9665 - loss: 0.0894 - val_accuracy: 0.9850 - val_loss: 0.0396\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9835 - loss: 0.0495 - val_accuracy: 0.9868 - val_loss: 0.0367\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9891 - loss: 0.0309 - val_accuracy: 0.9929 - val_loss: 0.0205\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9910 - loss: 0.0268 - val_accuracy: 0.9909 - val_loss: 0.0254\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9917 - loss: 0.0218 - val_accuracy: 0.9900 - val_loss: 0.0319\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9926 - loss: 0.0195 - val_accuracy: 0.9915 - val_loss: 0.0252\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9921 - loss: 0.0210 - val_accuracy: 0.9898 - val_loss: 0.0327\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9949 - loss: 0.0150 - val_accuracy: 0.9937 - val_loss: 0.0227\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9952 - loss: 0.0146 - val_accuracy: 0.9901 - val_loss: 0.0443\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000027.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9901497241922774 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 28\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7582AFF20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.8823 - loss: 3.1454 - val_accuracy: 0.9559 - val_loss: 0.0975\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9663 - loss: 0.0966 - val_accuracy: 0.9813 - val_loss: 0.0510\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9812 - loss: 0.0597 - val_accuracy: 0.9913 - val_loss: 0.0308\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9905 - loss: 0.0371 - val_accuracy: 0.9953 - val_loss: 0.0185\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9898 - loss: 0.0310 - val_accuracy: 0.9931 - val_loss: 0.0291\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9906 - loss: 0.0299 - val_accuracy: 0.9947 - val_loss: 0.0301\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9940 - loss: 0.0197 - val_accuracy: 0.9947 - val_loss: 0.0211\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9954 - loss: 0.0171 - val_accuracy: 0.9965 - val_loss: 0.0171\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9944 - loss: 0.0173 - val_accuracy: 0.9957 - val_loss: 0.0204\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9945 - loss: 0.0173 - val_accuracy: 0.9957 - val_loss: 0.0236\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000028.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 29\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75778FE60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9121 - loss: 1.4055 - val_accuracy: 0.9850 - val_loss: 0.0465\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9843 - loss: 0.0410 - val_accuracy: 0.9882 - val_loss: 0.0384\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9924 - loss: 0.0220 - val_accuracy: 0.9937 - val_loss: 0.0198\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9947 - loss: 0.0148 - val_accuracy: 0.9923 - val_loss: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9945 - loss: 0.0180 - val_accuracy: 0.9927 - val_loss: 0.0266\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9970 - loss: 0.0089 - val_accuracy: 0.9949 - val_loss: 0.0191\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9974 - loss: 0.0089 - val_accuracy: 0.9929 - val_loss: 0.0249\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9974 - loss: 0.0084 - val_accuracy: 0.9937 - val_loss: 0.0215\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0092 - val_accuracy: 0.9947 - val_loss: 0.0234\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9965 - loss: 0.0121 - val_accuracy: 0.9945 - val_loss: 0.0255\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000029.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 30\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A758909A90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.9132 - loss: 1.4520 - val_accuracy: 0.9760 - val_loss: 0.0680\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9757 - loss: 0.0593 - val_accuracy: 0.9903 - val_loss: 0.0272\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9887 - loss: 0.0341 - val_accuracy: 0.9923 - val_loss: 0.0231\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9924 - loss: 0.0221 - val_accuracy: 0.9945 - val_loss: 0.0210\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9956 - loss: 0.0143 - val_accuracy: 0.9931 - val_loss: 0.0259\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9943 - loss: 0.0153 - val_accuracy: 0.9941 - val_loss: 0.0186\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9957 - loss: 0.0123 - val_accuracy: 0.9949 - val_loss: 0.0181\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9962 - loss: 0.0128 - val_accuracy: 0.9955 - val_loss: 0.0191\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9967 - loss: 0.0104 - val_accuracy: 0.9919 - val_loss: 0.0326\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9951 - loss: 0.0143 - val_accuracy: 0.9949 - val_loss: 0.0186\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000030.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9948778565799843 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 31\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79CA94C20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.9075 - loss: 2.3782 - val_accuracy: 0.9726 - val_loss: 0.0689\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9762 - loss: 0.0674 - val_accuracy: 0.9921 - val_loss: 0.0247\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9896 - loss: 0.0296 - val_accuracy: 0.9957 - val_loss: 0.0129\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9929 - loss: 0.0233 - val_accuracy: 0.9911 - val_loss: 0.0236\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 67ms/step - accuracy: 0.9934 - loss: 0.0191 - val_accuracy: 0.9968 - val_loss: 0.0111\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9948 - loss: 0.0138 - val_accuracy: 0.9943 - val_loss: 0.0203\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9931 - loss: 0.0206 - val_accuracy: 0.9949 - val_loss: 0.0153\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9959 - loss: 0.0153 - val_accuracy: 0.9968 - val_loss: 0.0126\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9973 - loss: 0.0078 - val_accuracy: 0.9947 - val_loss: 0.0167\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9961 - loss: 0.0100 - val_accuracy: 0.9970 - val_loss: 0.0111\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000031.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9970449172576832 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 32\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A768495550>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9019 - loss: 2.6356 - val_accuracy: 0.9586 - val_loss: 0.1031\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9749 - loss: 0.0703 - val_accuracy: 0.9870 - val_loss: 0.0383\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9881 - loss: 0.0344 - val_accuracy: 0.9860 - val_loss: 0.0441\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9914 - loss: 0.0249 - val_accuracy: 0.9901 - val_loss: 0.0268\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9942 - loss: 0.0173 - val_accuracy: 0.9917 - val_loss: 0.0261\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9955 - loss: 0.0143 - val_accuracy: 0.9927 - val_loss: 0.0287\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9957 - loss: 0.0122 - val_accuracy: 0.9921 - val_loss: 0.0240\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9959 - loss: 0.0127 - val_accuracy: 0.9864 - val_loss: 0.0648\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9951 - loss: 0.0139 - val_accuracy: 0.9935 - val_loss: 0.0399\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9958 - loss: 0.0117 - val_accuracy: 0.9939 - val_loss: 0.0259\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000032.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 33\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75828E5A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.8944 - loss: 3.2423 - val_accuracy: 0.9667 - val_loss: 0.1101\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9579 - loss: 0.1063 - val_accuracy: 0.9833 - val_loss: 0.0579\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9798 - loss: 0.0669 - val_accuracy: 0.9888 - val_loss: 0.0375\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9872 - loss: 0.0477 - val_accuracy: 0.9900 - val_loss: 0.0420\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9894 - loss: 0.0346 - val_accuracy: 0.9868 - val_loss: 0.0434\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9921 - loss: 0.0273 - val_accuracy: 0.9951 - val_loss: 0.0184\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9946 - loss: 0.0206 - val_accuracy: 0.9896 - val_loss: 0.0249\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9943 - loss: 0.0204 - val_accuracy: 0.9953 - val_loss: 0.0148\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9948 - loss: 0.0170 - val_accuracy: 0.9941 - val_loss: 0.0199\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9943 - loss: 0.0169 - val_accuracy: 0.9939 - val_loss: 0.0283\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000033.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 34\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A758C75610>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9129 - loss: 2.2298 - val_accuracy: 0.9815 - val_loss: 0.0539\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9868 - loss: 0.0387 - val_accuracy: 0.9835 - val_loss: 0.0470\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9921 - loss: 0.0256 - val_accuracy: 0.9933 - val_loss: 0.0261\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9929 - loss: 0.0201 - val_accuracy: 0.9917 - val_loss: 0.0318\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9955 - loss: 0.0124 - val_accuracy: 0.9907 - val_loss: 0.0296\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9965 - loss: 0.0116 - val_accuracy: 0.9909 - val_loss: 0.0436\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9963 - loss: 0.0122 - val_accuracy: 0.9907 - val_loss: 0.0480\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9958 - loss: 0.0108 - val_accuracy: 0.9945 - val_loss: 0.0286\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9976 - loss: 0.0072 - val_accuracy: 0.9933 - val_loss: 0.0371\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9967 - loss: 0.0107 - val_accuracy: 0.9929 - val_loss: 0.0308\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000034.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9929078014184397 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 35\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75918F4D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.9126 - loss: 0.7604 - val_accuracy: 0.9838 - val_loss: 0.0507\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9781 - loss: 0.0602 - val_accuracy: 0.9876 - val_loss: 0.0322\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9876 - loss: 0.0319 - val_accuracy: 0.9925 - val_loss: 0.0204\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9923 - loss: 0.0229 - val_accuracy: 0.9947 - val_loss: 0.0145\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9935 - loss: 0.0204 - val_accuracy: 0.9931 - val_loss: 0.0179\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9946 - loss: 0.0153 - val_accuracy: 0.9827 - val_loss: 0.0493\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9929 - loss: 0.0207 - val_accuracy: 0.9959 - val_loss: 0.0106\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9970 - loss: 0.0093 - val_accuracy: 0.9957 - val_loss: 0.0137\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9963 - loss: 0.0106 - val_accuracy: 0.9915 - val_loss: 0.0293\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9937 - loss: 0.0168 - val_accuracy: 0.9939 - val_loss: 0.0169\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000035.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 36\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A45FA2A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.8980 - loss: 2.1108 - val_accuracy: 0.9669 - val_loss: 0.0920\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9634 - loss: 0.0903 - val_accuracy: 0.9836 - val_loss: 0.0541\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9815 - loss: 0.0526 - val_accuracy: 0.9862 - val_loss: 0.0437\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9860 - loss: 0.0418 - val_accuracy: 0.9724 - val_loss: 0.0794\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9903 - loss: 0.0287 - val_accuracy: 0.9900 - val_loss: 0.0449\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9927 - loss: 0.0218 - val_accuracy: 0.9933 - val_loss: 0.0282\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9959 - loss: 0.0128 - val_accuracy: 0.9947 - val_loss: 0.0307\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9957 - loss: 0.0127 - val_accuracy: 0.9929 - val_loss: 0.0396\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9954 - loss: 0.0157 - val_accuracy: 0.9937 - val_loss: 0.0463\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - accuracy: 0.9972 - loss: 0.0087 - val_accuracy: 0.9945 - val_loss: 0.0431\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000036.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 37\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7591B62D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9137 - loss: 1.0700 - val_accuracy: 0.9827 - val_loss: 0.0564\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9806 - loss: 0.0588 - val_accuracy: 0.9866 - val_loss: 0.0364\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9880 - loss: 0.0342 - val_accuracy: 0.9888 - val_loss: 0.0326\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9910 - loss: 0.0241 - val_accuracy: 0.9957 - val_loss: 0.0178\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9937 - loss: 0.0178 - val_accuracy: 0.9933 - val_loss: 0.0199\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9941 - loss: 0.0194 - val_accuracy: 0.9967 - val_loss: 0.0123\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9952 - loss: 0.0147 - val_accuracy: 0.9955 - val_loss: 0.0167\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9951 - loss: 0.0132 - val_accuracy: 0.9933 - val_loss: 0.0281\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9971 - loss: 0.0099 - val_accuracy: 0.9957 - val_loss: 0.0198\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9957 - loss: 0.0133 - val_accuracy: 0.9959 - val_loss: 0.0166\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000037.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 38\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79CD5CC20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9025 - loss: 1.1677 - val_accuracy: 0.9620 - val_loss: 0.0898\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9722 - loss: 0.0748 - val_accuracy: 0.9892 - val_loss: 0.0359\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9859 - loss: 0.0395 - val_accuracy: 0.9927 - val_loss: 0.0256\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9871 - loss: 0.0349 - val_accuracy: 0.9953 - val_loss: 0.0152\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9919 - loss: 0.0229 - val_accuracy: 0.9965 - val_loss: 0.0122\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9926 - loss: 0.0188 - val_accuracy: 0.9949 - val_loss: 0.0205\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9949 - loss: 0.0146 - val_accuracy: 0.9957 - val_loss: 0.0161\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9944 - loss: 0.0145 - val_accuracy: 0.9965 - val_loss: 0.0147\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9916 - loss: 0.0192 - val_accuracy: 0.9931 - val_loss: 0.0271\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9951 - loss: 0.0137 - val_accuracy: 0.9951 - val_loss: 0.0172\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000038.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 39\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7599E8320>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8944 - loss: 1.7013 - val_accuracy: 0.9736 - val_loss: 0.0797\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9605 - loss: 0.0987 - val_accuracy: 0.9835 - val_loss: 0.0521\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9771 - loss: 0.0629 - val_accuracy: 0.9933 - val_loss: 0.0304\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9864 - loss: 0.0403 - val_accuracy: 0.9925 - val_loss: 0.0266\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9885 - loss: 0.0346 - val_accuracy: 0.9856 - val_loss: 0.0553\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9907 - loss: 0.0284 - val_accuracy: 0.9842 - val_loss: 0.0542\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9910 - loss: 0.0253 - val_accuracy: 0.9939 - val_loss: 0.0326\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9930 - loss: 0.0198 - val_accuracy: 0.9945 - val_loss: 0.0313\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9931 - loss: 0.0177 - val_accuracy: 0.9961 - val_loss: 0.0255\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9960 - loss: 0.0129 - val_accuracy: 0.9939 - val_loss: 0.0488\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000039.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 40\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7599E9B80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.9175 - loss: 0.9173 - val_accuracy: 0.9836 - val_loss: 0.0480\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9818 - loss: 0.0527 - val_accuracy: 0.9925 - val_loss: 0.0246\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9888 - loss: 0.0290 - val_accuracy: 0.9898 - val_loss: 0.0292\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9934 - loss: 0.0193 - val_accuracy: 0.9900 - val_loss: 0.0228\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9925 - loss: 0.0200 - val_accuracy: 0.9941 - val_loss: 0.0197\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9953 - loss: 0.0122 - val_accuracy: 0.9939 - val_loss: 0.0209\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9963 - loss: 0.0124 - val_accuracy: 0.9923 - val_loss: 0.0221\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0110 - val_accuracy: 0.9931 - val_loss: 0.0301\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9957 - loss: 0.0132 - val_accuracy: 0.9965 - val_loss: 0.0185\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9970 - loss: 0.0093 - val_accuracy: 0.9939 - val_loss: 0.0261\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000040.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 41\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75D734C20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.8990 - loss: 2.4866 - val_accuracy: 0.9671 - val_loss: 0.0839\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9626 - loss: 0.0916 - val_accuracy: 0.9852 - val_loss: 0.0534\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9820 - loss: 0.0531 - val_accuracy: 0.9901 - val_loss: 0.0307\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9895 - loss: 0.0317 - val_accuracy: 0.9903 - val_loss: 0.0338\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9920 - loss: 0.0255 - val_accuracy: 0.9864 - val_loss: 0.0529\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9937 - loss: 0.0219 - val_accuracy: 0.9905 - val_loss: 0.0293\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9939 - loss: 0.0188 - val_accuracy: 0.9919 - val_loss: 0.0372\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9953 - loss: 0.0137 - val_accuracy: 0.9959 - val_loss: 0.0237\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9967 - loss: 0.0110 - val_accuracy: 0.9923 - val_loss: 0.0425\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9974 - loss: 0.0088 - val_accuracy: 0.9945 - val_loss: 0.0425\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000041.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 42\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A759967260>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 63ms/step - accuracy: 0.9111 - loss: 2.4610 - val_accuracy: 0.9827 - val_loss: 0.0492\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9837 - loss: 0.0495 - val_accuracy: 0.9923 - val_loss: 0.0275\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9912 - loss: 0.0261 - val_accuracy: 0.9943 - val_loss: 0.0154\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9941 - loss: 0.0163 - val_accuracy: 0.9945 - val_loss: 0.0171\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9952 - loss: 0.0139 - val_accuracy: 0.9955 - val_loss: 0.0154\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9960 - loss: 0.0114 - val_accuracy: 0.9929 - val_loss: 0.0213\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9962 - loss: 0.0085 - val_accuracy: 0.9947 - val_loss: 0.0173\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 0.9953 - val_loss: 0.0145\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9975 - loss: 0.0078 - val_accuracy: 0.9935 - val_loss: 0.0216\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9968 - loss: 0.0106 - val_accuracy: 0.9923 - val_loss: 0.0264\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000042.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9923167848699763 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 43\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75DAE3E60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.9159 - loss: 0.8816 - val_accuracy: 0.9805 - val_loss: 0.0567\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9824 - loss: 0.0512 - val_accuracy: 0.9876 - val_loss: 0.0323\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9912 - loss: 0.0242 - val_accuracy: 0.9933 - val_loss: 0.0197\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9952 - loss: 0.0154 - val_accuracy: 0.9968 - val_loss: 0.0102\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9966 - loss: 0.0099 - val_accuracy: 0.9951 - val_loss: 0.0154\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9953 - loss: 0.0143 - val_accuracy: 0.9959 - val_loss: 0.0132\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0110 - val_accuracy: 0.9959 - val_loss: 0.0180\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9968 - loss: 0.0089 - val_accuracy: 0.9967 - val_loss: 0.0106\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9969 - loss: 0.0101 - val_accuracy: 0.9903 - val_loss: 0.0413\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9952 - loss: 0.0153 - val_accuracy: 0.9967 - val_loss: 0.0124\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000043.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9966509062253743 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 44\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A759B62570>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9176 - loss: 1.0383 - val_accuracy: 0.9854 - val_loss: 0.0454\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9855 - loss: 0.0401 - val_accuracy: 0.9915 - val_loss: 0.0305\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9915 - loss: 0.0248 - val_accuracy: 0.9911 - val_loss: 0.0280\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9919 - loss: 0.0251 - val_accuracy: 0.9890 - val_loss: 0.0420\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9951 - loss: 0.0134 - val_accuracy: 0.9929 - val_loss: 0.0261\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9951 - loss: 0.0138 - val_accuracy: 0.9872 - val_loss: 0.0355\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9950 - loss: 0.0162 - val_accuracy: 0.9933 - val_loss: 0.0264\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9952 - loss: 0.0146 - val_accuracy: 0.9872 - val_loss: 0.0501\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9970 - loss: 0.0097 - val_accuracy: 0.9941 - val_loss: 0.0197\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9959 - loss: 0.0119 - val_accuracy: 0.9894 - val_loss: 0.0425\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000044.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9893617021276596 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 45\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A27B91F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.9028 - loss: 1.9645 - val_accuracy: 0.9744 - val_loss: 0.0690\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9784 - loss: 0.0574 - val_accuracy: 0.9905 - val_loss: 0.0340\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9885 - loss: 0.0347 - val_accuracy: 0.9929 - val_loss: 0.0273\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9922 - loss: 0.0248 - val_accuracy: 0.9963 - val_loss: 0.0210\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9945 - loss: 0.0172 - val_accuracy: 0.9921 - val_loss: 0.0309\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9958 - loss: 0.0148 - val_accuracy: 0.9937 - val_loss: 0.0221\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9957 - loss: 0.0125 - val_accuracy: 0.9949 - val_loss: 0.0325\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9960 - loss: 0.0117 - val_accuracy: 0.9949 - val_loss: 0.0176\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9952 - loss: 0.0145 - val_accuracy: 0.9968 - val_loss: 0.0149\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9962 - loss: 0.0115 - val_accuracy: 0.9961 - val_loss: 0.0251\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000045.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.996059889676911 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 46\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A759D20EC0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9142 - loss: 1.3417 - val_accuracy: 0.9557 - val_loss: 0.1039\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9694 - loss: 0.0830 - val_accuracy: 0.9791 - val_loss: 0.0539\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9799 - loss: 0.0518 - val_accuracy: 0.9866 - val_loss: 0.0440\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9900 - loss: 0.0278 - val_accuracy: 0.9911 - val_loss: 0.0287\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9920 - loss: 0.0220 - val_accuracy: 0.9927 - val_loss: 0.0380\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9940 - loss: 0.0182 - val_accuracy: 0.9943 - val_loss: 0.0304\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0132 - val_accuracy: 0.9947 - val_loss: 0.0239\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9951 - loss: 0.0134 - val_accuracy: 0.9967 - val_loss: 0.0318\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9960 - loss: 0.0126 - val_accuracy: 0.9947 - val_loss: 0.0403\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9961 - val_loss: 0.0229\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000046.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.996059889676911 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 47\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A758107D70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9027 - loss: 1.7690 - val_accuracy: 0.9699 - val_loss: 0.0833\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9684 - loss: 0.0811 - val_accuracy: 0.9817 - val_loss: 0.0506\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9805 - loss: 0.0516 - val_accuracy: 0.9878 - val_loss: 0.0432\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9897 - loss: 0.0293 - val_accuracy: 0.9907 - val_loss: 0.0312\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9918 - loss: 0.0227 - val_accuracy: 0.9923 - val_loss: 0.0277\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9912 - loss: 0.0251 - val_accuracy: 0.9939 - val_loss: 0.0282\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9934 - loss: 0.0187 - val_accuracy: 0.9919 - val_loss: 0.0318\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9950 - loss: 0.0136 - val_accuracy: 0.9929 - val_loss: 0.0386\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9941 - loss: 0.0181 - val_accuracy: 0.9929 - val_loss: 0.0334\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9947 - loss: 0.0159 - val_accuracy: 0.9931 - val_loss: 0.0302\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000047.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9931048069345941 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 48\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7593398E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9043 - loss: 1.8417 - val_accuracy: 0.9787 - val_loss: 0.0654\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9714 - loss: 0.0786 - val_accuracy: 0.9866 - val_loss: 0.0414\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9847 - loss: 0.0421 - val_accuracy: 0.9933 - val_loss: 0.0214\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9906 - loss: 0.0276 - val_accuracy: 0.9921 - val_loss: 0.0300\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9923 - loss: 0.0210 - val_accuracy: 0.9951 - val_loss: 0.0200\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9933 - loss: 0.0207 - val_accuracy: 0.9967 - val_loss: 0.0188\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9937 - loss: 0.0181 - val_accuracy: 0.9937 - val_loss: 0.0276\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9958 - loss: 0.0128 - val_accuracy: 0.9935 - val_loss: 0.0285\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9945 - loss: 0.0144 - val_accuracy: 0.9949 - val_loss: 0.0214\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9951 - loss: 0.0155 - val_accuracy: 0.9970 - val_loss: 0.0182\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000048.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9970449172576832 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 49\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75F664BF0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9142 - loss: 1.5690 - val_accuracy: 0.9740 - val_loss: 0.0611\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9856 - loss: 0.0432 - val_accuracy: 0.9941 - val_loss: 0.0208\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9897 - loss: 0.0282 - val_accuracy: 0.9907 - val_loss: 0.0270\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9926 - loss: 0.0226 - val_accuracy: 0.9947 - val_loss: 0.0161\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9936 - loss: 0.0163 - val_accuracy: 0.9957 - val_loss: 0.0196\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9945 - loss: 0.0147 - val_accuracy: 0.9953 - val_loss: 0.0202\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9949 - loss: 0.0151 - val_accuracy: 0.9927 - val_loss: 0.0336\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9941 - loss: 0.0131 - val_accuracy: 0.9965 - val_loss: 0.0172\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9970 - loss: 0.0091 - val_accuracy: 0.9961 - val_loss: 0.0224\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9965 - loss: 0.0106 - val_accuracy: 0.9959 - val_loss: 0.0212\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000049.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 50\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A759BD2000>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9177 - loss: 1.5842 - val_accuracy: 0.9823 - val_loss: 0.0464\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9836 - loss: 0.0442 - val_accuracy: 0.9937 - val_loss: 0.0202\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0257 - val_accuracy: 0.9945 - val_loss: 0.0169\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9945 - loss: 0.0140 - val_accuracy: 0.9937 - val_loss: 0.0218\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9951 - loss: 0.0130 - val_accuracy: 0.9963 - val_loss: 0.0178\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9941 - val_loss: 0.0278\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0097 - val_accuracy: 0.9921 - val_loss: 0.0382\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9821 - val_loss: 0.0642\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9952 - loss: 0.0140 - val_accuracy: 0.9968 - val_loss: 0.0117\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 0.9965 - val_loss: 0.0201\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000050.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9964539007092199 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 51\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A768118980>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8985 - loss: 1.2200 - val_accuracy: 0.9768 - val_loss: 0.1013\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - accuracy: 0.9645 - loss: 0.1088 - val_accuracy: 0.9872 - val_loss: 0.0633\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9812 - loss: 0.0706 - val_accuracy: 0.9921 - val_loss: 0.0303\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9865 - loss: 0.0474 - val_accuracy: 0.9951 - val_loss: 0.0200\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9919 - loss: 0.0284 - val_accuracy: 0.9917 - val_loss: 0.0263\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9927 - loss: 0.0257 - val_accuracy: 0.9978 - val_loss: 0.0091\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9915 - loss: 0.0257 - val_accuracy: 0.9959 - val_loss: 0.0124\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9937 - loss: 0.0193 - val_accuracy: 0.9953 - val_loss: 0.0155\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9946 - loss: 0.0166 - val_accuracy: 0.9945 - val_loss: 0.0174\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9926 - loss: 0.0248 - val_accuracy: 0.9970 - val_loss: 0.0091\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000051.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9970449172576832 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 52\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A26B5610>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.9168 - loss: 1.0575 - val_accuracy: 0.9799 - val_loss: 0.0609\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.9797 - loss: 0.0529 - val_accuracy: 0.9909 - val_loss: 0.0293\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9936 - loss: 0.0236 - val_accuracy: 0.9913 - val_loss: 0.0284\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9946 - loss: 0.0163 - val_accuracy: 0.9963 - val_loss: 0.0151\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9959 - loss: 0.0123 - val_accuracy: 0.9945 - val_loss: 0.0211\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9963 - loss: 0.0114 - val_accuracy: 0.9959 - val_loss: 0.0161\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9971 - loss: 0.0089 - val_accuracy: 0.9905 - val_loss: 0.0319\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9982 - loss: 0.0090 - val_accuracy: 0.9901 - val_loss: 0.0416\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9960 - loss: 0.0136 - val_accuracy: 0.9949 - val_loss: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9978 - loss: 0.0076 - val_accuracy: 0.9935 - val_loss: 0.0237\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000052.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9934988179669031 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 53\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A26B5610>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9043 - loss: 1.3658 - val_accuracy: 0.9661 - val_loss: 0.0792\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9505 - loss: 0.1041 - val_accuracy: 0.9862 - val_loss: 0.0437\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9774 - loss: 0.0583 - val_accuracy: 0.9937 - val_loss: 0.0278\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9848 - loss: 0.0416 - val_accuracy: 0.9937 - val_loss: 0.0221\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9887 - loss: 0.0335 - val_accuracy: 0.9929 - val_loss: 0.0294\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9917 - loss: 0.0250 - val_accuracy: 0.9951 - val_loss: 0.0196\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9890 - loss: 0.0297 - val_accuracy: 0.9953 - val_loss: 0.0224\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9952 - loss: 0.0153 - val_accuracy: 0.9955 - val_loss: 0.0295\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9950 - loss: 0.0161 - val_accuracy: 0.9961 - val_loss: 0.0200\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9941 - loss: 0.0165 - val_accuracy: 0.9943 - val_loss: 0.0283\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000053.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9942868400315209 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 54\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A768378E00>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9135 - loss: 1.1228 - val_accuracy: 0.9738 - val_loss: 0.0663\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9751 - loss: 0.0636 - val_accuracy: 0.9750 - val_loss: 0.0646\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9844 - loss: 0.0428 - val_accuracy: 0.9935 - val_loss: 0.0219\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0270 - val_accuracy: 0.9935 - val_loss: 0.0177\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9939 - loss: 0.0169 - val_accuracy: 0.9947 - val_loss: 0.0166\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9957 - loss: 0.0150 - val_accuracy: 0.9953 - val_loss: 0.0147\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9943 - loss: 0.0171 - val_accuracy: 0.9982 - val_loss: 0.0094\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9967 - loss: 0.0099 - val_accuracy: 0.9967 - val_loss: 0.0160\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0137 - val_accuracy: 0.9925 - val_loss: 0.0201\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9965 - loss: 0.0110 - val_accuracy: 0.9959 - val_loss: 0.0201\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000054.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 55\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A76CA0B830>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9232 - loss: 1.0536 - val_accuracy: 0.9768 - val_loss: 0.0596\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9837 - loss: 0.0463 - val_accuracy: 0.9835 - val_loss: 0.0459\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9927 - loss: 0.0210 - val_accuracy: 0.9951 - val_loss: 0.0171\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9935 - loss: 0.0184 - val_accuracy: 0.9829 - val_loss: 0.0640\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9944 - loss: 0.0154 - val_accuracy: 0.9921 - val_loss: 0.0273\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0124 - val_accuracy: 0.9953 - val_loss: 0.0237\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9968 - loss: 0.0094 - val_accuracy: 0.9947 - val_loss: 0.0192\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9950 - loss: 0.0125 - val_accuracy: 0.9931 - val_loss: 0.0335\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9923 - val_loss: 0.0359\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9976 - loss: 0.0071 - val_accuracy: 0.9927 - val_loss: 0.0445\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000055.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9927107959022853 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 56\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A76C861040>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - accuracy: 0.8955 - loss: 3.0776 - val_accuracy: 0.9793 - val_loss: 0.0650\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9771 - loss: 0.0650 - val_accuracy: 0.9900 - val_loss: 0.0335\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9881 - loss: 0.0371 - val_accuracy: 0.9846 - val_loss: 0.0456\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9919 - loss: 0.0260 - val_accuracy: 0.9903 - val_loss: 0.0273\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - accuracy: 0.9937 - loss: 0.0202 - val_accuracy: 0.9811 - val_loss: 0.0679\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9917 - loss: 0.0263 - val_accuracy: 0.9941 - val_loss: 0.0194\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9930 - loss: 0.0193 - val_accuracy: 0.9943 - val_loss: 0.0189\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9956 - loss: 0.0135 - val_accuracy: 0.9803 - val_loss: 0.0994\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9902 - loss: 0.0241 - val_accuracy: 0.9959 - val_loss: 0.0186\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9944 - loss: 0.0158 - val_accuracy: 0.9953 - val_loss: 0.0259\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000056.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 57\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7700603B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - accuracy: 0.9041 - loss: 2.5289 - val_accuracy: 0.9649 - val_loss: 0.0985\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9721 - loss: 0.0743 - val_accuracy: 0.9842 - val_loss: 0.0500\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9858 - loss: 0.0383 - val_accuracy: 0.9931 - val_loss: 0.0265\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9927 - loss: 0.0211 - val_accuracy: 0.9943 - val_loss: 0.0240\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9960 - loss: 0.0121 - val_accuracy: 0.9957 - val_loss: 0.0203\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9956 - loss: 0.0125 - val_accuracy: 0.9905 - val_loss: 0.0383\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9960 - loss: 0.0126 - val_accuracy: 0.9949 - val_loss: 0.0191\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9973 - loss: 0.0079 - val_accuracy: 0.9829 - val_loss: 0.0864\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9962 - loss: 0.0097 - val_accuracy: 0.9903 - val_loss: 0.0394\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9964 - loss: 0.0109 - val_accuracy: 0.9953 - val_loss: 0.0228\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000057.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 58\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A759964290>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9057 - loss: 1.9569 - val_accuracy: 0.9736 - val_loss: 0.0704\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9733 - loss: 0.0702 - val_accuracy: 0.9856 - val_loss: 0.0364\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9860 - loss: 0.0413 - val_accuracy: 0.9903 - val_loss: 0.0249\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9910 - loss: 0.0275 - val_accuracy: 0.9917 - val_loss: 0.0224\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9928 - loss: 0.0224 - val_accuracy: 0.9917 - val_loss: 0.0237\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9931 - loss: 0.0208 - val_accuracy: 0.9909 - val_loss: 0.0267\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9943 - loss: 0.0167 - val_accuracy: 0.9892 - val_loss: 0.0341\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9943 - loss: 0.0155 - val_accuracy: 0.9927 - val_loss: 0.0198\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0133 - val_accuracy: 0.9955 - val_loss: 0.0123\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9938 - loss: 0.0169 - val_accuracy: 0.9937 - val_loss: 0.0223\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000058.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9936958234830575 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 59\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7596F8F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.8984 - loss: 1.8366 - val_accuracy: 0.9744 - val_loss: 0.0695\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9683 - loss: 0.0807 - val_accuracy: 0.9821 - val_loss: 0.0449\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9843 - loss: 0.0468 - val_accuracy: 0.9909 - val_loss: 0.0219\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9905 - loss: 0.0248 - val_accuracy: 0.9929 - val_loss: 0.0234\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9926 - loss: 0.0206 - val_accuracy: 0.9941 - val_loss: 0.0181\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9943 - loss: 0.0199 - val_accuracy: 0.9961 - val_loss: 0.0145\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9957 - loss: 0.0109 - val_accuracy: 0.9955 - val_loss: 0.0170\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9919 - loss: 0.0255 - val_accuracy: 0.9959 - val_loss: 0.0142\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9964 - loss: 0.0107 - val_accuracy: 0.9959 - val_loss: 0.0136\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9949 - loss: 0.0149 - val_accuracy: 0.9963 - val_loss: 0.0125\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000059.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9962568951930654 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 60\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75D2E8E90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9031 - loss: 1.3451 - val_accuracy: 0.9793 - val_loss: 0.0738\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9741 - loss: 0.0753 - val_accuracy: 0.9894 - val_loss: 0.0420\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9854 - loss: 0.0472 - val_accuracy: 0.9929 - val_loss: 0.0377\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9912 - loss: 0.0301 - val_accuracy: 0.9919 - val_loss: 0.0374\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9928 - loss: 0.0227 - val_accuracy: 0.9921 - val_loss: 0.0359\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9943 - loss: 0.0202 - val_accuracy: 0.9951 - val_loss: 0.0219\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9960 - loss: 0.0139 - val_accuracy: 0.9931 - val_loss: 0.0367\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0123 - val_accuracy: 0.9955 - val_loss: 0.0279\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9963 - loss: 0.0130 - val_accuracy: 0.9943 - val_loss: 0.0379\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9964 - loss: 0.0110 - val_accuracy: 0.9955 - val_loss: 0.0230\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000060.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9954688731284476 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 61\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A77D6E6060>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.8994 - loss: 1.9945 - val_accuracy: 0.9730 - val_loss: 0.0799\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9717 - loss: 0.0793 - val_accuracy: 0.9872 - val_loss: 0.0416\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9846 - loss: 0.0479 - val_accuracy: 0.9935 - val_loss: 0.0244\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9934 - loss: 0.0246 - val_accuracy: 0.9949 - val_loss: 0.0197\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9950 - loss: 0.0171 - val_accuracy: 0.9756 - val_loss: 0.0875\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9961 - loss: 0.0169 - val_accuracy: 0.9970 - val_loss: 0.0130\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9967 - loss: 0.0118 - val_accuracy: 0.9892 - val_loss: 0.0461\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9972 - loss: 0.0104 - val_accuracy: 0.9949 - val_loss: 0.0185\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9963 - loss: 0.0114 - val_accuracy: 0.9961 - val_loss: 0.0169\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9956 - loss: 0.0125 - val_accuracy: 0.9951 - val_loss: 0.0161\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000061.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 62\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A77879C9B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9130 - loss: 1.1794 - val_accuracy: 0.9750 - val_loss: 0.0633\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9748 - loss: 0.0666 - val_accuracy: 0.9921 - val_loss: 0.0305\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9875 - loss: 0.0349 - val_accuracy: 0.9901 - val_loss: 0.0297\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9934 - loss: 0.0235 - val_accuracy: 0.9951 - val_loss: 0.0204\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9955 - loss: 0.0160 - val_accuracy: 0.9911 - val_loss: 0.0330\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9947 - loss: 0.0177 - val_accuracy: 0.9927 - val_loss: 0.0289\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9950 - loss: 0.0157 - val_accuracy: 0.9903 - val_loss: 0.0262\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9950 - loss: 0.0132 - val_accuracy: 0.9957 - val_loss: 0.0190\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9980 - loss: 0.0081 - val_accuracy: 0.9933 - val_loss: 0.0262\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9950 - loss: 0.0134 - val_accuracy: 0.9947 - val_loss: 0.0297\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000062.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 63\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7776DB6E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.9168 - loss: 1.2706 - val_accuracy: 0.9850 - val_loss: 0.0510\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9811 - loss: 0.0556 - val_accuracy: 0.9925 - val_loss: 0.0228\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9920 - loss: 0.0234 - val_accuracy: 0.9923 - val_loss: 0.0250\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9927 - loss: 0.0224 - val_accuracy: 0.9947 - val_loss: 0.0190\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9952 - loss: 0.0160 - val_accuracy: 0.9937 - val_loss: 0.0199\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9947 - loss: 0.0139 - val_accuracy: 0.9955 - val_loss: 0.0111\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9962 - loss: 0.0123 - val_accuracy: 0.9976 - val_loss: 0.0096\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9971 - loss: 0.0082 - val_accuracy: 0.9965 - val_loss: 0.0136\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9948 - loss: 0.0159 - val_accuracy: 0.9976 - val_loss: 0.0102\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - accuracy: 0.9970 - loss: 0.0100 - val_accuracy: 0.9972 - val_loss: 0.0116\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000063.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9972419227738377 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 64\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A77AAD5D90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8975 - loss: 2.6158 - val_accuracy: 0.9762 - val_loss: 0.0676\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9731 - loss: 0.0703 - val_accuracy: 0.9685 - val_loss: 0.0761\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9850 - loss: 0.0410 - val_accuracy: 0.9901 - val_loss: 0.0287\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9918 - loss: 0.0255 - val_accuracy: 0.9909 - val_loss: 0.0275\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9909 - loss: 0.0239 - val_accuracy: 0.9935 - val_loss: 0.0192\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9946 - loss: 0.0164 - val_accuracy: 0.9925 - val_loss: 0.0253\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9949 - loss: 0.0143 - val_accuracy: 0.9931 - val_loss: 0.0221\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9959 - loss: 0.0131 - val_accuracy: 0.9882 - val_loss: 0.0361\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0119 - val_accuracy: 0.9937 - val_loss: 0.0208\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9975 - loss: 0.0090 - val_accuracy: 0.9894 - val_loss: 0.0398\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000064.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9893617021276596 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 65\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A77CF7D7F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9061 - loss: 1.9370 - val_accuracy: 0.9766 - val_loss: 0.0657\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9703 - loss: 0.0755 - val_accuracy: 0.9768 - val_loss: 0.0623\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9797 - loss: 0.0524 - val_accuracy: 0.9923 - val_loss: 0.0272\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9888 - loss: 0.0358 - val_accuracy: 0.9866 - val_loss: 0.0374\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9903 - loss: 0.0292 - val_accuracy: 0.9957 - val_loss: 0.0188\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9943 - loss: 0.0195 - val_accuracy: 0.9929 - val_loss: 0.0217\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9926 - loss: 0.0202 - val_accuracy: 0.9927 - val_loss: 0.0335\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9956 - loss: 0.0148 - val_accuracy: 0.9943 - val_loss: 0.0204\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9954 - loss: 0.0130 - val_accuracy: 0.9588 - val_loss: 0.2172\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9933 - loss: 0.0205 - val_accuracy: 0.9937 - val_loss: 0.0205\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000065.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9936958234830575 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 66\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A77D261970>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9028 - loss: 1.8642 - val_accuracy: 0.9663 - val_loss: 0.0847\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9717 - loss: 0.0753 - val_accuracy: 0.9866 - val_loss: 0.0381\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9840 - loss: 0.0421 - val_accuracy: 0.9890 - val_loss: 0.0307\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.9933 - loss: 0.0198 - val_accuracy: 0.9921 - val_loss: 0.0214\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9949 - loss: 0.0138 - val_accuracy: 0.9929 - val_loss: 0.0202\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9938 - loss: 0.0191 - val_accuracy: 0.9949 - val_loss: 0.0159\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9939 - loss: 0.0175 - val_accuracy: 0.9896 - val_loss: 0.0349\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9974 - loss: 0.0098 - val_accuracy: 0.9947 - val_loss: 0.0150\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9968 - loss: 0.0112 - val_accuracy: 0.9854 - val_loss: 0.0506\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9955 - loss: 0.0146 - val_accuracy: 0.9941 - val_loss: 0.0198\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000066.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9940898345153665 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 67\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A78279C9B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.8970 - loss: 2.5813 - val_accuracy: 0.9673 - val_loss: 0.0836\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9671 - loss: 0.0883 - val_accuracy: 0.9791 - val_loss: 0.0563\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9810 - loss: 0.0528 - val_accuracy: 0.9864 - val_loss: 0.0379\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9902 - loss: 0.0291 - val_accuracy: 0.9919 - val_loss: 0.0237\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9910 - loss: 0.0264 - val_accuracy: 0.9915 - val_loss: 0.0258\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9930 - loss: 0.0200 - val_accuracy: 0.9929 - val_loss: 0.0242\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9943 - loss: 0.0165 - val_accuracy: 0.9898 - val_loss: 0.0331\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9935 - loss: 0.0180 - val_accuracy: 0.9905 - val_loss: 0.0401\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9931 - loss: 0.0237 - val_accuracy: 0.9929 - val_loss: 0.0293\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9963 - loss: 0.0117 - val_accuracy: 0.9949 - val_loss: 0.0217\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000067.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9948778565799843 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 68\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A783A07B30>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.9014 - loss: 3.2651 - val_accuracy: 0.9468 - val_loss: 0.1138\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9723 - loss: 0.0711 - val_accuracy: 0.9868 - val_loss: 0.0342\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9874 - loss: 0.0376 - val_accuracy: 0.9917 - val_loss: 0.0236\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9933 - loss: 0.0215 - val_accuracy: 0.9939 - val_loss: 0.0196\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9936 - loss: 0.0216 - val_accuracy: 0.9862 - val_loss: 0.0492\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9942 - loss: 0.0167 - val_accuracy: 0.9919 - val_loss: 0.0290\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9956 - loss: 0.0126 - val_accuracy: 0.9933 - val_loss: 0.0279\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9960 - loss: 0.0122 - val_accuracy: 0.9937 - val_loss: 0.0173\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9973 - loss: 0.0081 - val_accuracy: 0.9919 - val_loss: 0.0237\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9965 - loss: 0.0098 - val_accuracy: 0.9939 - val_loss: 0.0242\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000068.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 69\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A787E65C10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9085 - loss: 0.9723 - val_accuracy: 0.9846 - val_loss: 0.0462\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9841 - loss: 0.0482 - val_accuracy: 0.9903 - val_loss: 0.0272\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9903 - loss: 0.0267 - val_accuracy: 0.9925 - val_loss: 0.0244\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9949 - loss: 0.0182 - val_accuracy: 0.9953 - val_loss: 0.0137\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9945 - loss: 0.0171 - val_accuracy: 0.9970 - val_loss: 0.0122\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9942 - loss: 0.0187 - val_accuracy: 0.9955 - val_loss: 0.0142\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9979 - loss: 0.0077 - val_accuracy: 0.9961 - val_loss: 0.0145\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9959 - loss: 0.0117 - val_accuracy: 0.9953 - val_loss: 0.0164\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9968 - loss: 0.0100 - val_accuracy: 0.9929 - val_loss: 0.0210\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9961 - loss: 0.0109 - val_accuracy: 0.9789 - val_loss: 0.0960\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000069.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9789204097714737 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 70\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A789088110>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.8931 - loss: 2.3053 - val_accuracy: 0.9781 - val_loss: 0.0745\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9667 - loss: 0.0927 - val_accuracy: 0.9535 - val_loss: 0.1273\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9834 - loss: 0.0567 - val_accuracy: 0.9921 - val_loss: 0.0283\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 0.9884 - loss: 0.0389 - val_accuracy: 0.9955 - val_loss: 0.0174\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9935 - loss: 0.0248 - val_accuracy: 0.9961 - val_loss: 0.0215\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9928 - loss: 0.0208 - val_accuracy: 0.9963 - val_loss: 0.0174\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9898 - loss: 0.0323 - val_accuracy: 0.9943 - val_loss: 0.0289\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9955 - loss: 0.0150 - val_accuracy: 0.9968 - val_loss: 0.0198\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9953 - loss: 0.0151 - val_accuracy: 0.9935 - val_loss: 0.0259\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.9936 - loss: 0.0199 - val_accuracy: 0.9957 - val_loss: 0.0250\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000070.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 71\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7837B6D20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.9107 - loss: 2.5141 - val_accuracy: 0.9870 - val_loss: 0.0454\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9781 - loss: 0.0549 - val_accuracy: 0.9953 - val_loss: 0.0204\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9893 - loss: 0.0311 - val_accuracy: 0.9860 - val_loss: 0.0393\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9928 - loss: 0.0220 - val_accuracy: 0.9972 - val_loss: 0.0114\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9941 - loss: 0.0168 - val_accuracy: 0.9913 - val_loss: 0.0273\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9906 - loss: 0.0262 - val_accuracy: 0.9959 - val_loss: 0.0170\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9928 - loss: 0.0185 - val_accuracy: 0.9949 - val_loss: 0.0211\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9964 - loss: 0.0139 - val_accuracy: 0.9974 - val_loss: 0.0183\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9950 - loss: 0.0157 - val_accuracy: 0.9965 - val_loss: 0.0247\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9969 - loss: 0.0101 - val_accuracy: 0.9965 - val_loss: 0.0255\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000071.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9964539007092199 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 72\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A78F6D6360>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 66ms/step - accuracy: 0.9098 - loss: 1.6332 - val_accuracy: 0.9712 - val_loss: 0.0695\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9713 - loss: 0.0764 - val_accuracy: 0.9892 - val_loss: 0.0288\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9879 - loss: 0.0340 - val_accuracy: 0.9927 - val_loss: 0.0179\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9914 - loss: 0.0226 - val_accuracy: 0.9874 - val_loss: 0.0301\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9925 - loss: 0.0197 - val_accuracy: 0.9943 - val_loss: 0.0164\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9941 - loss: 0.0179 - val_accuracy: 0.9957 - val_loss: 0.0133\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9959 - loss: 0.0130 - val_accuracy: 0.9915 - val_loss: 0.0284\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9940 - loss: 0.0156 - val_accuracy: 0.9907 - val_loss: 0.0310\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9954 - loss: 0.0123 - val_accuracy: 0.9949 - val_loss: 0.0160\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9961 - loss: 0.0095 - val_accuracy: 0.9963 - val_loss: 0.0117\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000072.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9962568951930654 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 73\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A78F71B5C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9196 - loss: 1.6311 - val_accuracy: 0.9807 - val_loss: 0.0525\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9749 - loss: 0.0654 - val_accuracy: 0.9901 - val_loss: 0.0284\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9864 - loss: 0.0365 - val_accuracy: 0.9939 - val_loss: 0.0190\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9913 - loss: 0.0226 - val_accuracy: 0.9957 - val_loss: 0.0120\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9956 - loss: 0.0150 - val_accuracy: 0.9951 - val_loss: 0.0130\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9955 - loss: 0.0122 - val_accuracy: 0.9941 - val_loss: 0.0154\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9950 - loss: 0.0152 - val_accuracy: 0.9951 - val_loss: 0.0156\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9976 - loss: 0.0076 - val_accuracy: 0.9955 - val_loss: 0.0111\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9960 - loss: 0.0128 - val_accuracy: 0.9937 - val_loss: 0.0247\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9974 - loss: 0.0077 - val_accuracy: 0.9974 - val_loss: 0.0078\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000073.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9974389282899921 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 74\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A76C7B3E60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9044 - loss: 1.7070 - val_accuracy: 0.9641 - val_loss: 0.0855\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9774 - loss: 0.0609 - val_accuracy: 0.9864 - val_loss: 0.0425\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9915 - loss: 0.0278 - val_accuracy: 0.9923 - val_loss: 0.0222\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9939 - loss: 0.0183 - val_accuracy: 0.9931 - val_loss: 0.0227\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9949 - loss: 0.0155 - val_accuracy: 0.9959 - val_loss: 0.0119\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9963 - loss: 0.0113 - val_accuracy: 0.9939 - val_loss: 0.0199\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9951 - loss: 0.0143 - val_accuracy: 0.9949 - val_loss: 0.0180\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9963 - loss: 0.0087 - val_accuracy: 0.9967 - val_loss: 0.0142\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9937 - loss: 0.0215 - val_accuracy: 0.9968 - val_loss: 0.0116\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9977 - loss: 0.0076 - val_accuracy: 0.9817 - val_loss: 0.0513\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000074.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9816784869976359 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 75\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A759816870>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8983 - loss: 3.0591 - val_accuracy: 0.9746 - val_loss: 0.0798\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9665 - loss: 0.0836 - val_accuracy: 0.9848 - val_loss: 0.0500\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9834 - loss: 0.0470 - val_accuracy: 0.9896 - val_loss: 0.0333\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9886 - loss: 0.0320 - val_accuracy: 0.9911 - val_loss: 0.0294\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9924 - loss: 0.0221 - val_accuracy: 0.9935 - val_loss: 0.0228\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9940 - loss: 0.0174 - val_accuracy: 0.9949 - val_loss: 0.0270\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9955 - loss: 0.0137 - val_accuracy: 0.9907 - val_loss: 0.0320\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9953 - loss: 0.0152 - val_accuracy: 0.9939 - val_loss: 0.0228\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9977 - loss: 0.0082 - val_accuracy: 0.9927 - val_loss: 0.0255\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0121 - val_accuracy: 0.9923 - val_loss: 0.0304\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000075.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9923167848699763 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 76\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A767E4F6B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9058 - loss: 1.4882 - val_accuracy: 0.9783 - val_loss: 0.0621\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.9706 - loss: 0.0745 - val_accuracy: 0.9909 - val_loss: 0.0292\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9866 - loss: 0.0478 - val_accuracy: 0.9947 - val_loss: 0.0169\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9918 - loss: 0.0277 - val_accuracy: 0.9953 - val_loss: 0.0144\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9949 - loss: 0.0199 - val_accuracy: 0.9963 - val_loss: 0.0110\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9938 - loss: 0.0207 - val_accuracy: 0.9963 - val_loss: 0.0145\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9953 - loss: 0.0158 - val_accuracy: 0.9959 - val_loss: 0.0106\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9972 - loss: 0.0120 - val_accuracy: 0.9957 - val_loss: 0.0202\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0130 - val_accuracy: 0.9935 - val_loss: 0.0200\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9965 - loss: 0.0111 - val_accuracy: 0.9945 - val_loss: 0.0208\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000076.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 77\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75D9132C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.8889 - loss: 3.1873 - val_accuracy: 0.9681 - val_loss: 0.0941\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9571 - loss: 0.1069 - val_accuracy: 0.9704 - val_loss: 0.0816\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9755 - loss: 0.0750 - val_accuracy: 0.9825 - val_loss: 0.0525\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9820 - loss: 0.0572 - val_accuracy: 0.9856 - val_loss: 0.0490\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9901 - loss: 0.0364 - val_accuracy: 0.9866 - val_loss: 0.0628\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9905 - loss: 0.0328 - val_accuracy: 0.9917 - val_loss: 0.0338\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9939 - loss: 0.0223 - val_accuracy: 0.9911 - val_loss: 0.0457\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9933 - loss: 0.0219 - val_accuracy: 0.9935 - val_loss: 0.0278\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9931 - loss: 0.0217 - val_accuracy: 0.9925 - val_loss: 0.0381\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9943 - loss: 0.0186 - val_accuracy: 0.9945 - val_loss: 0.0253\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000077.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 78\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A787FB9700>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9091 - loss: 1.8947 - val_accuracy: 0.9730 - val_loss: 0.0733\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9681 - loss: 0.0804 - val_accuracy: 0.9838 - val_loss: 0.0446\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9813 - loss: 0.0538 - val_accuracy: 0.9898 - val_loss: 0.0319\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9869 - loss: 0.0365 - val_accuracy: 0.9915 - val_loss: 0.0259\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9902 - loss: 0.0282 - val_accuracy: 0.9925 - val_loss: 0.0257\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9939 - loss: 0.0185 - val_accuracy: 0.9886 - val_loss: 0.0407\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9930 - loss: 0.0180 - val_accuracy: 0.9896 - val_loss: 0.0322\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9949 - loss: 0.0158 - val_accuracy: 0.9913 - val_loss: 0.0239\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9968 - loss: 0.0096 - val_accuracy: 0.9923 - val_loss: 0.0312\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9973 - loss: 0.0107 - val_accuracy: 0.9919 - val_loss: 0.0316\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000078.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9919227738376675 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 79\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A77CF7F350>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.9064 - loss: 2.3332 - val_accuracy: 0.9748 - val_loss: 0.0759\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9757 - loss: 0.0615 - val_accuracy: 0.9866 - val_loss: 0.0392\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9891 - loss: 0.0335 - val_accuracy: 0.9939 - val_loss: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9930 - loss: 0.0205 - val_accuracy: 0.9933 - val_loss: 0.0274\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9944 - loss: 0.0135 - val_accuracy: 0.9925 - val_loss: 0.0354\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9959 - loss: 0.0142 - val_accuracy: 0.9943 - val_loss: 0.0255\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9946 - loss: 0.0141 - val_accuracy: 0.9911 - val_loss: 0.0489\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.9944 - loss: 0.0143 - val_accuracy: 0.9947 - val_loss: 0.0252\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9971 - loss: 0.0098 - val_accuracy: 0.9941 - val_loss: 0.0313\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9944 - loss: 0.0159 - val_accuracy: 0.9947 - val_loss: 0.0205\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000079.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 80\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A790EE41A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9082 - loss: 2.4346 - val_accuracy: 0.9805 - val_loss: 0.0542\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9789 - loss: 0.0582 - val_accuracy: 0.9935 - val_loss: 0.0277\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9902 - loss: 0.0260 - val_accuracy: 0.9880 - val_loss: 0.0403\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9930 - loss: 0.0212 - val_accuracy: 0.9941 - val_loss: 0.0225\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9952 - loss: 0.0137 - val_accuracy: 0.9913 - val_loss: 0.0293\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9951 - loss: 0.0151 - val_accuracy: 0.9937 - val_loss: 0.0253\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9943 - loss: 0.0177 - val_accuracy: 0.9919 - val_loss: 0.0309\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9953 - loss: 0.0160 - val_accuracy: 0.9968 - val_loss: 0.0179\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9959 - loss: 0.0127 - val_accuracy: 0.9925 - val_loss: 0.0305\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9958 - loss: 0.0101 - val_accuracy: 0.9945 - val_loss: 0.0217\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000080.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 81\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79A9D8FB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9044 - loss: 2.0728 - val_accuracy: 0.9630 - val_loss: 0.0888\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9710 - loss: 0.0744 - val_accuracy: 0.9835 - val_loss: 0.0491\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9850 - loss: 0.0392 - val_accuracy: 0.9913 - val_loss: 0.0286\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9908 - loss: 0.0284 - val_accuracy: 0.9842 - val_loss: 0.0450\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9923 - loss: 0.0196 - val_accuracy: 0.9919 - val_loss: 0.0234\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9933 - loss: 0.0169 - val_accuracy: 0.9931 - val_loss: 0.0264\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9965 - loss: 0.0121 - val_accuracy: 0.9957 - val_loss: 0.0201\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9983 - loss: 0.0078 - val_accuracy: 0.9945 - val_loss: 0.0247\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9957 - loss: 0.0127 - val_accuracy: 0.9911 - val_loss: 0.0342\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9979 - loss: 0.0089 - val_accuracy: 0.9945 - val_loss: 0.0318\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000081.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 82\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79A9FA030>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8961 - loss: 2.5613 - val_accuracy: 0.9427 - val_loss: 0.1456\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9638 - loss: 0.0951 - val_accuracy: 0.9844 - val_loss: 0.0487\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9815 - loss: 0.0512 - val_accuracy: 0.9864 - val_loss: 0.0457\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9890 - loss: 0.0329 - val_accuracy: 0.9903 - val_loss: 0.0309\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9921 - loss: 0.0218 - val_accuracy: 0.9736 - val_loss: 0.0711\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9924 - loss: 0.0210 - val_accuracy: 0.9919 - val_loss: 0.0368\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9929 - loss: 0.0222 - val_accuracy: 0.9915 - val_loss: 0.0265\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9933 - loss: 0.0193 - val_accuracy: 0.9921 - val_loss: 0.0288\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9944 - loss: 0.0145 - val_accuracy: 0.9884 - val_loss: 0.0414\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9961 - loss: 0.0126 - val_accuracy: 0.9925 - val_loss: 0.0297\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000082.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9925137903861309 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 83\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A19E37A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9060 - loss: 2.2000 - val_accuracy: 0.9777 - val_loss: 0.0582\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9751 - loss: 0.0645 - val_accuracy: 0.9876 - val_loss: 0.0372\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9844 - loss: 0.0465 - val_accuracy: 0.9931 - val_loss: 0.0237\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9921 - loss: 0.0237 - val_accuracy: 0.9919 - val_loss: 0.0259\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9948 - loss: 0.0197 - val_accuracy: 0.9941 - val_loss: 0.0177\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9959 - loss: 0.0140 - val_accuracy: 0.9933 - val_loss: 0.0217\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9955 - loss: 0.0150 - val_accuracy: 0.9945 - val_loss: 0.0217\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9964 - loss: 0.0123 - val_accuracy: 0.9933 - val_loss: 0.0282\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9975 - loss: 0.0081 - val_accuracy: 0.9947 - val_loss: 0.0197\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9972 - loss: 0.0096 - val_accuracy: 0.9949 - val_loss: 0.0241\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000083.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9948778565799843 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 84\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A1AC24B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8977 - loss: 1.2837 - val_accuracy: 0.9732 - val_loss: 0.0878\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9678 - loss: 0.0951 - val_accuracy: 0.9840 - val_loss: 0.0606\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9848 - loss: 0.0578 - val_accuracy: 0.9933 - val_loss: 0.0257\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9894 - loss: 0.0376 - val_accuracy: 0.9878 - val_loss: 0.0415\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9920 - loss: 0.0254 - val_accuracy: 0.9907 - val_loss: 0.0369\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9904 - loss: 0.0264 - val_accuracy: 0.9939 - val_loss: 0.0249\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9926 - loss: 0.0220 - val_accuracy: 0.9965 - val_loss: 0.0162\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9928 - loss: 0.0194 - val_accuracy: 0.9943 - val_loss: 0.0279\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9940 - loss: 0.0183 - val_accuracy: 0.9925 - val_loss: 0.0265\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9931 - loss: 0.0185 - val_accuracy: 0.9925 - val_loss: 0.0445\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000084.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9925137903861309 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 85\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79A72F6B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9074 - loss: 1.6282 - val_accuracy: 0.9803 - val_loss: 0.0729\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9791 - loss: 0.0552 - val_accuracy: 0.9886 - val_loss: 0.0375\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9862 - loss: 0.0375 - val_accuracy: 0.9943 - val_loss: 0.0230\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9919 - loss: 0.0231 - val_accuracy: 0.9939 - val_loss: 0.0226\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9926 - loss: 0.0227 - val_accuracy: 0.9896 - val_loss: 0.0391\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9941 - loss: 0.0152 - val_accuracy: 0.9935 - val_loss: 0.0293\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9947 - loss: 0.0171 - val_accuracy: 0.9959 - val_loss: 0.0215\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9946 - loss: 0.0141 - val_accuracy: 0.9941 - val_loss: 0.0282\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9953 - loss: 0.0161 - val_accuracy: 0.9923 - val_loss: 0.0440\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9961 - loss: 0.0107 - val_accuracy: 0.9953 - val_loss: 0.0322\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000085.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 86\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A767EC5790>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.8972 - loss: 2.9343 - val_accuracy: 0.9716 - val_loss: 0.0756\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9645 - loss: 0.0891 - val_accuracy: 0.9803 - val_loss: 0.0618\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9800 - loss: 0.0548 - val_accuracy: 0.9874 - val_loss: 0.0366\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9885 - loss: 0.0311 - val_accuracy: 0.9907 - val_loss: 0.0255\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9933 - loss: 0.0204 - val_accuracy: 0.9925 - val_loss: 0.0209\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9945 - loss: 0.0174 - val_accuracy: 0.9921 - val_loss: 0.0200\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9961 - loss: 0.0120 - val_accuracy: 0.9941 - val_loss: 0.0197\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9946 - loss: 0.0132 - val_accuracy: 0.9935 - val_loss: 0.0194\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9942 - loss: 0.0204 - val_accuracy: 0.9911 - val_loss: 0.0322\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9962 - loss: 0.0121 - val_accuracy: 0.9963 - val_loss: 0.0106\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000086.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9962568951930654 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 87\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A2281070>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9033 - loss: 1.8443 - val_accuracy: 0.9576 - val_loss: 0.0987\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9612 - loss: 0.0984 - val_accuracy: 0.9840 - val_loss: 0.0453\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.9802 - loss: 0.0558 - val_accuracy: 0.9913 - val_loss: 0.0287\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9887 - loss: 0.0358 - val_accuracy: 0.9850 - val_loss: 0.0384\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9891 - loss: 0.0315 - val_accuracy: 0.9955 - val_loss: 0.0194\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9917 - loss: 0.0260 - val_accuracy: 0.9953 - val_loss: 0.0174\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9943 - loss: 0.0176 - val_accuracy: 0.9921 - val_loss: 0.0313\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9930 - loss: 0.0219 - val_accuracy: 0.9900 - val_loss: 0.0327\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9940 - loss: 0.0179 - val_accuracy: 0.9967 - val_loss: 0.0147\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9947 - loss: 0.0149 - val_accuracy: 0.9955 - val_loss: 0.0179\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000087.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9954688731284476 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 88\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A462BE60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - accuracy: 0.9078 - loss: 2.2261 - val_accuracy: 0.9764 - val_loss: 0.0630\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9776 - loss: 0.0613 - val_accuracy: 0.9872 - val_loss: 0.0349\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - accuracy: 0.9877 - loss: 0.0355 - val_accuracy: 0.9888 - val_loss: 0.0297\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9914 - loss: 0.0241 - val_accuracy: 0.9949 - val_loss: 0.0152\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9950 - loss: 0.0143 - val_accuracy: 0.9925 - val_loss: 0.0228\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9962 - loss: 0.0116 - val_accuracy: 0.9858 - val_loss: 0.0579\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9946 - loss: 0.0166 - val_accuracy: 0.9941 - val_loss: 0.0244\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9977 - loss: 0.0081 - val_accuracy: 0.9945 - val_loss: 0.0171\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9972 - loss: 0.0085 - val_accuracy: 0.9901 - val_loss: 0.0326\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9975 - loss: 0.0097 - val_accuracy: 0.9953 - val_loss: 0.0190\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000088.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 89\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7F3CA66F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9011 - loss: 1.9679 - val_accuracy: 0.9730 - val_loss: 0.0743\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9702 - loss: 0.0824 - val_accuracy: 0.9884 - val_loss: 0.0466\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9874 - loss: 0.0494 - val_accuracy: 0.9931 - val_loss: 0.0311\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9926 - loss: 0.0346 - val_accuracy: 0.9959 - val_loss: 0.0178\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9945 - loss: 0.0225 - val_accuracy: 0.9972 - val_loss: 0.0173\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9949 - loss: 0.0204 - val_accuracy: 0.9967 - val_loss: 0.0173\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9958 - loss: 0.0153 - val_accuracy: 0.9957 - val_loss: 0.0232\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9918 - loss: 0.0267 - val_accuracy: 0.9970 - val_loss: 0.0180\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9939 - loss: 0.0194 - val_accuracy: 0.9961 - val_loss: 0.0236\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9966 - loss: 0.0115 - val_accuracy: 0.9970 - val_loss: 0.0181\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000089.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9970449172576832 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 90\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A211AF30>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 63ms/step - accuracy: 0.9058 - loss: 1.4921 - val_accuracy: 0.9783 - val_loss: 0.0622\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9768 - loss: 0.0607 - val_accuracy: 0.9919 - val_loss: 0.0324\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9877 - loss: 0.0343 - val_accuracy: 0.9900 - val_loss: 0.0409\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9915 - loss: 0.0221 - val_accuracy: 0.9905 - val_loss: 0.0317\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9941 - loss: 0.0186 - val_accuracy: 0.9921 - val_loss: 0.0292\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9940 - loss: 0.0193 - val_accuracy: 0.9872 - val_loss: 0.0387\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9937 - loss: 0.0196 - val_accuracy: 0.9911 - val_loss: 0.0380\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9955 - loss: 0.0125 - val_accuracy: 0.9943 - val_loss: 0.0317\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9960 - loss: 0.0116 - val_accuracy: 0.9943 - val_loss: 0.0255\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9957 - loss: 0.0126 - val_accuracy: 0.9939 - val_loss: 0.0258\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000090.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 91\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A193CC80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9073 - loss: 2.1328 - val_accuracy: 0.9748 - val_loss: 0.0678\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9748 - loss: 0.0656 - val_accuracy: 0.9901 - val_loss: 0.0340\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9897 - loss: 0.0307 - val_accuracy: 0.9888 - val_loss: 0.0384\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9925 - loss: 0.0207 - val_accuracy: 0.9949 - val_loss: 0.0215\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9943 - loss: 0.0174 - val_accuracy: 0.9921 - val_loss: 0.0256\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9929 - loss: 0.0179 - val_accuracy: 0.9813 - val_loss: 0.1076\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9947 - loss: 0.0197 - val_accuracy: 0.9959 - val_loss: 0.0221\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9968 - loss: 0.0111 - val_accuracy: 0.9949 - val_loss: 0.0173\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9946 - loss: 0.0176 - val_accuracy: 0.9939 - val_loss: 0.0242\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9958 - loss: 0.0107 - val_accuracy: 0.9915 - val_loss: 0.0508\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000091.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9915287628053585 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 92\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A790DD6990>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9070 - loss: 1.2055 - val_accuracy: 0.9773 - val_loss: 0.0603\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9751 - loss: 0.0672 - val_accuracy: 0.9907 - val_loss: 0.0295\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9859 - loss: 0.0409 - val_accuracy: 0.9905 - val_loss: 0.0259\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9915 - loss: 0.0252 - val_accuracy: 0.9901 - val_loss: 0.0296\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9921 - loss: 0.0269 - val_accuracy: 0.9933 - val_loss: 0.0242\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9941 - loss: 0.0169 - val_accuracy: 0.9961 - val_loss: 0.0142\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9955 - loss: 0.0144 - val_accuracy: 0.9963 - val_loss: 0.0163\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9899 - loss: 0.0303 - val_accuracy: 0.9961 - val_loss: 0.0168\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9944 - loss: 0.0167 - val_accuracy: 0.9951 - val_loss: 0.0149\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9972 - loss: 0.0081 - val_accuracy: 0.9974 - val_loss: 0.0101\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000092.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9974389282899921 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 93\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75EF9AED0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.9129 - loss: 2.5856 - val_accuracy: 0.9836 - val_loss: 0.0492\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9806 - loss: 0.0499 - val_accuracy: 0.9815 - val_loss: 0.0538\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0333 - val_accuracy: 0.9939 - val_loss: 0.0208\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9945 - loss: 0.0176 - val_accuracy: 0.9939 - val_loss: 0.0285\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9956 - loss: 0.0142 - val_accuracy: 0.9968 - val_loss: 0.0170\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9951 - loss: 0.0141 - val_accuracy: 0.9965 - val_loss: 0.0168\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9956 - loss: 0.0135 - val_accuracy: 0.9972 - val_loss: 0.0154\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9970 - loss: 0.0093 - val_accuracy: 0.9949 - val_loss: 0.0178\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9960 - loss: 0.0123 - val_accuracy: 0.9967 - val_loss: 0.0154\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9970 - loss: 0.0087 - val_accuracy: 0.9961 - val_loss: 0.0242\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000093.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.996059889676911 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 94\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79A927950>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9068 - loss: 1.9032 - val_accuracy: 0.9793 - val_loss: 0.0646\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9762 - loss: 0.0615 - val_accuracy: 0.9907 - val_loss: 0.0290\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9912 - loss: 0.0286 - val_accuracy: 0.9953 - val_loss: 0.0184\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9929 - loss: 0.0199 - val_accuracy: 0.9963 - val_loss: 0.0145\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9941 - loss: 0.0188 - val_accuracy: 0.9933 - val_loss: 0.0269\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9930 - loss: 0.0178 - val_accuracy: 0.9968 - val_loss: 0.0135\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9965 - loss: 0.0118 - val_accuracy: 0.9935 - val_loss: 0.0281\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9968 - loss: 0.0094 - val_accuracy: 0.9947 - val_loss: 0.0198\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9962 - loss: 0.0131 - val_accuracy: 0.9949 - val_loss: 0.0260\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9959 - loss: 0.0117 - val_accuracy: 0.9970 - val_loss: 0.0106\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000094.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9970449172576832 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 95\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A78F863D70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9056 - loss: 1.4603 - val_accuracy: 0.9811 - val_loss: 0.0631\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9732 - loss: 0.0699 - val_accuracy: 0.9882 - val_loss: 0.0421\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9876 - loss: 0.0357 - val_accuracy: 0.9941 - val_loss: 0.0248\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9914 - loss: 0.0270 - val_accuracy: 0.9921 - val_loss: 0.0330\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9929 - loss: 0.0202 - val_accuracy: 0.9933 - val_loss: 0.0338\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9944 - loss: 0.0172 - val_accuracy: 0.9913 - val_loss: 0.0344\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9940 - loss: 0.0192 - val_accuracy: 0.9959 - val_loss: 0.0236\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9963 - loss: 0.0120 - val_accuracy: 0.9961 - val_loss: 0.0182\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9979 - loss: 0.0080 - val_accuracy: 0.9961 - val_loss: 0.0278\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9969 - loss: 0.0108 - val_accuracy: 0.9931 - val_loss: 0.0415\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000095.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9931048069345941 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 96\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7F3D82E40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9007 - loss: 1.8822 - val_accuracy: 0.9811 - val_loss: 0.0625\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9736 - loss: 0.0711 - val_accuracy: 0.9876 - val_loss: 0.0364\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9895 - loss: 0.0316 - val_accuracy: 0.9957 - val_loss: 0.0140\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9929 - loss: 0.0215 - val_accuracy: 0.9941 - val_loss: 0.0177\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9940 - loss: 0.0190 - val_accuracy: 0.9921 - val_loss: 0.0290\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9937 - loss: 0.0179 - val_accuracy: 0.9947 - val_loss: 0.0161\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9948 - loss: 0.0165 - val_accuracy: 0.9967 - val_loss: 0.0132\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9973 - loss: 0.0101 - val_accuracy: 0.9955 - val_loss: 0.0150\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9969 - loss: 0.0101 - val_accuracy: 0.9949 - val_loss: 0.0163\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9969 - loss: 0.0079 - val_accuracy: 0.9961 - val_loss: 0.0171\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000096.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.996059889676911 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 97\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7A20C7320>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8976 - loss: 3.5002 - val_accuracy: 0.9563 - val_loss: 0.1025\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9721 - loss: 0.0730 - val_accuracy: 0.9858 - val_loss: 0.0385\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9843 - loss: 0.0385 - val_accuracy: 0.9850 - val_loss: 0.0409\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9940 - loss: 0.0169 - val_accuracy: 0.9900 - val_loss: 0.0319\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9948 - loss: 0.0144 - val_accuracy: 0.9941 - val_loss: 0.0196\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9932 - loss: 0.0171 - val_accuracy: 0.9939 - val_loss: 0.0179\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9963 - loss: 0.0126 - val_accuracy: 0.9941 - val_loss: 0.0184\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9970 - loss: 0.0085 - val_accuracy: 0.9941 - val_loss: 0.0195\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9979 - loss: 0.0067 - val_accuracy: 0.9874 - val_loss: 0.0582\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9962 - loss: 0.0112 - val_accuracy: 0.9868 - val_loss: 0.0514\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000097.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9868006304176516 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 98\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7F6C54FE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8929 - loss: 2.7606 - val_accuracy: 0.9655 - val_loss: 0.0866\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9643 - loss: 0.0880 - val_accuracy: 0.9905 - val_loss: 0.0476\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9840 - loss: 0.0461 - val_accuracy: 0.9945 - val_loss: 0.0228\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9872 - loss: 0.0366 - val_accuracy: 0.9949 - val_loss: 0.0188\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9933 - loss: 0.0253 - val_accuracy: 0.9943 - val_loss: 0.0181\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9914 - loss: 0.0270 - val_accuracy: 0.9965 - val_loss: 0.0167\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9945 - loss: 0.0168 - val_accuracy: 0.9937 - val_loss: 0.0261\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9922 - loss: 0.0209 - val_accuracy: 0.9945 - val_loss: 0.0163\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9948 - loss: 0.0173 - val_accuracy: 0.9955 - val_loss: 0.0184\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9968 - loss: 0.0097 - val_accuracy: 0.9959 - val_loss: 0.0204\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000098.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 99\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79ADD9CA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8988 - loss: 1.9547 - val_accuracy: 0.9781 - val_loss: 0.0618\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9741 - loss: 0.0709 - val_accuracy: 0.9896 - val_loss: 0.0401\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9898 - loss: 0.0327 - val_accuracy: 0.9894 - val_loss: 0.0340\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9892 - loss: 0.0309 - val_accuracy: 0.9935 - val_loss: 0.0207\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9927 - loss: 0.0237 - val_accuracy: 0.9862 - val_loss: 0.0320\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9944 - loss: 0.0183 - val_accuracy: 0.9945 - val_loss: 0.0156\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9964 - loss: 0.0140 - val_accuracy: 0.9903 - val_loss: 0.0267\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9952 - loss: 0.0158 - val_accuracy: 0.9876 - val_loss: 0.0422\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9959 - loss: 0.0137 - val_accuracy: 0.9963 - val_loss: 0.0178\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9974 - loss: 0.0079 - val_accuracy: 0.9933 - val_loss: 0.0285\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000099.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9933018124507487 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 100\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7F704AD50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.9073 - loss: 1.8153 - val_accuracy: 0.9703 - val_loss: 0.0724\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9729 - loss: 0.0724 - val_accuracy: 0.9878 - val_loss: 0.0416\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9840 - loss: 0.0507 - val_accuracy: 0.9835 - val_loss: 0.0430\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9865 - loss: 0.0404 - val_accuracy: 0.9927 - val_loss: 0.0212\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9914 - loss: 0.0272 - val_accuracy: 0.9894 - val_loss: 0.0314\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9952 - loss: 0.0177 - val_accuracy: 0.9876 - val_loss: 0.0304\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9930 - loss: 0.0226 - val_accuracy: 0.9968 - val_loss: 0.0121\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9939 - loss: 0.0188 - val_accuracy: 0.9959 - val_loss: 0.0157\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9948 - loss: 0.0167 - val_accuracy: 0.9909 - val_loss: 0.0204\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9937 - loss: 0.0170 - val_accuracy: 0.9947 - val_loss: 0.0233\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000100.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 101\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7F711EC60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.9028 - loss: 3.0134 - val_accuracy: 0.9768 - val_loss: 0.0636\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9770 - loss: 0.0632 - val_accuracy: 0.9862 - val_loss: 0.0386\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9879 - loss: 0.0353 - val_accuracy: 0.9903 - val_loss: 0.0312\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9925 - loss: 0.0225 - val_accuracy: 0.9915 - val_loss: 0.0275\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9926 - loss: 0.0214 - val_accuracy: 0.9900 - val_loss: 0.0291\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9949 - loss: 0.0153 - val_accuracy: 0.9907 - val_loss: 0.0251\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9955 - loss: 0.0133 - val_accuracy: 0.9915 - val_loss: 0.0268\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9942 - loss: 0.0154 - val_accuracy: 0.9905 - val_loss: 0.0299\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9962 - loss: 0.0102 - val_accuracy: 0.9929 - val_loss: 0.0251\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9957 - loss: 0.0123 - val_accuracy: 0.9927 - val_loss: 0.0252\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000101.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9927107959022853 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 102\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A5263EF0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9166 - loss: 1.1080 - val_accuracy: 0.9771 - val_loss: 0.0686\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9720 - loss: 0.0712 - val_accuracy: 0.9943 - val_loss: 0.0223\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9867 - loss: 0.0366 - val_accuracy: 0.9945 - val_loss: 0.0175\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9930 - loss: 0.0236 - val_accuracy: 0.9959 - val_loss: 0.0170\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9940 - loss: 0.0187 - val_accuracy: 0.9959 - val_loss: 0.0134\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9944 - loss: 0.0172 - val_accuracy: 0.9976 - val_loss: 0.0135\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9961 - loss: 0.0120 - val_accuracy: 0.9935 - val_loss: 0.0232\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9971 - loss: 0.0089 - val_accuracy: 0.9980 - val_loss: 0.0144\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9960 - loss: 0.0129 - val_accuracy: 0.9915 - val_loss: 0.0429\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9962 - loss: 0.0135 - val_accuracy: 0.9921 - val_loss: 0.0283\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000102.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9921197793538219 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 103\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A5263D40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8960 - loss: 1.4617 - val_accuracy: 0.9693 - val_loss: 0.1044\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9576 - loss: 0.1211 - val_accuracy: 0.9833 - val_loss: 0.0796\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9779 - loss: 0.0771 - val_accuracy: 0.9884 - val_loss: 0.0585\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9857 - loss: 0.0533 - val_accuracy: 0.9919 - val_loss: 0.0418\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9907 - loss: 0.0381 - val_accuracy: 0.9943 - val_loss: 0.0438\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9925 - loss: 0.0272 - val_accuracy: 0.9961 - val_loss: 0.0321\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9924 - loss: 0.0254 - val_accuracy: 0.9921 - val_loss: 0.0512\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9925 - loss: 0.0230 - val_accuracy: 0.9933 - val_loss: 0.0394\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9920 - loss: 0.0249 - val_accuracy: 0.9921 - val_loss: 0.0627\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9961 - loss: 0.0148 - val_accuracy: 0.9911 - val_loss: 0.0765\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000103.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9911347517730497 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 104\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A55FA3F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9054 - loss: 1.8055 - val_accuracy: 0.9797 - val_loss: 0.0641\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9706 - loss: 0.0774 - val_accuracy: 0.9903 - val_loss: 0.0308\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9850 - loss: 0.0433 - val_accuracy: 0.9907 - val_loss: 0.0252\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9907 - loss: 0.0287 - val_accuracy: 0.9927 - val_loss: 0.0210\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9931 - loss: 0.0241 - val_accuracy: 0.9925 - val_loss: 0.0218\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9924 - loss: 0.0226 - val_accuracy: 0.9905 - val_loss: 0.0323\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9953 - loss: 0.0145 - val_accuracy: 0.9963 - val_loss: 0.0136\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9944 - loss: 0.0171 - val_accuracy: 0.9961 - val_loss: 0.0116\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9939 - loss: 0.0157 - val_accuracy: 0.9963 - val_loss: 0.0142\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9940 - loss: 0.0192 - val_accuracy: 0.9965 - val_loss: 0.0147\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000104.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9964539007092199 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 105\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A5BF29F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9140 - loss: 0.8642 - val_accuracy: 0.9730 - val_loss: 0.0641\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9756 - loss: 0.0574 - val_accuracy: 0.9937 - val_loss: 0.0186\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9882 - loss: 0.0324 - val_accuracy: 0.9980 - val_loss: 0.0118\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9896 - loss: 0.0259 - val_accuracy: 0.9968 - val_loss: 0.0121\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9931 - loss: 0.0198 - val_accuracy: 0.9965 - val_loss: 0.0132\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9927 - loss: 0.0221 - val_accuracy: 0.9953 - val_loss: 0.0148\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9966 - loss: 0.0108 - val_accuracy: 0.9929 - val_loss: 0.0248\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9963 - loss: 0.0135 - val_accuracy: 0.9974 - val_loss: 0.0089\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9944 - loss: 0.0160 - val_accuracy: 0.9967 - val_loss: 0.0165\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9968 - loss: 0.0096 - val_accuracy: 0.9968 - val_loss: 0.0202\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000105.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9968479117415288 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 106\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A59EE420>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.9006 - loss: 1.4510 - val_accuracy: 0.9760 - val_loss: 0.0777\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9676 - loss: 0.0901 - val_accuracy: 0.9758 - val_loss: 0.0632\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9779 - loss: 0.0642 - val_accuracy: 0.9907 - val_loss: 0.0369\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9905 - loss: 0.0398 - val_accuracy: 0.9931 - val_loss: 0.0274\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9900 - loss: 0.0360 - val_accuracy: 0.9951 - val_loss: 0.0198\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9919 - loss: 0.0253 - val_accuracy: 0.9872 - val_loss: 0.0497\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9894 - loss: 0.0316 - val_accuracy: 0.9931 - val_loss: 0.0211\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9962 - loss: 0.0181 - val_accuracy: 0.9947 - val_loss: 0.0270\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9968 - loss: 0.0130 - val_accuracy: 0.9951 - val_loss: 0.0171\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9964 - loss: 0.0121 - val_accuracy: 0.9935 - val_loss: 0.0352\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000106.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9934988179669031 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 107\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8109743B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8973 - loss: 2.1283 - val_accuracy: 0.9734 - val_loss: 0.0893\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9721 - loss: 0.0727 - val_accuracy: 0.9909 - val_loss: 0.0287\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9871 - loss: 0.0353 - val_accuracy: 0.9860 - val_loss: 0.0357\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9905 - loss: 0.0226 - val_accuracy: 0.9935 - val_loss: 0.0201\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9933 - loss: 0.0183 - val_accuracy: 0.9955 - val_loss: 0.0222\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9968 - loss: 0.0100 - val_accuracy: 0.9935 - val_loss: 0.0185\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9941 - loss: 0.0156 - val_accuracy: 0.9953 - val_loss: 0.0185\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9955 - loss: 0.0131 - val_accuracy: 0.9957 - val_loss: 0.0173\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9965 - loss: 0.0113 - val_accuracy: 0.9955 - val_loss: 0.0219\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9984 - loss: 0.0059 - val_accuracy: 0.9955 - val_loss: 0.0193\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000107.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9954688731284476 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 108\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A51AB9B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.9231 - loss: 0.8029 - val_accuracy: 0.9833 - val_loss: 0.0478\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9840 - loss: 0.0449 - val_accuracy: 0.9921 - val_loss: 0.0268\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9920 - loss: 0.0275 - val_accuracy: 0.9959 - val_loss: 0.0173\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9942 - loss: 0.0159 - val_accuracy: 0.9929 - val_loss: 0.0224\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9964 - loss: 0.0135 - val_accuracy: 0.9949 - val_loss: 0.0165\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9971 - loss: 0.0100 - val_accuracy: 0.9963 - val_loss: 0.0141\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9961 - loss: 0.0107 - val_accuracy: 0.9943 - val_loss: 0.0204\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9946 - loss: 0.0151 - val_accuracy: 0.9967 - val_loss: 0.0124\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9975 - loss: 0.0073 - val_accuracy: 0.9959 - val_loss: 0.0153\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9976 - loss: 0.0085 - val_accuracy: 0.9937 - val_loss: 0.0358\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000108.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9936958234830575 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 109\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A5976060>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.9128 - loss: 1.5656 - val_accuracy: 0.9770 - val_loss: 0.0665\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9810 - loss: 0.0554 - val_accuracy: 0.9880 - val_loss: 0.0360\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9922 - loss: 0.0225 - val_accuracy: 0.9909 - val_loss: 0.0322\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9955 - loss: 0.0140 - val_accuracy: 0.9935 - val_loss: 0.0205\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9972 - loss: 0.0085 - val_accuracy: 0.9911 - val_loss: 0.0286\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9963 - loss: 0.0113 - val_accuracy: 0.9900 - val_loss: 0.0331\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9946 - loss: 0.0160 - val_accuracy: 0.9901 - val_loss: 0.0393\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9967 - loss: 0.0090 - val_accuracy: 0.9923 - val_loss: 0.0421\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9933 - loss: 0.0177 - val_accuracy: 0.9933 - val_loss: 0.0304\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9981 - loss: 0.0039 - val_accuracy: 0.9935 - val_loss: 0.0368\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000109.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9934988179669031 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 110\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8161EAA80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9045 - loss: 2.1417 - val_accuracy: 0.9565 - val_loss: 0.0896\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9810 - loss: 0.0570 - val_accuracy: 0.9955 - val_loss: 0.0193\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9898 - loss: 0.0289 - val_accuracy: 0.9963 - val_loss: 0.0129\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9931 - loss: 0.0193 - val_accuracy: 0.9929 - val_loss: 0.0218\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9939 - loss: 0.0211 - val_accuracy: 0.9937 - val_loss: 0.0216\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 0.9968 - val_loss: 0.0117\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9933 - loss: 0.0232 - val_accuracy: 0.9959 - val_loss: 0.0168\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9969 - loss: 0.0085 - val_accuracy: 0.9970 - val_loss: 0.0097\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9973 - loss: 0.0080 - val_accuracy: 0.9967 - val_loss: 0.0107\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9971 - loss: 0.0086 - val_accuracy: 0.9955 - val_loss: 0.0166\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000110.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9954688731284476 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 111\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A81854F4A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8894 - loss: 2.0352 - val_accuracy: 0.9630 - val_loss: 0.1089\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9617 - loss: 0.1018 - val_accuracy: 0.9872 - val_loss: 0.0386\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9800 - loss: 0.0606 - val_accuracy: 0.9915 - val_loss: 0.0311\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9853 - loss: 0.0466 - val_accuracy: 0.9939 - val_loss: 0.0243\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9907 - loss: 0.0321 - val_accuracy: 0.9963 - val_loss: 0.0180\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9914 - loss: 0.0291 - val_accuracy: 0.9933 - val_loss: 0.0214\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9877 - loss: 0.0337 - val_accuracy: 0.9907 - val_loss: 0.0394\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9918 - loss: 0.0252 - val_accuracy: 0.9951 - val_loss: 0.0230\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9926 - loss: 0.0253 - val_accuracy: 0.9961 - val_loss: 0.0191\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9942 - loss: 0.0184 - val_accuracy: 0.9917 - val_loss: 0.0348\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000111.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.991725768321513 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 112\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A818489E50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 63ms/step - accuracy: 0.8900 - loss: 2.4850 - val_accuracy: 0.9612 - val_loss: 0.1002\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9611 - loss: 0.0909 - val_accuracy: 0.9823 - val_loss: 0.0526\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9789 - loss: 0.0507 - val_accuracy: 0.9943 - val_loss: 0.0223\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9860 - loss: 0.0406 - val_accuracy: 0.9905 - val_loss: 0.0285\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9860 - loss: 0.0350 - val_accuracy: 0.9864 - val_loss: 0.0441\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9915 - loss: 0.0269 - val_accuracy: 0.9903 - val_loss: 0.0351\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9891 - loss: 0.0313 - val_accuracy: 0.9898 - val_loss: 0.0429\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9896 - loss: 0.0276 - val_accuracy: 0.9931 - val_loss: 0.0280\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9915 - loss: 0.0239 - val_accuracy: 0.9919 - val_loss: 0.0259\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9917 - loss: 0.0242 - val_accuracy: 0.9955 - val_loss: 0.0242\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000112.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9954688731284476 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 113\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A81B7BDB50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.8986 - loss: 2.9107 - val_accuracy: 0.9842 - val_loss: 0.0541\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9769 - loss: 0.0647 - val_accuracy: 0.9858 - val_loss: 0.0447\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9884 - loss: 0.0358 - val_accuracy: 0.9941 - val_loss: 0.0191\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.9933 - loss: 0.0215 - val_accuracy: 0.9937 - val_loss: 0.0242\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9951 - loss: 0.0161 - val_accuracy: 0.9961 - val_loss: 0.0133\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9951 - loss: 0.0171 - val_accuracy: 0.9965 - val_loss: 0.0127\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9955 - val_loss: 0.0160\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9966 - loss: 0.0112 - val_accuracy: 0.9955 - val_loss: 0.0156\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9974 - loss: 0.0101 - val_accuracy: 0.9965 - val_loss: 0.0130\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9959 - loss: 0.0130 - val_accuracy: 0.9963 - val_loss: 0.0110\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000113.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9962568951930654 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 114\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A54A9A90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9117 - loss: 1.3893 - val_accuracy: 0.9779 - val_loss: 0.0575\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9762 - loss: 0.0627 - val_accuracy: 0.9915 - val_loss: 0.0305\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9856 - loss: 0.0387 - val_accuracy: 0.9935 - val_loss: 0.0236\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9916 - loss: 0.0257 - val_accuracy: 0.9957 - val_loss: 0.0145\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9925 - loss: 0.0218 - val_accuracy: 0.9951 - val_loss: 0.0181\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9947 - loss: 0.0178 - val_accuracy: 0.9929 - val_loss: 0.0217\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9942 - loss: 0.0167 - val_accuracy: 0.9965 - val_loss: 0.0125\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9956 - loss: 0.0136 - val_accuracy: 0.9976 - val_loss: 0.0080\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9965 - loss: 0.0091 - val_accuracy: 0.9907 - val_loss: 0.0419\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9971 - loss: 0.0092 - val_accuracy: 0.9872 - val_loss: 0.0699\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000114.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9871946414499606 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 115\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75DBE6360>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9183 - loss: 0.7647 - val_accuracy: 0.9817 - val_loss: 0.0559\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9798 - loss: 0.0555 - val_accuracy: 0.9827 - val_loss: 0.0490\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9884 - loss: 0.0344 - val_accuracy: 0.9927 - val_loss: 0.0222\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9921 - loss: 0.0215 - val_accuracy: 0.9937 - val_loss: 0.0213\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9933 - loss: 0.0188 - val_accuracy: 0.9949 - val_loss: 0.0193\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9948 - loss: 0.0145 - val_accuracy: 0.9929 - val_loss: 0.0254\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9942 - loss: 0.0152 - val_accuracy: 0.9921 - val_loss: 0.0287\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9961 - loss: 0.0111 - val_accuracy: 0.9931 - val_loss: 0.0408\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9962 - loss: 0.0105 - val_accuracy: 0.9884 - val_loss: 0.0462\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9950 - loss: 0.0150 - val_accuracy: 0.9886 - val_loss: 0.0492\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000115.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9885736800630418 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 116\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7700AF6B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9092 - loss: 2.2014 - val_accuracy: 0.9785 - val_loss: 0.0643\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9783 - loss: 0.0621 - val_accuracy: 0.9907 - val_loss: 0.0283\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9900 - loss: 0.0285 - val_accuracy: 0.9866 - val_loss: 0.0352\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9929 - loss: 0.0222 - val_accuracy: 0.9939 - val_loss: 0.0199\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9947 - loss: 0.0165 - val_accuracy: 0.9941 - val_loss: 0.0209\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9963 - loss: 0.0120 - val_accuracy: 0.9957 - val_loss: 0.0191\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9963 - loss: 0.0114 - val_accuracy: 0.9925 - val_loss: 0.0364\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9957 - loss: 0.0120 - val_accuracy: 0.9943 - val_loss: 0.0206\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9964 - loss: 0.0104 - val_accuracy: 0.9929 - val_loss: 0.0203\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9966 - loss: 0.0101 - val_accuracy: 0.9951 - val_loss: 0.0194\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000116.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 117\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8A5A528D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9017 - loss: 2.9174 - val_accuracy: 0.9632 - val_loss: 0.0946\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9599 - loss: 0.1017 - val_accuracy: 0.9779 - val_loss: 0.0575\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9775 - loss: 0.0585 - val_accuracy: 0.9900 - val_loss: 0.0313\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9860 - loss: 0.0376 - val_accuracy: 0.9919 - val_loss: 0.0266\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9918 - loss: 0.0237 - val_accuracy: 0.9710 - val_loss: 0.0747\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9924 - loss: 0.0229 - val_accuracy: 0.9951 - val_loss: 0.0184\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9947 - loss: 0.0152 - val_accuracy: 0.9919 - val_loss: 0.0265\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9952 - loss: 0.0145 - val_accuracy: 0.9953 - val_loss: 0.0152\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9965 - loss: 0.0095 - val_accuracy: 0.9947 - val_loss: 0.0179\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9950 - loss: 0.0151 - val_accuracy: 0.9949 - val_loss: 0.0148\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000117.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9948778565799843 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 118\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8185D4C20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.8993 - loss: 1.1832 - val_accuracy: 0.9827 - val_loss: 0.0604\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9756 - loss: 0.0672 - val_accuracy: 0.9907 - val_loss: 0.0299\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9887 - loss: 0.0369 - val_accuracy: 0.9939 - val_loss: 0.0178\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9915 - loss: 0.0281 - val_accuracy: 0.9913 - val_loss: 0.0392\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9936 - loss: 0.0225 - val_accuracy: 0.9943 - val_loss: 0.0147\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9964 - loss: 0.0152 - val_accuracy: 0.9901 - val_loss: 0.0279\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9933 - loss: 0.0241 - val_accuracy: 0.9941 - val_loss: 0.0180\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9971 - loss: 0.0100 - val_accuracy: 0.9935 - val_loss: 0.0185\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9966 - loss: 0.0111 - val_accuracy: 0.9890 - val_loss: 0.0447\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9941 - loss: 0.0171 - val_accuracy: 0.9955 - val_loss: 0.0225\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000118.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9954688731284476 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 119\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79AEA6A80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8924 - loss: 2.2286 - val_accuracy: 0.9462 - val_loss: 0.1290\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9597 - loss: 0.1078 - val_accuracy: 0.9850 - val_loss: 0.0669\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9801 - loss: 0.0636 - val_accuracy: 0.9915 - val_loss: 0.0364\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9872 - loss: 0.0462 - val_accuracy: 0.9894 - val_loss: 0.0504\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9910 - loss: 0.0330 - val_accuracy: 0.9937 - val_loss: 0.0277\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9947 - loss: 0.0226 - val_accuracy: 0.9945 - val_loss: 0.0270\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9964 - loss: 0.0158 - val_accuracy: 0.9917 - val_loss: 0.0439\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9940 - loss: 0.0198 - val_accuracy: 0.9901 - val_loss: 0.0456\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9947 - loss: 0.0185 - val_accuracy: 0.9941 - val_loss: 0.0320\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9924 - loss: 0.0234 - val_accuracy: 0.9935 - val_loss: 0.0329\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000119.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9934988179669031 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 120\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A820CCDF40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9175 - loss: 0.6135 - val_accuracy: 0.9888 - val_loss: 0.0363\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9835 - loss: 0.0457 - val_accuracy: 0.9872 - val_loss: 0.0422\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9930 - loss: 0.0205 - val_accuracy: 0.9955 - val_loss: 0.0177\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9945 - loss: 0.0150 - val_accuracy: 0.9862 - val_loss: 0.0660\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9941 - loss: 0.0182 - val_accuracy: 0.9937 - val_loss: 0.0299\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.9945 - val_loss: 0.0201\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9961 - loss: 0.0116 - val_accuracy: 0.9949 - val_loss: 0.0206\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9974 - loss: 0.0074 - val_accuracy: 0.9933 - val_loss: 0.0269\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9963 - loss: 0.0129 - val_accuracy: 0.9955 - val_loss: 0.0371\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9979 - loss: 0.0057 - val_accuracy: 0.9957 - val_loss: 0.0168\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000120.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 121\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79AA81A90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8956 - loss: 3.4409 - val_accuracy: 0.9738 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9691 - loss: 0.0773 - val_accuracy: 0.9801 - val_loss: 0.0507\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9858 - loss: 0.0445 - val_accuracy: 0.9917 - val_loss: 0.0248\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9890 - loss: 0.0317 - val_accuracy: 0.9866 - val_loss: 0.0403\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9899 - loss: 0.0312 - val_accuracy: 0.9937 - val_loss: 0.0228\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9931 - loss: 0.0218 - val_accuracy: 0.9939 - val_loss: 0.0188\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9914 - loss: 0.0226 - val_accuracy: 0.9935 - val_loss: 0.0195\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9935 - loss: 0.0206 - val_accuracy: 0.9947 - val_loss: 0.0189\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9891 - loss: 0.0317 - val_accuracy: 0.9941 - val_loss: 0.0221\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9935 - loss: 0.0167 - val_accuracy: 0.9947 - val_loss: 0.0262\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000121.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 122\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A82C7C60C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9163 - loss: 1.2165 - val_accuracy: 0.9779 - val_loss: 0.0602\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9781 - loss: 0.0594 - val_accuracy: 0.9712 - val_loss: 0.0776\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9892 - loss: 0.0334 - val_accuracy: 0.9907 - val_loss: 0.0285\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9913 - loss: 0.0234 - val_accuracy: 0.9945 - val_loss: 0.0285\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9950 - loss: 0.0150 - val_accuracy: 0.9931 - val_loss: 0.0268\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9930 - loss: 0.0206 - val_accuracy: 0.9909 - val_loss: 0.0298\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9949 - loss: 0.0145 - val_accuracy: 0.9925 - val_loss: 0.0226\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9973 - loss: 0.0115 - val_accuracy: 0.9915 - val_loss: 0.0256\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9964 - loss: 0.0104 - val_accuracy: 0.9911 - val_loss: 0.0367\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9964 - loss: 0.0101 - val_accuracy: 0.9923 - val_loss: 0.0294\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000122.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9923167848699763 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 123\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A822824BF0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.9098 - loss: 0.9366 - val_accuracy: 0.9803 - val_loss: 0.0573\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9774 - loss: 0.0592 - val_accuracy: 0.9903 - val_loss: 0.0268\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9888 - loss: 0.0317 - val_accuracy: 0.9915 - val_loss: 0.0254\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9910 - loss: 0.0258 - val_accuracy: 0.9965 - val_loss: 0.0113\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9915 - loss: 0.0239 - val_accuracy: 0.9943 - val_loss: 0.0153\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9939 - loss: 0.0198 - val_accuracy: 0.9915 - val_loss: 0.0307\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9954 - loss: 0.0143 - val_accuracy: 0.9976 - val_loss: 0.0129\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9963 - loss: 0.0111 - val_accuracy: 0.9959 - val_loss: 0.0169\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9970 - val_loss: 0.0124\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9975 - loss: 0.0077 - val_accuracy: 0.9813 - val_loss: 0.0741\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000123.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.981284475965327 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 124\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8227D8A10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9199 - loss: 0.9185 - val_accuracy: 0.9831 - val_loss: 0.0542\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9837 - loss: 0.0451 - val_accuracy: 0.9884 - val_loss: 0.0323\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9905 - loss: 0.0268 - val_accuracy: 0.9923 - val_loss: 0.0241\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9953 - loss: 0.0132 - val_accuracy: 0.9931 - val_loss: 0.0218\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9929 - loss: 0.0187 - val_accuracy: 0.9905 - val_loss: 0.0357\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9964 - loss: 0.0099 - val_accuracy: 0.9951 - val_loss: 0.0205\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9946 - loss: 0.0154 - val_accuracy: 0.9945 - val_loss: 0.0193\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9958 - loss: 0.0138 - val_accuracy: 0.9941 - val_loss: 0.0291\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9970 - loss: 0.0097 - val_accuracy: 0.9961 - val_loss: 0.0224\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9972 - loss: 0.0072 - val_accuracy: 0.9945 - val_loss: 0.0266\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000124.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 125\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A82273C530>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8949 - loss: 1.8891 - val_accuracy: 0.9697 - val_loss: 0.0793\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9667 - loss: 0.0775 - val_accuracy: 0.9842 - val_loss: 0.0405\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9863 - loss: 0.0401 - val_accuracy: 0.9927 - val_loss: 0.0218\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9914 - loss: 0.0227 - val_accuracy: 0.9935 - val_loss: 0.0195\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9955 - loss: 0.0195 - val_accuracy: 0.9941 - val_loss: 0.0227\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9949 - loss: 0.0172 - val_accuracy: 0.9959 - val_loss: 0.0145\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9952 - loss: 0.0160 - val_accuracy: 0.9961 - val_loss: 0.0124\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9940 - loss: 0.0155 - val_accuracy: 0.9961 - val_loss: 0.0142\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9967 - loss: 0.0116 - val_accuracy: 0.9968 - val_loss: 0.0108\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9964 - loss: 0.0123 - val_accuracy: 0.9917 - val_loss: 0.0230\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000125.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.991725768321513 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 126\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A83300C860>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9174 - loss: 1.0140 - val_accuracy: 0.9726 - val_loss: 0.0708\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9767 - loss: 0.0582 - val_accuracy: 0.9919 - val_loss: 0.0266\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9891 - loss: 0.0327 - val_accuracy: 0.9943 - val_loss: 0.0193\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9924 - loss: 0.0204 - val_accuracy: 0.9949 - val_loss: 0.0174\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9961 - loss: 0.0117 - val_accuracy: 0.9909 - val_loss: 0.0225\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9970 - loss: 0.0086 - val_accuracy: 0.9913 - val_loss: 0.0324\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9961 - loss: 0.0120 - val_accuracy: 0.9925 - val_loss: 0.0391\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9964 - loss: 0.0114 - val_accuracy: 0.9951 - val_loss: 0.0200\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9946 - loss: 0.0153 - val_accuracy: 0.9947 - val_loss: 0.0243\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9955 - loss: 0.0168 - val_accuracy: 0.9959 - val_loss: 0.0168\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000126.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 127\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8386E0F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8993 - loss: 3.7684 - val_accuracy: 0.9756 - val_loss: 0.0615\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9715 - loss: 0.0740 - val_accuracy: 0.9880 - val_loss: 0.0384\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9844 - loss: 0.0472 - val_accuracy: 0.9927 - val_loss: 0.0232\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9913 - loss: 0.0246 - val_accuracy: 0.9931 - val_loss: 0.0201\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9929 - loss: 0.0181 - val_accuracy: 0.9959 - val_loss: 0.0137\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9923 - loss: 0.0223 - val_accuracy: 0.9957 - val_loss: 0.0155\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9935 - loss: 0.0198 - val_accuracy: 0.9968 - val_loss: 0.0129\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9938 - loss: 0.0190 - val_accuracy: 0.9945 - val_loss: 0.0186\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9962 - loss: 0.0123 - val_accuracy: 0.9972 - val_loss: 0.0093\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9950 - loss: 0.0125 - val_accuracy: 0.9945 - val_loss: 0.0172\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000127.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 128\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8355A4D10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9053 - loss: 2.0539 - val_accuracy: 0.9852 - val_loss: 0.0541\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9734 - loss: 0.0629 - val_accuracy: 0.9945 - val_loss: 0.0252\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9873 - loss: 0.0365 - val_accuracy: 0.9951 - val_loss: 0.0187\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9926 - loss: 0.0232 - val_accuracy: 0.9933 - val_loss: 0.0248\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9931 - loss: 0.0183 - val_accuracy: 0.9929 - val_loss: 0.0214\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9948 - loss: 0.0157 - val_accuracy: 0.9965 - val_loss: 0.0149\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9955 - loss: 0.0119 - val_accuracy: 0.9949 - val_loss: 0.0254\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9950 - loss: 0.0139 - val_accuracy: 0.9963 - val_loss: 0.0137\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9969 - loss: 0.0096 - val_accuracy: 0.9968 - val_loss: 0.0143\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9956 - loss: 0.0152 - val_accuracy: 0.9943 - val_loss: 0.0275\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000128.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9942868400315209 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 129\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A839857AD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9095 - loss: 3.1305 - val_accuracy: 0.9746 - val_loss: 0.0744\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9740 - loss: 0.0706 - val_accuracy: 0.9927 - val_loss: 0.0247\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9873 - loss: 0.0363 - val_accuracy: 0.9901 - val_loss: 0.0295\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9927 - loss: 0.0213 - val_accuracy: 0.9955 - val_loss: 0.0167\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9954 - loss: 0.0163 - val_accuracy: 0.9913 - val_loss: 0.0275\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9951 - loss: 0.0152 - val_accuracy: 0.9937 - val_loss: 0.0221\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9954 - loss: 0.0118 - val_accuracy: 0.9961 - val_loss: 0.0143\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9968 - loss: 0.0121 - val_accuracy: 0.9972 - val_loss: 0.0100\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9960 - loss: 0.0128 - val_accuracy: 0.9967 - val_loss: 0.0129\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9966 - loss: 0.0111 - val_accuracy: 0.9967 - val_loss: 0.0164\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000129.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9966509062253743 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 130\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A83EE06210>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8823 - loss: 2.7329 - val_accuracy: 0.9348 - val_loss: 0.1220\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9464 - loss: 0.1321 - val_accuracy: 0.9825 - val_loss: 0.0850\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9694 - loss: 0.0923 - val_accuracy: 0.9856 - val_loss: 0.0662\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9779 - loss: 0.0629 - val_accuracy: 0.9838 - val_loss: 0.0580\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9815 - loss: 0.0509 - val_accuracy: 0.9931 - val_loss: 0.0316\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9874 - loss: 0.0378 - val_accuracy: 0.9961 - val_loss: 0.0222\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9904 - loss: 0.0295 - val_accuracy: 0.9935 - val_loss: 0.0315\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9877 - loss: 0.0357 - val_accuracy: 0.9917 - val_loss: 0.0460\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9916 - loss: 0.0251 - val_accuracy: 0.9957 - val_loss: 0.0281\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9940 - loss: 0.0208 - val_accuracy: 0.9972 - val_loss: 0.0217\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000130.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9972419227738377 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 131\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A841117D70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9120 - loss: 1.5959 - val_accuracy: 0.9766 - val_loss: 0.0715\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9740 - loss: 0.0705 - val_accuracy: 0.9789 - val_loss: 0.0553\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9846 - loss: 0.0403 - val_accuracy: 0.9803 - val_loss: 0.0560\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9898 - loss: 0.0273 - val_accuracy: 0.9884 - val_loss: 0.0381\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9930 - loss: 0.0195 - val_accuracy: 0.9927 - val_loss: 0.0310\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9939 - loss: 0.0169 - val_accuracy: 0.9905 - val_loss: 0.0357\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9933 - loss: 0.0162 - val_accuracy: 0.9884 - val_loss: 0.0498\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.9933 - loss: 0.0186 - val_accuracy: 0.9919 - val_loss: 0.0364\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9959 - loss: 0.0108 - val_accuracy: 0.9935 - val_loss: 0.0231\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9966 - loss: 0.0099 - val_accuracy: 0.9856 - val_loss: 0.0954\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000131.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.985618597320725 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 132\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8423CDC70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9135 - loss: 1.6059 - val_accuracy: 0.9787 - val_loss: 0.0610\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9788 - loss: 0.0602 - val_accuracy: 0.9900 - val_loss: 0.0304\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9881 - loss: 0.0317 - val_accuracy: 0.9884 - val_loss: 0.0320\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9925 - loss: 0.0226 - val_accuracy: 0.9931 - val_loss: 0.0248\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9935 - loss: 0.0187 - val_accuracy: 0.9929 - val_loss: 0.0278\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9963 - loss: 0.0090 - val_accuracy: 0.9937 - val_loss: 0.0241\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9959 - loss: 0.0102 - val_accuracy: 0.9921 - val_loss: 0.0302\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9963 - loss: 0.0105 - val_accuracy: 0.9927 - val_loss: 0.0251\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9968 - loss: 0.0119 - val_accuracy: 0.9957 - val_loss: 0.0204\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9979 - loss: 0.0063 - val_accuracy: 0.9919 - val_loss: 0.0496\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000132.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9919227738376675 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 133\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A84572ABD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9195 - loss: 0.9657 - val_accuracy: 0.9888 - val_loss: 0.0356\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9831 - loss: 0.0509 - val_accuracy: 0.9892 - val_loss: 0.0276\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9897 - loss: 0.0274 - val_accuracy: 0.9963 - val_loss: 0.0104\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9954 - loss: 0.0130 - val_accuracy: 0.9974 - val_loss: 0.0084\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9951 - loss: 0.0140 - val_accuracy: 0.9978 - val_loss: 0.0079\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9959 - loss: 0.0140 - val_accuracy: 0.9909 - val_loss: 0.0215\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9940 - loss: 0.0150 - val_accuracy: 0.9974 - val_loss: 0.0086\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9950 - loss: 0.0129 - val_accuracy: 0.9943 - val_loss: 0.0184\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9965 - loss: 0.0111 - val_accuracy: 0.9990 - val_loss: 0.0030\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9978 - loss: 0.0060 - val_accuracy: 0.9984 - val_loss: 0.0048\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000133.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9984239558707644 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 134\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A845941880>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8951 - loss: 2.4204 - val_accuracy: 0.9691 - val_loss: 0.0870\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9708 - loss: 0.0754 - val_accuracy: 0.9884 - val_loss: 0.0319\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9885 - loss: 0.0330 - val_accuracy: 0.9933 - val_loss: 0.0222\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9926 - loss: 0.0207 - val_accuracy: 0.9917 - val_loss: 0.0274\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9943 - loss: 0.0173 - val_accuracy: 0.9943 - val_loss: 0.0214\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9947 - loss: 0.0146 - val_accuracy: 0.9967 - val_loss: 0.0166\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9933 - loss: 0.0207 - val_accuracy: 0.9953 - val_loss: 0.0187\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9947 - loss: 0.0129 - val_accuracy: 0.9953 - val_loss: 0.0206\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9955 - loss: 0.0132 - val_accuracy: 0.9939 - val_loss: 0.0246\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9945 - loss: 0.0167 - val_accuracy: 0.9957 - val_loss: 0.0166\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000134.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 135\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8458D6480>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9055 - loss: 1.8321 - val_accuracy: 0.9833 - val_loss: 0.0690\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9733 - loss: 0.0695 - val_accuracy: 0.9823 - val_loss: 0.0489\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9877 - loss: 0.0330 - val_accuracy: 0.9953 - val_loss: 0.0195\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9908 - loss: 0.0267 - val_accuracy: 0.9963 - val_loss: 0.0138\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9934 - loss: 0.0221 - val_accuracy: 0.9941 - val_loss: 0.0211\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9948 - loss: 0.0170 - val_accuracy: 0.9967 - val_loss: 0.0108\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9952 - loss: 0.0153 - val_accuracy: 0.9951 - val_loss: 0.0180\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9942 - loss: 0.0170 - val_accuracy: 0.9951 - val_loss: 0.0145\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9955 - loss: 0.0133 - val_accuracy: 0.9941 - val_loss: 0.0248\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9979 - loss: 0.0089 - val_accuracy: 0.9959 - val_loss: 0.0232\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000135.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 136\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A847C07170>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9048 - loss: 1.4275 - val_accuracy: 0.9894 - val_loss: 0.0614\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9731 - loss: 0.0707 - val_accuracy: 0.9888 - val_loss: 0.0386\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9820 - loss: 0.0535 - val_accuracy: 0.9907 - val_loss: 0.0298\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9879 - loss: 0.0394 - val_accuracy: 0.9953 - val_loss: 0.0187\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9893 - loss: 0.0322 - val_accuracy: 0.9951 - val_loss: 0.0275\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9915 - loss: 0.0254 - val_accuracy: 0.9951 - val_loss: 0.0250\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9920 - loss: 0.0234 - val_accuracy: 0.9929 - val_loss: 0.0430\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9934 - loss: 0.0184 - val_accuracy: 0.9953 - val_loss: 0.0281\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9947 - loss: 0.0154 - val_accuracy: 0.9941 - val_loss: 0.0235\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9931 - loss: 0.0211 - val_accuracy: 0.9905 - val_loss: 0.0588\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000136.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 137\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A84933C170>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.9197 - loss: 1.2062 - val_accuracy: 0.9803 - val_loss: 0.0512\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9784 - loss: 0.0577 - val_accuracy: 0.9898 - val_loss: 0.0288\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9894 - loss: 0.0292 - val_accuracy: 0.9894 - val_loss: 0.0304\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9918 - loss: 0.0226 - val_accuracy: 0.9931 - val_loss: 0.0223\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9949 - loss: 0.0171 - val_accuracy: 0.9949 - val_loss: 0.0228\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9933 - loss: 0.0177 - val_accuracy: 0.9868 - val_loss: 0.0487\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9948 - loss: 0.0207 - val_accuracy: 0.9917 - val_loss: 0.0273\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9968 - loss: 0.0127 - val_accuracy: 0.9901 - val_loss: 0.0336\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9962 - loss: 0.0112 - val_accuracy: 0.9967 - val_loss: 0.0165\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9956 - loss: 0.0117 - val_accuracy: 0.9947 - val_loss: 0.0215\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000137.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 138\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8536406E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9217 - loss: 0.7161 - val_accuracy: 0.9831 - val_loss: 0.0448\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9851 - loss: 0.0414 - val_accuracy: 0.9941 - val_loss: 0.0214\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9925 - loss: 0.0218 - val_accuracy: 0.9833 - val_loss: 0.0581\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9950 - loss: 0.0162 - val_accuracy: 0.9953 - val_loss: 0.0166\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9957 - loss: 0.0115 - val_accuracy: 0.9919 - val_loss: 0.0303\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9952 - loss: 0.0124 - val_accuracy: 0.9951 - val_loss: 0.0169\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9953 - loss: 0.0126 - val_accuracy: 0.9961 - val_loss: 0.0128\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9973 - loss: 0.0074 - val_accuracy: 0.9963 - val_loss: 0.0157\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9974 - loss: 0.0087 - val_accuracy: 0.9961 - val_loss: 0.0122\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9983 - loss: 0.0059 - val_accuracy: 0.9965 - val_loss: 0.0166\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000138.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9964539007092199 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 139\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A83300C860>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9019 - loss: 2.4587 - val_accuracy: 0.9760 - val_loss: 0.0791\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9671 - loss: 0.0834 - val_accuracy: 0.9773 - val_loss: 0.0630\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9842 - loss: 0.0536 - val_accuracy: 0.9947 - val_loss: 0.0221\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9891 - loss: 0.0392 - val_accuracy: 0.9882 - val_loss: 0.0357\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9881 - loss: 0.0375 - val_accuracy: 0.9939 - val_loss: 0.0244\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9938 - loss: 0.0233 - val_accuracy: 0.9949 - val_loss: 0.0194\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9947 - loss: 0.0205 - val_accuracy: 0.9907 - val_loss: 0.0326\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 68ms/step - accuracy: 0.9948 - loss: 0.0166 - val_accuracy: 0.9953 - val_loss: 0.0179\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9947 - loss: 0.0163 - val_accuracy: 0.9937 - val_loss: 0.0310\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9971 - loss: 0.0100 - val_accuracy: 0.9974 - val_loss: 0.0141\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000139.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9974389282899921 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 140\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8353C8350>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8882 - loss: 1.9171 - val_accuracy: 0.9655 - val_loss: 0.1056\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9676 - loss: 0.1016 - val_accuracy: 0.9886 - val_loss: 0.0587\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9842 - loss: 0.0643 - val_accuracy: 0.9903 - val_loss: 0.0458\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9861 - loss: 0.0495 - val_accuracy: 0.9968 - val_loss: 0.0203\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9908 - loss: 0.0323 - val_accuracy: 0.9953 - val_loss: 0.0193\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9931 - loss: 0.0243 - val_accuracy: 0.9986 - val_loss: 0.0114\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9945 - loss: 0.0192 - val_accuracy: 0.9963 - val_loss: 0.0188\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9952 - loss: 0.0181 - val_accuracy: 0.9984 - val_loss: 0.0142\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9954 - loss: 0.0172 - val_accuracy: 0.9978 - val_loss: 0.0135\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9934 - loss: 0.0209 - val_accuracy: 0.9982 - val_loss: 0.0124\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000140.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.99822695035461 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 141\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A75EFCAB10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.9147 - loss: 1.2322 - val_accuracy: 0.9655 - val_loss: 0.0852\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9802 - loss: 0.0562 - val_accuracy: 0.9813 - val_loss: 0.0471\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9901 - loss: 0.0288 - val_accuracy: 0.9933 - val_loss: 0.0214\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9931 - loss: 0.0206 - val_accuracy: 0.9955 - val_loss: 0.0216\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9947 - loss: 0.0157 - val_accuracy: 0.9953 - val_loss: 0.0158\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9913 - val_loss: 0.0303\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9942 - loss: 0.0166 - val_accuracy: 0.9967 - val_loss: 0.0174\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9953 - loss: 0.0159 - val_accuracy: 0.9963 - val_loss: 0.0259\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9968 - loss: 0.0097 - val_accuracy: 0.9965 - val_loss: 0.0189\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9971 - loss: 0.0090 - val_accuracy: 0.9982 - val_loss: 0.0133\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000141.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.99822695035461 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 142\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A78B097EC0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.9206 - loss: 0.7957 - val_accuracy: 0.9760 - val_loss: 0.0618\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9787 - loss: 0.0565 - val_accuracy: 0.9898 - val_loss: 0.0328\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9898 - loss: 0.0299 - val_accuracy: 0.9933 - val_loss: 0.0204\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9940 - loss: 0.0192 - val_accuracy: 0.9894 - val_loss: 0.0336\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9955 - loss: 0.0123 - val_accuracy: 0.9929 - val_loss: 0.0227\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9935 - val_loss: 0.0206\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9965 - loss: 0.0121 - val_accuracy: 0.9947 - val_loss: 0.0249\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9962 - loss: 0.0091 - val_accuracy: 0.9931 - val_loss: 0.0247\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9962 - loss: 0.0112 - val_accuracy: 0.9929 - val_loss: 0.0291\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9976 - loss: 0.0065 - val_accuracy: 0.9963 - val_loss: 0.0178\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000142.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9962568951930654 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 143\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8458FE0C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9166 - loss: 1.1331 - val_accuracy: 0.9760 - val_loss: 0.0633\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9774 - loss: 0.0609 - val_accuracy: 0.9921 - val_loss: 0.0304\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9897 - loss: 0.0301 - val_accuracy: 0.9823 - val_loss: 0.0436\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9925 - loss: 0.0219 - val_accuracy: 0.9921 - val_loss: 0.0192\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9949 - loss: 0.0149 - val_accuracy: 0.9943 - val_loss: 0.0168\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9959 - loss: 0.0107 - val_accuracy: 0.9967 - val_loss: 0.0100\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9969 - loss: 0.0104 - val_accuracy: 0.9967 - val_loss: 0.0103\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9970 - loss: 0.0090 - val_accuracy: 0.9970 - val_loss: 0.0111\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9974 - loss: 0.0073 - val_accuracy: 0.9951 - val_loss: 0.0144\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9979 - loss: 0.0068 - val_accuracy: 0.9961 - val_loss: 0.0148\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000143.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.996059889676911 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 144\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A83545CC20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.9089 - loss: 3.2970 - val_accuracy: 0.9746 - val_loss: 0.0712\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9796 - loss: 0.0530 - val_accuracy: 0.9868 - val_loss: 0.0373\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9887 - loss: 0.0309 - val_accuracy: 0.9945 - val_loss: 0.0207\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9950 - loss: 0.0166 - val_accuracy: 0.9913 - val_loss: 0.0315\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9948 - loss: 0.0152 - val_accuracy: 0.9939 - val_loss: 0.0191\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9955 - val_loss: 0.0214\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9941 - loss: 0.0152 - val_accuracy: 0.9927 - val_loss: 0.0335\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9953 - loss: 0.0142 - val_accuracy: 0.9939 - val_loss: 0.0286\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9960 - loss: 0.0110 - val_accuracy: 0.9921 - val_loss: 0.0329\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9970 - loss: 0.0080 - val_accuracy: 0.9953 - val_loss: 0.0204\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000144.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 145\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8458D77D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8954 - loss: 1.7721 - val_accuracy: 0.9604 - val_loss: 0.1063\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9569 - loss: 0.1052 - val_accuracy: 0.9773 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9792 - loss: 0.0614 - val_accuracy: 0.9925 - val_loss: 0.0288\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9880 - loss: 0.0370 - val_accuracy: 0.9905 - val_loss: 0.0453\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9905 - loss: 0.0276 - val_accuracy: 0.9955 - val_loss: 0.0196\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9916 - loss: 0.0259 - val_accuracy: 0.9909 - val_loss: 0.0358\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9902 - loss: 0.0292 - val_accuracy: 0.9968 - val_loss: 0.0165\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9952 - loss: 0.0168 - val_accuracy: 0.9961 - val_loss: 0.0200\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9951 - loss: 0.0161 - val_accuracy: 0.9773 - val_loss: 0.1331\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9928 - loss: 0.0228 - val_accuracy: 0.9968 - val_loss: 0.0187\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000145.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9968479117415288 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 146\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8228244A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.9021 - loss: 1.8925 - val_accuracy: 0.9762 - val_loss: 0.0698\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9732 - loss: 0.0695 - val_accuracy: 0.9866 - val_loss: 0.0424\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9857 - loss: 0.0369 - val_accuracy: 0.9903 - val_loss: 0.0327\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9920 - loss: 0.0267 - val_accuracy: 0.9909 - val_loss: 0.0323\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9943 - loss: 0.0187 - val_accuracy: 0.9937 - val_loss: 0.0257\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9940 - loss: 0.0153 - val_accuracy: 0.9929 - val_loss: 0.0302\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9937 - loss: 0.0200 - val_accuracy: 0.9933 - val_loss: 0.0231\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9955 - loss: 0.0108 - val_accuracy: 0.9923 - val_loss: 0.0289\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9959 - loss: 0.0132 - val_accuracy: 0.9927 - val_loss: 0.0273\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9959 - loss: 0.0122 - val_accuracy: 0.9911 - val_loss: 0.0283\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000146.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9911347517730497 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 147\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79AABBE60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8942 - loss: 1.7244 - val_accuracy: 0.9681 - val_loss: 0.0913\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9646 - loss: 0.0874 - val_accuracy: 0.9913 - val_loss: 0.0412\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9853 - loss: 0.0453 - val_accuracy: 0.9945 - val_loss: 0.0225\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9906 - loss: 0.0303 - val_accuracy: 0.9929 - val_loss: 0.0224\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9950 - loss: 0.0211 - val_accuracy: 0.9949 - val_loss: 0.0209\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9953 - loss: 0.0174 - val_accuracy: 0.9923 - val_loss: 0.0291\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9958 - loss: 0.0161 - val_accuracy: 0.9967 - val_loss: 0.0121\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9972 - loss: 0.0110 - val_accuracy: 0.9935 - val_loss: 0.0364\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.9963 - loss: 0.0134 - val_accuracy: 0.9970 - val_loss: 0.0141\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.9937 - val_loss: 0.0372\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000147.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9936958234830575 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 148\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A794423380>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 68ms/step - accuracy: 0.8938 - loss: 3.3325 - val_accuracy: 0.9592 - val_loss: 0.1028\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9593 - loss: 0.1035 - val_accuracy: 0.9712 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9791 - loss: 0.0586 - val_accuracy: 0.9829 - val_loss: 0.0602\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9863 - loss: 0.0395 - val_accuracy: 0.9888 - val_loss: 0.0340\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9899 - loss: 0.0227 - val_accuracy: 0.9955 - val_loss: 0.0152\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9931 - loss: 0.0224 - val_accuracy: 0.9939 - val_loss: 0.0171\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9936 - loss: 0.0170 - val_accuracy: 0.9953 - val_loss: 0.0161\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9950 - loss: 0.0161 - val_accuracy: 0.9917 - val_loss: 0.0272\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9933 - loss: 0.0190 - val_accuracy: 0.9972 - val_loss: 0.0136\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9947 - loss: 0.0135 - val_accuracy: 0.9968 - val_loss: 0.0157\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000148.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9968479117415288 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 149\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79563D070>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9121 - loss: 2.4127 - val_accuracy: 0.9785 - val_loss: 0.0657\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9793 - loss: 0.0563 - val_accuracy: 0.9905 - val_loss: 0.0296\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9920 - loss: 0.0244 - val_accuracy: 0.9919 - val_loss: 0.0254\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9939 - loss: 0.0177 - val_accuracy: 0.9931 - val_loss: 0.0250\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9942 - loss: 0.0160 - val_accuracy: 0.9951 - val_loss: 0.0171\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9966 - loss: 0.0107 - val_accuracy: 0.9955 - val_loss: 0.0163\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9951 - loss: 0.0172 - val_accuracy: 0.9955 - val_loss: 0.0217\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9980 - loss: 0.0066 - val_accuracy: 0.9945 - val_loss: 0.0198\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9965 - loss: 0.0100 - val_accuracy: 0.9925 - val_loss: 0.0234\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9972 - loss: 0.0090 - val_accuracy: 0.9957 - val_loss: 0.0118\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000149.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 150\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A798A16A80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8983 - loss: 2.2204 - val_accuracy: 0.9734 - val_loss: 0.0986\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9691 - loss: 0.0885 - val_accuracy: 0.9876 - val_loss: 0.0482\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9853 - loss: 0.0516 - val_accuracy: 0.9901 - val_loss: 0.0312\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9899 - loss: 0.0392 - val_accuracy: 0.9935 - val_loss: 0.0229\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9950 - loss: 0.0219 - val_accuracy: 0.9872 - val_loss: 0.0533\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9935 - loss: 0.0213 - val_accuracy: 0.9917 - val_loss: 0.0329\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9944 - loss: 0.0196 - val_accuracy: 0.9943 - val_loss: 0.0254\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9970 - loss: 0.0140 - val_accuracy: 0.9961 - val_loss: 0.0182\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9965 - loss: 0.0109 - val_accuracy: 0.9947 - val_loss: 0.0213\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9941 - loss: 0.0195 - val_accuracy: 0.9947 - val_loss: 0.0223\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000150.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 151\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A799C76720>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9038 - loss: 2.0991 - val_accuracy: 0.9730 - val_loss: 0.0806\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9700 - loss: 0.0817 - val_accuracy: 0.9868 - val_loss: 0.0434\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9825 - loss: 0.0478 - val_accuracy: 0.9919 - val_loss: 0.0286\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9877 - loss: 0.0357 - val_accuracy: 0.9903 - val_loss: 0.0327\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9902 - loss: 0.0267 - val_accuracy: 0.9907 - val_loss: 0.0265\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9932 - loss: 0.0215 - val_accuracy: 0.9925 - val_loss: 0.0325\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9936 - loss: 0.0200 - val_accuracy: 0.9943 - val_loss: 0.0255\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9945 - loss: 0.0144 - val_accuracy: 0.9905 - val_loss: 0.0281\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9931 - loss: 0.0204 - val_accuracy: 0.9909 - val_loss: 0.0336\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9944 - loss: 0.0158 - val_accuracy: 0.9907 - val_loss: 0.0308\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000151.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9907407407407407 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 152\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A79A059C10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8925 - loss: 2.5313 - val_accuracy: 0.9572 - val_loss: 0.1022\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9580 - loss: 0.1091 - val_accuracy: 0.9695 - val_loss: 0.0749\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9699 - loss: 0.0739 - val_accuracy: 0.9831 - val_loss: 0.0468\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9828 - loss: 0.0493 - val_accuracy: 0.9866 - val_loss: 0.0362\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9881 - loss: 0.0350 - val_accuracy: 0.9929 - val_loss: 0.0238\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9904 - loss: 0.0272 - val_accuracy: 0.9949 - val_loss: 0.0158\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9921 - loss: 0.0205 - val_accuracy: 0.9872 - val_loss: 0.0447\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9919 - loss: 0.0200 - val_accuracy: 0.9905 - val_loss: 0.0356\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9930 - loss: 0.0196 - val_accuracy: 0.9941 - val_loss: 0.0190\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9933 - loss: 0.0191 - val_accuracy: 0.9953 - val_loss: 0.0161\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000152.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 153\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7978B0110>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8971 - loss: 2.4121 - val_accuracy: 0.9718 - val_loss: 0.0778\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9727 - loss: 0.0724 - val_accuracy: 0.9775 - val_loss: 0.0604\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9834 - loss: 0.0474 - val_accuracy: 0.9850 - val_loss: 0.0423\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9904 - loss: 0.0265 - val_accuracy: 0.9783 - val_loss: 0.0905\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9915 - loss: 0.0238 - val_accuracy: 0.9898 - val_loss: 0.0373\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9943 - loss: 0.0164 - val_accuracy: 0.9815 - val_loss: 0.0580\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9936 - loss: 0.0184 - val_accuracy: 0.9907 - val_loss: 0.0334\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9950 - loss: 0.0144 - val_accuracy: 0.9905 - val_loss: 0.0317\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9946 - loss: 0.0176 - val_accuracy: 0.9888 - val_loss: 0.0460\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9958 - loss: 0.0122 - val_accuracy: 0.9896 - val_loss: 0.0413\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000153.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.989558707643814 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 154\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7978B0110>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9067 - loss: 1.2477 - val_accuracy: 0.9699 - val_loss: 0.0804\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9744 - loss: 0.0712 - val_accuracy: 0.9764 - val_loss: 0.0594\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9844 - loss: 0.0394 - val_accuracy: 0.9846 - val_loss: 0.0431\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9901 - loss: 0.0299 - val_accuracy: 0.9913 - val_loss: 0.0257\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9947 - loss: 0.0190 - val_accuracy: 0.9931 - val_loss: 0.0234\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9941 - loss: 0.0181 - val_accuracy: 0.9937 - val_loss: 0.0184\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9942 - loss: 0.0149 - val_accuracy: 0.9923 - val_loss: 0.0301\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9959 - loss: 0.0121 - val_accuracy: 0.9931 - val_loss: 0.0239\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9939 - loss: 0.0198 - val_accuracy: 0.9941 - val_loss: 0.0241\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9972 - loss: 0.0090 - val_accuracy: 0.9933 - val_loss: 0.0207\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000154.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9933018124507487 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 155\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7F73A7E00>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8968 - loss: 1.6587 - val_accuracy: 0.9740 - val_loss: 0.0799\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9708 - loss: 0.0789 - val_accuracy: 0.9858 - val_loss: 0.0449\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9911 - loss: 0.0277 - val_accuracy: 0.9933 - val_loss: 0.0259\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9947 - loss: 0.0195 - val_accuracy: 0.9945 - val_loss: 0.0170\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9947 - loss: 0.0186 - val_accuracy: 0.9953 - val_loss: 0.0233\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9966 - loss: 0.0133 - val_accuracy: 0.9955 - val_loss: 0.0181\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9955 - loss: 0.0129 - val_accuracy: 0.9939 - val_loss: 0.0292\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9957 - loss: 0.0142 - val_accuracy: 0.9978 - val_loss: 0.0135\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9979 - loss: 0.0077 - val_accuracy: 0.9963 - val_loss: 0.0165\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9944 - loss: 0.0203 - val_accuracy: 0.9941 - val_loss: 0.0444\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000155.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9940898345153665 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 156\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8D5ADB830>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.9150 - loss: 1.6167 - val_accuracy: 0.9854 - val_loss: 0.0422\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9812 - loss: 0.0575 - val_accuracy: 0.9874 - val_loss: 0.0371\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9898 - loss: 0.0270 - val_accuracy: 0.9927 - val_loss: 0.0223\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9944 - loss: 0.0182 - val_accuracy: 0.9945 - val_loss: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 66ms/step - accuracy: 0.9967 - loss: 0.0125 - val_accuracy: 0.9953 - val_loss: 0.0206\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9962 - loss: 0.0107 - val_accuracy: 0.9945 - val_loss: 0.0190\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9958 - loss: 0.0115 - val_accuracy: 0.9947 - val_loss: 0.0211\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9973 - loss: 0.0070 - val_accuracy: 0.9937 - val_loss: 0.0213\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9972 - loss: 0.0075 - val_accuracy: 0.9949 - val_loss: 0.0207\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9964 - loss: 0.0117 - val_accuracy: 0.9959 - val_loss: 0.0196\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000156.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 157\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8D7267AD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9087 - loss: 1.2659 - val_accuracy: 0.9490 - val_loss: 0.1014\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9766 - loss: 0.0613 - val_accuracy: 0.9901 - val_loss: 0.0252\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9877 - loss: 0.0346 - val_accuracy: 0.9888 - val_loss: 0.0327\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9943 - loss: 0.0224 - val_accuracy: 0.9955 - val_loss: 0.0160\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9907 - loss: 0.0258 - val_accuracy: 0.9957 - val_loss: 0.0132\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9929 - loss: 0.0206 - val_accuracy: 0.9943 - val_loss: 0.0185\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9901 - loss: 0.0267 - val_accuracy: 0.9967 - val_loss: 0.0116\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9955 - loss: 0.0129 - val_accuracy: 0.9856 - val_loss: 0.0464\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9953 - loss: 0.0133 - val_accuracy: 0.9965 - val_loss: 0.0092\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9964 - loss: 0.0115 - val_accuracy: 0.9905 - val_loss: 0.0397\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000157.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 158\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8D74B9A90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.9078 - loss: 2.0547 - val_accuracy: 0.9770 - val_loss: 0.0626\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9736 - loss: 0.0666 - val_accuracy: 0.9880 - val_loss: 0.0424\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9859 - loss: 0.0384 - val_accuracy: 0.9901 - val_loss: 0.0344\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9929 - loss: 0.0231 - val_accuracy: 0.9915 - val_loss: 0.0281\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9930 - loss: 0.0221 - val_accuracy: 0.9949 - val_loss: 0.0256\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9933 - loss: 0.0200 - val_accuracy: 0.9953 - val_loss: 0.0173\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9944 - loss: 0.0169 - val_accuracy: 0.9925 - val_loss: 0.0266\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9951 - loss: 0.0130 - val_accuracy: 0.9968 - val_loss: 0.0124\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9963 - loss: 0.0100 - val_accuracy: 0.9967 - val_loss: 0.0159\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9948 - loss: 0.0148 - val_accuracy: 0.9953 - val_loss: 0.0244\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000158.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 159\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8D786BB30>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8890 - loss: 2.7189 - val_accuracy: 0.9726 - val_loss: 0.0797\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9649 - loss: 0.0868 - val_accuracy: 0.9905 - val_loss: 0.0280\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9857 - loss: 0.0407 - val_accuracy: 0.9915 - val_loss: 0.0238\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9906 - loss: 0.0276 - val_accuracy: 0.9963 - val_loss: 0.0139\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9952 - loss: 0.0163 - val_accuracy: 0.9957 - val_loss: 0.0132\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9933 - loss: 0.0188 - val_accuracy: 0.9937 - val_loss: 0.0168\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9927 - loss: 0.0205 - val_accuracy: 0.9974 - val_loss: 0.0059\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9955 - loss: 0.0141 - val_accuracy: 0.9984 - val_loss: 0.0042\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9953 - loss: 0.0123 - val_accuracy: 0.9976 - val_loss: 0.0053\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9955 - loss: 0.0133 - val_accuracy: 0.9978 - val_loss: 0.0057\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000159.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.997832939322301 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 160\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001AA682F4140>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8892 - loss: 2.6893 - val_accuracy: 0.9600 - val_loss: 0.1167\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9611 - loss: 0.1177 - val_accuracy: 0.9722 - val_loss: 0.0794\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9808 - loss: 0.0628 - val_accuracy: 0.9892 - val_loss: 0.0387\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9888 - loss: 0.0402 - val_accuracy: 0.9961 - val_loss: 0.0200\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9909 - loss: 0.0319 - val_accuracy: 0.9935 - val_loss: 0.0256\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9945 - loss: 0.0214 - val_accuracy: 0.9961 - val_loss: 0.0173\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9943 - loss: 0.0181 - val_accuracy: 0.9970 - val_loss: 0.0164\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9922 - loss: 0.0222 - val_accuracy: 0.9961 - val_loss: 0.0159\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9938 - loss: 0.0191 - val_accuracy: 0.9970 - val_loss: 0.0142\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9960 - loss: 0.0142 - val_accuracy: 0.9967 - val_loss: 0.0163\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000160.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9966509062253743 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 161\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001AA682B6870>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 65ms/step - accuracy: 0.9059 - loss: 3.0270 - val_accuracy: 0.9628 - val_loss: 0.0977\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9746 - loss: 0.0685 - val_accuracy: 0.9734 - val_loss: 0.0646\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9844 - loss: 0.0438 - val_accuracy: 0.9842 - val_loss: 0.0386\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9887 - loss: 0.0294 - val_accuracy: 0.9903 - val_loss: 0.0268\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9929 - loss: 0.0181 - val_accuracy: 0.9913 - val_loss: 0.0266\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9950 - loss: 0.0147 - val_accuracy: 0.9868 - val_loss: 0.0346\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9938 - loss: 0.0168 - val_accuracy: 0.9931 - val_loss: 0.0241\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9962 - loss: 0.0116 - val_accuracy: 0.9888 - val_loss: 0.0461\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9952 - loss: 0.0180 - val_accuracy: 0.9909 - val_loss: 0.0282\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9956 - loss: 0.0104 - val_accuracy: 0.9870 - val_loss: 0.0418\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000161.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9869976359338062 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 162\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACF6A2EE70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.9183 - loss: 1.4320 - val_accuracy: 0.9846 - val_loss: 0.0425\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9841 - loss: 0.0419 - val_accuracy: 0.9819 - val_loss: 0.0571\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9914 - loss: 0.0276 - val_accuracy: 0.9909 - val_loss: 0.0255\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9935 - loss: 0.0190 - val_accuracy: 0.9896 - val_loss: 0.0336\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9956 - loss: 0.0140 - val_accuracy: 0.9935 - val_loss: 0.0193\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9961 - loss: 0.0123 - val_accuracy: 0.9935 - val_loss: 0.0235\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9947 - loss: 0.0139 - val_accuracy: 0.9915 - val_loss: 0.0389\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9952 - loss: 0.0125 - val_accuracy: 0.9907 - val_loss: 0.0345\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9964 - loss: 0.0103 - val_accuracy: 0.9951 - val_loss: 0.0234\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9969 - loss: 0.0086 - val_accuracy: 0.9939 - val_loss: 0.0260\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000162.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 163\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001AA682B76B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8956 - loss: 2.4875 - val_accuracy: 0.9716 - val_loss: 0.0863\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9659 - loss: 0.0872 - val_accuracy: 0.9844 - val_loss: 0.0564\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9831 - loss: 0.0493 - val_accuracy: 0.9931 - val_loss: 0.0338\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9873 - loss: 0.0378 - val_accuracy: 0.9923 - val_loss: 0.0334\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9941 - loss: 0.0211 - val_accuracy: 0.9939 - val_loss: 0.0258\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9934 - loss: 0.0220 - val_accuracy: 0.9953 - val_loss: 0.0333\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9941 - loss: 0.0164 - val_accuracy: 0.9947 - val_loss: 0.0340\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9951 - loss: 0.0141 - val_accuracy: 0.9921 - val_loss: 0.0551\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9952 - loss: 0.0135 - val_accuracy: 0.9929 - val_loss: 0.0500\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9954 - loss: 0.0151 - val_accuracy: 0.9947 - val_loss: 0.0396\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000163.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 164\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACC0247470>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8974 - loss: 2.0395 - val_accuracy: 0.9663 - val_loss: 0.1223\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9666 - loss: 0.1001 - val_accuracy: 0.9856 - val_loss: 0.0621\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9816 - loss: 0.0569 - val_accuracy: 0.9903 - val_loss: 0.0291\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9903 - loss: 0.0372 - val_accuracy: 0.9943 - val_loss: 0.0221\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9912 - loss: 0.0305 - val_accuracy: 0.9856 - val_loss: 0.0479\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9949 - loss: 0.0197 - val_accuracy: 0.9957 - val_loss: 0.0140\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9947 - loss: 0.0182 - val_accuracy: 0.9955 - val_loss: 0.0135\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9945 - loss: 0.0191 - val_accuracy: 0.9953 - val_loss: 0.0175\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9956 - loss: 0.0129 - val_accuracy: 0.9921 - val_loss: 0.0322\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9965 - loss: 0.0111 - val_accuracy: 0.9965 - val_loss: 0.0149\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000164.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9964539007092199 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 165\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACB2904410>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.9080 - loss: 2.9604 - val_accuracy: 0.9632 - val_loss: 0.0980\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9761 - loss: 0.0630 - val_accuracy: 0.9856 - val_loss: 0.0494\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9858 - loss: 0.0381 - val_accuracy: 0.9917 - val_loss: 0.0300\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7009s\u001b[0m 11s/step - accuracy: 0.9912 - loss: 0.0264 - val_accuracy: 0.9943 - val_loss: 0.0269\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9932 - loss: 0.0167 - val_accuracy: 0.9915 - val_loss: 0.0270\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9949 - loss: 0.0139 - val_accuracy: 0.9929 - val_loss: 0.0291\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 81ms/step - accuracy: 0.9941 - loss: 0.0154 - val_accuracy: 0.9901 - val_loss: 0.0347\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 84ms/step - accuracy: 0.9955 - loss: 0.0118 - val_accuracy: 0.9941 - val_loss: 0.0260\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 85ms/step - accuracy: 0.9965 - loss: 0.0113 - val_accuracy: 0.9854 - val_loss: 0.0817\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 95ms/step - accuracy: 0.9941 - loss: 0.0177 - val_accuracy: 0.9905 - val_loss: 0.0621\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000165.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 166\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACAE41BE60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 107ms/step - accuracy: 0.9171 - loss: 0.8138 - val_accuracy: 0.9888 - val_loss: 0.0335\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 101ms/step - accuracy: 0.9849 - loss: 0.0378 - val_accuracy: 0.9965 - val_loss: 0.0158\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9913 - loss: 0.0243 - val_accuracy: 0.9941 - val_loss: 0.0198\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 92ms/step - accuracy: 0.9948 - loss: 0.0158 - val_accuracy: 0.9965 - val_loss: 0.0112\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 86ms/step - accuracy: 0.9954 - loss: 0.0115 - val_accuracy: 0.9941 - val_loss: 0.0212\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 89ms/step - accuracy: 0.9969 - loss: 0.0110 - val_accuracy: 0.9970 - val_loss: 0.0160\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 86ms/step - accuracy: 0.9969 - loss: 0.0090 - val_accuracy: 0.9911 - val_loss: 0.0419\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9959 - loss: 0.0133 - val_accuracy: 0.9909 - val_loss: 0.0313\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9972 - loss: 0.0097 - val_accuracy: 0.9965 - val_loss: 0.0144\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9960 - loss: 0.0107 - val_accuracy: 0.9967 - val_loss: 0.0178\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000166.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9966509062253743 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 167\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACB2904410>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 80ms/step - accuracy: 0.8991 - loss: 2.7840 - val_accuracy: 0.9691 - val_loss: 0.0892\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 78ms/step - accuracy: 0.9705 - loss: 0.0796 - val_accuracy: 0.9872 - val_loss: 0.0379\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9848 - loss: 0.0432 - val_accuracy: 0.9925 - val_loss: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9926 - loss: 0.0249 - val_accuracy: 0.9943 - val_loss: 0.0193\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 82ms/step - accuracy: 0.9958 - loss: 0.0163 - val_accuracy: 0.9951 - val_loss: 0.0202\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9932 - loss: 0.0193 - val_accuracy: 0.9968 - val_loss: 0.0109\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9966 - loss: 0.0130 - val_accuracy: 0.9972 - val_loss: 0.0123\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9951 - loss: 0.0139 - val_accuracy: 0.9943 - val_loss: 0.0175\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9947 - loss: 0.0155 - val_accuracy: 0.9961 - val_loss: 0.0127\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9966 - loss: 0.0112 - val_accuracy: 0.9959 - val_loss: 0.0171\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000167.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 168\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACB45C42F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8976 - loss: 2.0660 - val_accuracy: 0.9691 - val_loss: 0.0856\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9641 - loss: 0.0895 - val_accuracy: 0.9836 - val_loss: 0.0463\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9802 - loss: 0.0546 - val_accuracy: 0.9901 - val_loss: 0.0305\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9865 - loss: 0.0357 - val_accuracy: 0.9817 - val_loss: 0.0442\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9884 - loss: 0.0304 - val_accuracy: 0.9917 - val_loss: 0.0262\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9900 - loss: 0.0267 - val_accuracy: 0.9921 - val_loss: 0.0267\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9939 - loss: 0.0188 - val_accuracy: 0.9941 - val_loss: 0.0209\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9957 - loss: 0.0140 - val_accuracy: 0.9898 - val_loss: 0.0340\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9945 - loss: 0.0138 - val_accuracy: 0.9931 - val_loss: 0.0326\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9960 - loss: 0.0117 - val_accuracy: 0.9919 - val_loss: 0.0273\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000168.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9919227738376675 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 169\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8D786BB30>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8992 - loss: 2.3727 - val_accuracy: 0.9754 - val_loss: 0.0800\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 66ms/step - accuracy: 0.9728 - loss: 0.0704 - val_accuracy: 0.9872 - val_loss: 0.0476\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9859 - loss: 0.0439 - val_accuracy: 0.9888 - val_loss: 0.0390\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9893 - loss: 0.0286 - val_accuracy: 0.9921 - val_loss: 0.0356\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9929 - loss: 0.0236 - val_accuracy: 0.9911 - val_loss: 0.0337\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9937 - loss: 0.0215 - val_accuracy: 0.9937 - val_loss: 0.0338\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9953 - loss: 0.0153 - val_accuracy: 0.9901 - val_loss: 0.0461\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9975 - loss: 0.0097 - val_accuracy: 0.9931 - val_loss: 0.0349\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9962 - loss: 0.0125 - val_accuracy: 0.9943 - val_loss: 0.0351\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9962 - loss: 0.0124 - val_accuracy: 0.9927 - val_loss: 0.0374\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000169.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9927107959022853 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 170\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7F3BFCFB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8860 - loss: 3.9998 - val_accuracy: 0.9669 - val_loss: 0.0880\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9625 - loss: 0.0938 - val_accuracy: 0.9801 - val_loss: 0.0722\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9812 - loss: 0.0609 - val_accuracy: 0.9866 - val_loss: 0.0424\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9920 - loss: 0.0375 - val_accuracy: 0.9947 - val_loss: 0.0212\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9936 - loss: 0.0265 - val_accuracy: 0.9945 - val_loss: 0.0217\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9939 - loss: 0.0214 - val_accuracy: 0.9970 - val_loss: 0.0123\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9933 - loss: 0.0194 - val_accuracy: 0.9970 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9963 - loss: 0.0139 - val_accuracy: 0.9978 - val_loss: 0.0093\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9969 - loss: 0.0128 - val_accuracy: 0.9968 - val_loss: 0.0148\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9953 - loss: 0.0153 - val_accuracy: 0.9957 - val_loss: 0.0148\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000170.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 171\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A81848BE60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9084 - loss: 1.5432 - val_accuracy: 0.9756 - val_loss: 0.0670\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9809 - loss: 0.0554 - val_accuracy: 0.9848 - val_loss: 0.0418\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9905 - loss: 0.0287 - val_accuracy: 0.9919 - val_loss: 0.0251\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9941 - loss: 0.0171 - val_accuracy: 0.9929 - val_loss: 0.0248\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9947 - loss: 0.0154 - val_accuracy: 0.9927 - val_loss: 0.0218\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9957 - loss: 0.0119 - val_accuracy: 0.9907 - val_loss: 0.0319\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9956 - loss: 0.0121 - val_accuracy: 0.9959 - val_loss: 0.0110\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9952 - loss: 0.0136 - val_accuracy: 0.9953 - val_loss: 0.0124\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9960 - loss: 0.0123 - val_accuracy: 0.9957 - val_loss: 0.0160\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9969 - loss: 0.0091 - val_accuracy: 0.9941 - val_loss: 0.0289\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000171.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9940898345153665 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 172\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8D6F577A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8978 - loss: 2.0548 - val_accuracy: 0.9726 - val_loss: 0.0770\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9622 - loss: 0.0966 - val_accuracy: 0.9858 - val_loss: 0.0467\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9806 - loss: 0.0627 - val_accuracy: 0.9919 - val_loss: 0.0330\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9879 - loss: 0.0442 - val_accuracy: 0.9961 - val_loss: 0.0207\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9911 - loss: 0.0310 - val_accuracy: 0.9957 - val_loss: 0.0187\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9931 - loss: 0.0245 - val_accuracy: 0.9968 - val_loss: 0.0116\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9953 - loss: 0.0173 - val_accuracy: 0.9945 - val_loss: 0.0265\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9920 - loss: 0.0237 - val_accuracy: 0.9961 - val_loss: 0.0190\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9961 - loss: 0.0138 - val_accuracy: 0.9943 - val_loss: 0.0187\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9949 - loss: 0.0157 - val_accuracy: 0.9957 - val_loss: 0.0176\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000172.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 173\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A8D76406E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.9006 - loss: 1.6257 - val_accuracy: 0.9732 - val_loss: 0.0770\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9747 - loss: 0.0702 - val_accuracy: 0.9817 - val_loss: 0.0558\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9858 - loss: 0.0410 - val_accuracy: 0.9870 - val_loss: 0.0410\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9897 - loss: 0.0282 - val_accuracy: 0.9911 - val_loss: 0.0263\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9904 - loss: 0.0241 - val_accuracy: 0.9874 - val_loss: 0.0434\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9941 - loss: 0.0165 - val_accuracy: 0.9939 - val_loss: 0.0219\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9932 - loss: 0.0203 - val_accuracy: 0.9905 - val_loss: 0.0248\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9927 - loss: 0.0232 - val_accuracy: 0.9927 - val_loss: 0.0281\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9953 - loss: 0.0114 - val_accuracy: 0.9931 - val_loss: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9958 - loss: 0.0114 - val_accuracy: 0.9945 - val_loss: 0.0207\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000173.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 174\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACAE688920>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.9075 - loss: 1.9117 - val_accuracy: 0.9738 - val_loss: 0.0723\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9771 - loss: 0.0612 - val_accuracy: 0.9868 - val_loss: 0.0387\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9883 - loss: 0.0336 - val_accuracy: 0.9870 - val_loss: 0.0401\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9916 - loss: 0.0241 - val_accuracy: 0.9821 - val_loss: 0.0564\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9927 - loss: 0.0206 - val_accuracy: 0.9870 - val_loss: 0.0453\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9931 - loss: 0.0190 - val_accuracy: 0.9880 - val_loss: 0.0393\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9950 - loss: 0.0146 - val_accuracy: 0.9831 - val_loss: 0.0780\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9970 - loss: 0.0101 - val_accuracy: 0.9868 - val_loss: 0.0518\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9944 - loss: 0.0143 - val_accuracy: 0.9913 - val_loss: 0.0499\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 69ms/step - accuracy: 0.9954 - loss: 0.0135 - val_accuracy: 0.9919 - val_loss: 0.0509\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000174.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9919227738376675 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 175\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7977170E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 66ms/step - accuracy: 0.9146 - loss: 1.1481 - val_accuracy: 0.9896 - val_loss: 0.0427\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9798 - loss: 0.0560 - val_accuracy: 0.9951 - val_loss: 0.0207\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9906 - loss: 0.0294 - val_accuracy: 0.9955 - val_loss: 0.0171\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9934 - loss: 0.0220 - val_accuracy: 0.9953 - val_loss: 0.0153\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9946 - loss: 0.0152 - val_accuracy: 0.9935 - val_loss: 0.0185\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9940 - loss: 0.0151 - val_accuracy: 0.9901 - val_loss: 0.0387\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9948 - loss: 0.0158 - val_accuracy: 0.9935 - val_loss: 0.0207\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9947 - loss: 0.0151 - val_accuracy: 0.9965 - val_loss: 0.0173\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9967 - loss: 0.0101 - val_accuracy: 0.9927 - val_loss: 0.0284\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9929 - val_loss: 0.0226\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000175.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9929078014184397 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 176\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A84355B9E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 67ms/step - accuracy: 0.9122 - loss: 1.8437 - val_accuracy: 0.9874 - val_loss: 0.0416\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9836 - loss: 0.0435 - val_accuracy: 0.9921 - val_loss: 0.0238\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9910 - loss: 0.0249 - val_accuracy: 0.9925 - val_loss: 0.0233\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9941 - loss: 0.0168 - val_accuracy: 0.9933 - val_loss: 0.0266\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9932 - loss: 0.0183 - val_accuracy: 0.9945 - val_loss: 0.0200\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9951 - loss: 0.0141 - val_accuracy: 0.9957 - val_loss: 0.0146\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9949 - loss: 0.0140 - val_accuracy: 0.9968 - val_loss: 0.0105\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9956 - loss: 0.0123 - val_accuracy: 0.9974 - val_loss: 0.0075\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9967 - loss: 0.0102 - val_accuracy: 0.9949 - val_loss: 0.0193\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9964 - loss: 0.0110 - val_accuracy: 0.9951 - val_loss: 0.0177\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000176.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 177\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A830E13E60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9060 - loss: 1.1823 - val_accuracy: 0.9734 - val_loss: 0.0759\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 71ms/step - accuracy: 0.9748 - loss: 0.0731 - val_accuracy: 0.9911 - val_loss: 0.0331\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9871 - loss: 0.0391 - val_accuracy: 0.9943 - val_loss: 0.0183\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9892 - loss: 0.0321 - val_accuracy: 0.9945 - val_loss: 0.0200\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9933 - loss: 0.0226 - val_accuracy: 0.9959 - val_loss: 0.0174\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9934 - loss: 0.0207 - val_accuracy: 0.9968 - val_loss: 0.0168\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9951 - loss: 0.0162 - val_accuracy: 0.9970 - val_loss: 0.0127\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9950 - loss: 0.0155 - val_accuracy: 0.9967 - val_loss: 0.0154\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9959 - loss: 0.0138 - val_accuracy: 0.9951 - val_loss: 0.0221\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9973 - loss: 0.0090 - val_accuracy: 0.9967 - val_loss: 0.0185\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000177.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9966509062253743 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 178\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A845943530>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 66ms/step - accuracy: 0.8978 - loss: 2.7676 - val_accuracy: 0.9738 - val_loss: 0.0693\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9695 - loss: 0.0776 - val_accuracy: 0.9866 - val_loss: 0.0346\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9846 - loss: 0.0390 - val_accuracy: 0.9939 - val_loss: 0.0156\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9879 - loss: 0.0312 - val_accuracy: 0.9896 - val_loss: 0.0307\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9914 - loss: 0.0258 - val_accuracy: 0.9939 - val_loss: 0.0165\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9938 - loss: 0.0186 - val_accuracy: 0.9888 - val_loss: 0.0302\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9958 - loss: 0.0162 - val_accuracy: 0.9945 - val_loss: 0.0208\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9938 - loss: 0.0208 - val_accuracy: 0.9972 - val_loss: 0.0087\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9955 - loss: 0.0126 - val_accuracy: 0.9921 - val_loss: 0.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9957 - loss: 0.0128 - val_accuracy: 0.9953 - val_loss: 0.0201\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000178.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 179\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001A7AB9DAB70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8955 - loss: 2.5138 - val_accuracy: 0.9836 - val_loss: 0.0610\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9721 - loss: 0.0749 - val_accuracy: 0.9927 - val_loss: 0.0289\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9885 - loss: 0.0367 - val_accuracy: 0.9937 - val_loss: 0.0230\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9917 - loss: 0.0235 - val_accuracy: 0.9937 - val_loss: 0.0253\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9940 - loss: 0.0193 - val_accuracy: 0.9880 - val_loss: 0.0365\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9940 - loss: 0.0192 - val_accuracy: 0.9949 - val_loss: 0.0217\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9944 - loss: 0.0162 - val_accuracy: 0.9968 - val_loss: 0.0146\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9945 - loss: 0.0152 - val_accuracy: 0.9970 - val_loss: 0.0155\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9963 - loss: 0.0116 - val_accuracy: 0.9957 - val_loss: 0.0175\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9970 - loss: 0.0086 - val_accuracy: 0.9968 - val_loss: 0.0171\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000179.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9968479117415288 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 180\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACBEB06B70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8954 - loss: 2.0337 - val_accuracy: 0.9738 - val_loss: 0.0690\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9750 - loss: 0.0678 - val_accuracy: 0.9571 - val_loss: 0.1316\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9872 - loss: 0.0319 - val_accuracy: 0.9917 - val_loss: 0.0281\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9937 - loss: 0.0188 - val_accuracy: 0.9886 - val_loss: 0.0418\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9938 - loss: 0.0195 - val_accuracy: 0.9955 - val_loss: 0.0185\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9952 - loss: 0.0148 - val_accuracy: 0.9919 - val_loss: 0.0302\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9904 - loss: 0.0307 - val_accuracy: 0.9947 - val_loss: 0.0208\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9976 - loss: 0.0078 - val_accuracy: 0.9795 - val_loss: 0.0658\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9958 - loss: 0.0115 - val_accuracy: 0.9951 - val_loss: 0.0197\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9965 - loss: 0.0100 - val_accuracy: 0.9949 - val_loss: 0.0252\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000180.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9948778565799843 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 181\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACBEA7D7C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9066 - loss: 1.5836 - val_accuracy: 0.9781 - val_loss: 0.0641\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9730 - loss: 0.0727 - val_accuracy: 0.9813 - val_loss: 0.0570\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9850 - loss: 0.0424 - val_accuracy: 0.9915 - val_loss: 0.0304\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9893 - loss: 0.0320 - val_accuracy: 0.9880 - val_loss: 0.0427\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9920 - loss: 0.0229 - val_accuracy: 0.9923 - val_loss: 0.0285\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9938 - loss: 0.0196 - val_accuracy: 0.9935 - val_loss: 0.0248\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9938 - loss: 0.0168 - val_accuracy: 0.9917 - val_loss: 0.0330\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9942 - loss: 0.0173 - val_accuracy: 0.9927 - val_loss: 0.0262\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9944 - loss: 0.0159 - val_accuracy: 0.9894 - val_loss: 0.0386\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9946 - loss: 0.0142 - val_accuracy: 0.9941 - val_loss: 0.0428\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000181.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9940898345153665 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 182\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACBEBEC890>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.9227 - loss: 0.9612 - val_accuracy: 0.9836 - val_loss: 0.0418\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9878 - loss: 0.0395 - val_accuracy: 0.9896 - val_loss: 0.0267\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9929 - loss: 0.0185 - val_accuracy: 0.9957 - val_loss: 0.0138\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9936 - loss: 0.0171 - val_accuracy: 0.9967 - val_loss: 0.0163\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9956 - loss: 0.0123 - val_accuracy: 0.9961 - val_loss: 0.0138\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9966 - loss: 0.0107 - val_accuracy: 0.9959 - val_loss: 0.0121\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9960 - loss: 0.0112 - val_accuracy: 0.9968 - val_loss: 0.0104\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9975 - loss: 0.0068 - val_accuracy: 0.9939 - val_loss: 0.0200\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9955 - loss: 0.0163 - val_accuracy: 0.9972 - val_loss: 0.0121\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9967 - loss: 0.0117 - val_accuracy: 0.9967 - val_loss: 0.0117\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000182.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9966509062253743 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 183\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACBF014830>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.9036 - loss: 2.1702 - val_accuracy: 0.9744 - val_loss: 0.0692\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9685 - loss: 0.0805 - val_accuracy: 0.9815 - val_loss: 0.0541\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9829 - loss: 0.0478 - val_accuracy: 0.9894 - val_loss: 0.0279\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9897 - loss: 0.0308 - val_accuracy: 0.9852 - val_loss: 0.0461\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9841 - loss: 0.0385 - val_accuracy: 0.9915 - val_loss: 0.0300\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9934 - loss: 0.0189 - val_accuracy: 0.9931 - val_loss: 0.0200\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9939 - loss: 0.0188 - val_accuracy: 0.9951 - val_loss: 0.0163\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9941 - loss: 0.0168 - val_accuracy: 0.9947 - val_loss: 0.0129\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9952 - loss: 0.0165 - val_accuracy: 0.9947 - val_loss: 0.0182\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9949 - loss: 0.0169 - val_accuracy: 0.9953 - val_loss: 0.0170\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000183.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9952718676122931 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 184\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACD604F710>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.9124 - loss: 1.0673 - val_accuracy: 0.9838 - val_loss: 0.0447\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9787 - loss: 0.0575 - val_accuracy: 0.9953 - val_loss: 0.0175\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9904 - loss: 0.0299 - val_accuracy: 0.9957 - val_loss: 0.0138\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9904 - loss: 0.0271 - val_accuracy: 0.9974 - val_loss: 0.0118\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9945 - loss: 0.0200 - val_accuracy: 0.9955 - val_loss: 0.0150\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9958 - loss: 0.0158 - val_accuracy: 0.9961 - val_loss: 0.0144\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9799 - val_loss: 0.0865\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9931 - loss: 0.0187 - val_accuracy: 0.9927 - val_loss: 0.0269\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9964 - loss: 0.0103 - val_accuracy: 0.9974 - val_loss: 0.0141\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9965 - loss: 0.0111 - val_accuracy: 0.9984 - val_loss: 0.0073\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000184.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9984239558707644 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 185\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACCF9D28A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9095 - loss: 2.3869 - val_accuracy: 0.9752 - val_loss: 0.0768\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9707 - loss: 0.0772 - val_accuracy: 0.9868 - val_loss: 0.0427\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9844 - loss: 0.0414 - val_accuracy: 0.9915 - val_loss: 0.0304\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9871 - loss: 0.0346 - val_accuracy: 0.9907 - val_loss: 0.0381\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9927 - loss: 0.0231 - val_accuracy: 0.9913 - val_loss: 0.0309\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9935 - loss: 0.0177 - val_accuracy: 0.9785 - val_loss: 0.0670\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9940 - loss: 0.0175 - val_accuracy: 0.9943 - val_loss: 0.0241\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9943 - loss: 0.0181 - val_accuracy: 0.9921 - val_loss: 0.0347\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9955 - loss: 0.0147 - val_accuracy: 0.9921 - val_loss: 0.0373\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9971 - loss: 0.0087 - val_accuracy: 0.9901 - val_loss: 0.0449\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000185.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9901497241922774 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 186\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACD1F98F50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8987 - loss: 3.6117 - val_accuracy: 0.9783 - val_loss: 0.0600\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9735 - loss: 0.0706 - val_accuracy: 0.9911 - val_loss: 0.0274\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9882 - loss: 0.0330 - val_accuracy: 0.9925 - val_loss: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9928 - loss: 0.0222 - val_accuracy: 0.9917 - val_loss: 0.0286\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9943 - loss: 0.0169 - val_accuracy: 0.9957 - val_loss: 0.0143\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9935 - loss: 0.0185 - val_accuracy: 0.9951 - val_loss: 0.0146\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9951 - loss: 0.0148 - val_accuracy: 0.9947 - val_loss: 0.0188\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9935 - loss: 0.0179 - val_accuracy: 0.9961 - val_loss: 0.0137\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9962 - loss: 0.0098 - val_accuracy: 0.9949 - val_loss: 0.0152\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 0.9931 - val_loss: 0.0189\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000186.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9931048069345941 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 187\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACD849B260>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.9237 - loss: 1.5791 - val_accuracy: 0.9838 - val_loss: 0.0474\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9836 - loss: 0.0436 - val_accuracy: 0.9935 - val_loss: 0.0206\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9920 - loss: 0.0223 - val_accuracy: 0.9955 - val_loss: 0.0147\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9950 - loss: 0.0137 - val_accuracy: 0.9931 - val_loss: 0.0225\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9958 - loss: 0.0114 - val_accuracy: 0.9951 - val_loss: 0.0221\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9978 - loss: 0.0082 - val_accuracy: 0.9965 - val_loss: 0.0121\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9970 - loss: 0.0089 - val_accuracy: 0.9965 - val_loss: 0.0169\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9968 - loss: 0.0106 - val_accuracy: 0.9941 - val_loss: 0.0263\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9960 - loss: 0.0097 - val_accuracy: 0.9945 - val_loss: 0.0173\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9961 - loss: 0.0139 - val_accuracy: 0.9933 - val_loss: 0.0205\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000187.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9933018124507487 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 188\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACDCAE6150>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.9060 - loss: 0.8892 - val_accuracy: 0.9811 - val_loss: 0.0695\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9716 - loss: 0.0767 - val_accuracy: 0.9854 - val_loss: 0.0491\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9829 - loss: 0.0522 - val_accuracy: 0.9870 - val_loss: 0.0465\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9889 - loss: 0.0403 - val_accuracy: 0.9915 - val_loss: 0.0371\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9888 - loss: 0.0333 - val_accuracy: 0.9941 - val_loss: 0.0304\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9910 - loss: 0.0315 - val_accuracy: 0.9876 - val_loss: 0.0553\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9926 - loss: 0.0233 - val_accuracy: 0.9941 - val_loss: 0.0252\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9942 - loss: 0.0195 - val_accuracy: 0.9929 - val_loss: 0.0376\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9903 - loss: 0.0254 - val_accuracy: 0.9943 - val_loss: 0.0230\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9944 - loss: 0.0158 - val_accuracy: 0.9945 - val_loss: 0.0282\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000188.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9944838455476753 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 189\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACDA751040>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.9211 - loss: 1.0946 - val_accuracy: 0.9868 - val_loss: 0.0403\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9845 - loss: 0.0446 - val_accuracy: 0.9882 - val_loss: 0.0416\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9900 - loss: 0.0265 - val_accuracy: 0.9953 - val_loss: 0.0153\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9955 - loss: 0.0150 - val_accuracy: 0.9913 - val_loss: 0.0275\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9940 - loss: 0.0152 - val_accuracy: 0.9929 - val_loss: 0.0245\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9952 - loss: 0.0144 - val_accuracy: 0.9965 - val_loss: 0.0118\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9950 - loss: 0.0149 - val_accuracy: 0.9927 - val_loss: 0.0278\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9971 - loss: 0.0092 - val_accuracy: 0.9931 - val_loss: 0.0237\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9955 - loss: 0.0133 - val_accuracy: 0.9892 - val_loss: 0.0493\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9958 - loss: 0.0111 - val_accuracy: 0.9961 - val_loss: 0.0163\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000189.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.996059889676911 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 190\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACE10B44D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.9107 - loss: 1.8309 - val_accuracy: 0.9718 - val_loss: 0.0803\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9750 - loss: 0.0668 - val_accuracy: 0.9858 - val_loss: 0.0485\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9855 - loss: 0.0406 - val_accuracy: 0.9898 - val_loss: 0.0314\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9915 - loss: 0.0253 - val_accuracy: 0.9860 - val_loss: 0.0407\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9947 - loss: 0.0173 - val_accuracy: 0.9890 - val_loss: 0.0363\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9952 - loss: 0.0139 - val_accuracy: 0.9866 - val_loss: 0.0511\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9944 - loss: 0.0139 - val_accuracy: 0.9919 - val_loss: 0.0333\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9956 - loss: 0.0137 - val_accuracy: 0.9905 - val_loss: 0.0319\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9976 - loss: 0.0094 - val_accuracy: 0.9907 - val_loss: 0.0331\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9967 - loss: 0.0103 - val_accuracy: 0.9947 - val_loss: 0.0225\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000190.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9946808510638298 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 191\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACDC97D700>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.9038 - loss: 1.3251 - val_accuracy: 0.9768 - val_loss: 0.0632\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9763 - loss: 0.0658 - val_accuracy: 0.9882 - val_loss: 0.0348\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9890 - loss: 0.0346 - val_accuracy: 0.9949 - val_loss: 0.0195\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9915 - loss: 0.0243 - val_accuracy: 0.9949 - val_loss: 0.0215\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9936 - loss: 0.0179 - val_accuracy: 0.9931 - val_loss: 0.0290\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9928 - loss: 0.0198 - val_accuracy: 0.9909 - val_loss: 0.0277\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9927 - loss: 0.0206 - val_accuracy: 0.9933 - val_loss: 0.0274\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9946 - loss: 0.0140 - val_accuracy: 0.9923 - val_loss: 0.0294\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9949 - loss: 0.0141 - val_accuracy: 0.9957 - val_loss: 0.0182\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9956 - loss: 0.0149 - val_accuracy: 0.9935 - val_loss: 0.0285\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000191.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9934988179669031 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 192\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACE4521C10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.9000 - loss: 1.8140 - val_accuracy: 0.9653 - val_loss: 0.0850\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9734 - loss: 0.0722 - val_accuracy: 0.9923 - val_loss: 0.0318\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 69ms/step - accuracy: 0.9839 - loss: 0.0441 - val_accuracy: 0.9937 - val_loss: 0.0271\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9907 - loss: 0.0313 - val_accuracy: 0.9963 - val_loss: 0.0165\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9923 - loss: 0.0259 - val_accuracy: 0.9941 - val_loss: 0.0246\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9939 - loss: 0.0202 - val_accuracy: 0.9933 - val_loss: 0.0286\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9924 - loss: 0.0203 - val_accuracy: 0.9959 - val_loss: 0.0222\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9962 - loss: 0.0150 - val_accuracy: 0.9959 - val_loss: 0.0283\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9950 - loss: 0.0143 - val_accuracy: 0.9919 - val_loss: 0.0483\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9961 - loss: 0.0116 - val_accuracy: 0.9972 - val_loss: 0.0144\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000192.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9972419227738377 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 193\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACE4C07170>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.9117 - loss: 0.8002 - val_accuracy: 0.9840 - val_loss: 0.0457\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9824 - loss: 0.0481 - val_accuracy: 0.9903 - val_loss: 0.0306\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9894 - loss: 0.0294 - val_accuracy: 0.9953 - val_loss: 0.0147\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9945 - loss: 0.0182 - val_accuracy: 0.9953 - val_loss: 0.0138\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9953 - loss: 0.0127 - val_accuracy: 0.9892 - val_loss: 0.0335\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 72ms/step - accuracy: 0.9944 - loss: 0.0162 - val_accuracy: 0.9913 - val_loss: 0.0282\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9951 - loss: 0.0106 - val_accuracy: 0.9945 - val_loss: 0.0186\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9975 - loss: 0.0075 - val_accuracy: 0.9939 - val_loss: 0.0213\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9958 - loss: 0.0118 - val_accuracy: 0.9913 - val_loss: 0.0328\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9972 - loss: 0.0097 - val_accuracy: 0.9957 - val_loss: 0.0156\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000193.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 194\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACE4D51D00>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.9124 - loss: 0.7835 - val_accuracy: 0.9653 - val_loss: 0.0845\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9835 - loss: 0.0465 - val_accuracy: 0.9941 - val_loss: 0.0203\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9916 - loss: 0.0246 - val_accuracy: 0.9953 - val_loss: 0.0146\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9946 - loss: 0.0153 - val_accuracy: 0.9967 - val_loss: 0.0107\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9957 - loss: 0.0120 - val_accuracy: 0.9955 - val_loss: 0.0136\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9956 - loss: 0.0138 - val_accuracy: 0.9939 - val_loss: 0.0235\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9979 - loss: 0.0075 - val_accuracy: 0.9961 - val_loss: 0.0141\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9944 - loss: 0.0157 - val_accuracy: 0.9951 - val_loss: 0.0163\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9975 - loss: 0.0058 - val_accuracy: 0.9945 - val_loss: 0.0191\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9968 - loss: 0.0088 - val_accuracy: 0.9957 - val_loss: 0.0227\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000194.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 195\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACF0010650>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8832 - loss: 1.6929 - val_accuracy: 0.9563 - val_loss: 0.1087\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9586 - loss: 0.1144 - val_accuracy: 0.9840 - val_loss: 0.0732\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9775 - loss: 0.0750 - val_accuracy: 0.9866 - val_loss: 0.0573\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9817 - loss: 0.0584 - val_accuracy: 0.9943 - val_loss: 0.0266\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9891 - loss: 0.0397 - val_accuracy: 0.9935 - val_loss: 0.0281\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9869 - loss: 0.0372 - val_accuracy: 0.9925 - val_loss: 0.0337\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9928 - loss: 0.0248 - val_accuracy: 0.9953 - val_loss: 0.0264\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9922 - loss: 0.0245 - val_accuracy: 0.9878 - val_loss: 0.0496\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9892 - loss: 0.0305 - val_accuracy: 0.9963 - val_loss: 0.0201\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9938 - loss: 0.0197 - val_accuracy: 0.9959 - val_loss: 0.0280\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000195.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9958628841607565 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 196\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACE4B6B170>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8890 - loss: 3.6258 - val_accuracy: 0.9795 - val_loss: 0.0839\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9681 - loss: 0.0909 - val_accuracy: 0.9876 - val_loss: 0.0586\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9805 - loss: 0.0572 - val_accuracy: 0.9898 - val_loss: 0.0340\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9876 - loss: 0.0451 - val_accuracy: 0.9921 - val_loss: 0.0297\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9915 - loss: 0.0316 - val_accuracy: 0.9933 - val_loss: 0.0225\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9926 - loss: 0.0233 - val_accuracy: 0.9744 - val_loss: 0.1019\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9933 - loss: 0.0217 - val_accuracy: 0.9974 - val_loss: 0.0129\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9945 - loss: 0.0163 - val_accuracy: 0.9898 - val_loss: 0.0422\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9955 - loss: 0.0139 - val_accuracy: 0.9965 - val_loss: 0.0201\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9954 - loss: 0.0143 - val_accuracy: 0.9951 - val_loss: 0.0197\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000196.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 197\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACF02D0CB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 79ms/step - accuracy: 0.9118 - loss: 1.1443 - val_accuracy: 0.9616 - val_loss: 0.0922\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9762 - loss: 0.0642 - val_accuracy: 0.9890 - val_loss: 0.0299\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9879 - loss: 0.0331 - val_accuracy: 0.9941 - val_loss: 0.0185\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9919 - loss: 0.0224 - val_accuracy: 0.9850 - val_loss: 0.0475\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9926 - loss: 0.0191 - val_accuracy: 0.9945 - val_loss: 0.0208\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9955 - loss: 0.0118 - val_accuracy: 0.9949 - val_loss: 0.0185\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9951 - loss: 0.0112 - val_accuracy: 0.9953 - val_loss: 0.0173\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9954 - loss: 0.0140 - val_accuracy: 0.9921 - val_loss: 0.0279\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9974 - loss: 0.0085 - val_accuracy: 0.9957 - val_loss: 0.0162\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9977 - loss: 0.0063 - val_accuracy: 0.9963 - val_loss: 0.0152\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000197.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9962568951930654 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 198\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACF0725010>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8982 - loss: 2.7520 - val_accuracy: 0.9758 - val_loss: 0.0631\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9761 - loss: 0.0640 - val_accuracy: 0.9870 - val_loss: 0.0368\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9890 - loss: 0.0322 - val_accuracy: 0.9860 - val_loss: 0.0362\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9922 - loss: 0.0217 - val_accuracy: 0.9850 - val_loss: 0.0424\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9930 - loss: 0.0214 - val_accuracy: 0.9882 - val_loss: 0.0330\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9950 - loss: 0.0175 - val_accuracy: 0.9905 - val_loss: 0.0292\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9950 - loss: 0.0121 - val_accuracy: 0.9917 - val_loss: 0.0260\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9957 - loss: 0.0121 - val_accuracy: 0.9929 - val_loss: 0.0237\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9949 - loss: 0.0147 - val_accuracy: 0.9911 - val_loss: 0.0356\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9968 - loss: 0.0097 - val_accuracy: 0.9921 - val_loss: 0.0269\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000198.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9921197793538219 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 199\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001AD00C81370>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.9079 - loss: 2.2051 - val_accuracy: 0.9829 - val_loss: 0.0545\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9750 - loss: 0.0594 - val_accuracy: 0.9905 - val_loss: 0.0274\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9890 - loss: 0.0346 - val_accuracy: 0.9937 - val_loss: 0.0191\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9921 - loss: 0.0247 - val_accuracy: 0.9941 - val_loss: 0.0157\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9939 - loss: 0.0183 - val_accuracy: 0.9970 - val_loss: 0.0102\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9952 - loss: 0.0131 - val_accuracy: 0.9965 - val_loss: 0.0164\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9939 - loss: 0.0158 - val_accuracy: 0.9947 - val_loss: 0.0265\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9925 - loss: 0.0207 - val_accuracy: 0.9953 - val_loss: 0.0111\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9953 - loss: 0.0153 - val_accuracy: 0.9965 - val_loss: 0.0137\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9938 - loss: 0.0147 - val_accuracy: 0.9911 - val_loss: 0.0325\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000199.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9911347517730497 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 200\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "        job = self.__job__\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=job.kernelSize, activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Conv2D(filters=64, kernel_size=job.kernelSize, activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=job.poolSize)(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <config.configuration.Job object at 0x000001ACF0613AA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.9080 - loss: 0.9219 - val_accuracy: 0.9618 - val_loss: 0.1022\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9756 - loss: 0.0880 - val_accuracy: 0.9909 - val_loss: 0.0574\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9865 - loss: 0.0548 - val_accuracy: 0.9882 - val_loss: 0.0626\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9896 - loss: 0.0397 - val_accuracy: 0.9968 - val_loss: 0.0311\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9904 - loss: 0.0340 - val_accuracy: 0.9943 - val_loss: 0.0322\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9925 - loss: 0.0287 - val_accuracy: 0.9909 - val_loss: 0.0520\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9956 - loss: 0.0190 - val_accuracy: 0.9945 - val_loss: 0.0368\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9926 - loss: 0.0231 - val_accuracy: 0.9933 - val_loss: 0.0412\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9927 - loss: 0.0224 - val_accuracy: 0.9880 - val_loss: 0.0810\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9954 - loss: 0.0149 - val_accuracy: 0.9905 - val_loss: 0.0638\n",
      "Saving model: output/ASVspoof-2019_training_mfcc_2025-03-29T17-36-18.258353_random_000200.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n"
     ]
    }
   ],
   "source": [
    "bulkTrainingProc = BulkModelTrainingProcessor(job,\n",
    "                                              model_cnn_definition.ModelCnnDefinition,\n",
    "                                              BasicModelTrainingProcessor,\n",
    "                                              BasicModelEvaluationProcessor)\n",
    "\n",
    "bulkTrainingProc.process(random_state_lowValue, random_state_highValue, X, y_encoded, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-deepfake-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
