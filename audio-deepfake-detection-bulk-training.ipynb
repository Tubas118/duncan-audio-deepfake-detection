{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookName = 'audio-deepfake-detection-bulk-training'\n",
    "runJobId = 'ASVspoof-2019_training'\n",
    "# runJobId = 'ASVspoof-2019_small-eval-1'     # This should fail\n",
    "random_state_lowValue = 4\n",
    "random_state_highValue = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration.configuration as configuration\n",
    "import model_definitions.model_cnn_definition as model_cnn_definition\n",
    "from preprocessors.mel_spectrogram import MelSpectrogramPreprocessor\n",
    "from notebook_utils import notebookToPython\n",
    "from processors.basic_model_training_processor import BasicModelTrainingProcessor\n",
    "from processors.basic_model_evaluation_processor import BasicModelEvaluationProcessor\n",
    "from processors.bulk_model_training_processor import BulkModelTrainingProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write python file\n",
      "Generating new model name: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349.libjob\n",
      "Assigned model name: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349.libjob\n"
     ]
    }
   ],
   "source": [
    "config = configuration.ConfigLoader('config.yml')\n",
    "\n",
    "notebookToPython(notebookName)\n",
    "job = config.getJobConfig(runJobId)\n",
    "\n",
    "if (job.newModelGenerated == False):\n",
    "    raise ValueError(\"This notebook is meant for training. Select a job without a value for 'persisted-model' set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:/Users/tubas/workspace/Deepfake/data/ASVspoof-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt...\n",
      "fullDataPath: C:/Users/tubas/workspace/Deepfake/data/ASVspoof-2019/LA/ASVspoof2019_LA_train/flac\n",
      "Loading audio files: 1269\n",
      "Loading audio files: 2538\n",
      "Loading audio files: 3807\n",
      "Loading audio files: 5076\n",
      "Loading audio files: 6345\n",
      "Loading audio files: 7614\n",
      "Loading audio files: 8883\n",
      "Loading audio files: 10152\n",
      "Loading audio files: 11421\n",
      "Loading audio files: 12690\n",
      "Loading audio files: 13959\n",
      "Loading audio files: 15228\n",
      "Loading audio files: 16497\n",
      "Loading audio files: 17766\n",
      "Loading audio files: 19035\n",
      "Loading audio files: 20304\n",
      "Loading audio files: 21573\n",
      "Loading audio files: 22842\n",
      "Loading audio files: 24111\n",
      "Loading audio files: 25380\n",
      "Number of audio files load: 25380\n"
     ]
    }
   ],
   "source": [
    "generator = MelSpectrogramPreprocessor()\n",
    "X, y_encoded = generator.extract_features_multipleSource(job, job.dataPathSuffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting training and test data - traininSplitRandomState: 4\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21AEEA660>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.8808 - loss: 5.0670 - val_accuracy: 0.9092 - val_loss: 0.2096\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9058 - loss: 0.2441 - val_accuracy: 0.8976 - val_loss: 0.3244\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8962 - loss: 0.3303 - val_accuracy: 0.8976 - val_loss: 0.3251\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.8993 - loss: 0.3240 - val_accuracy: 0.8976 - val_loss: 0.3242\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8980 - loss: 0.3265 - val_accuracy: 0.8976 - val_loss: 0.3242\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8987 - loss: 0.3240 - val_accuracy: 0.8976 - val_loss: 0.3253\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8990 - loss: 0.3237 - val_accuracy: 0.8976 - val_loss: 0.3249\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.8963 - loss: 0.3283 - val_accuracy: 0.8976 - val_loss: 0.3239\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9008 - loss: 0.3171 - val_accuracy: 0.8976 - val_loss: 0.3240\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.8980 - loss: 0.3245 - val_accuracy: 0.8976 - val_loss: 0.3239\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000004.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8975571315996848 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 5\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21EAFC3B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.8854 - loss: 4.3655 - val_accuracy: 0.8956 - val_loss: 0.2427\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.8982 - loss: 0.2456 - val_accuracy: 0.9033 - val_loss: 0.2163\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9074 - loss: 0.2137 - val_accuracy: 0.9080 - val_loss: 0.2076\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9186 - loss: 0.1890 - val_accuracy: 0.9460 - val_loss: 0.1342\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9373 - loss: 0.1352 - val_accuracy: 0.9592 - val_loss: 0.1105\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9502 - loss: 0.1119 - val_accuracy: 0.9663 - val_loss: 0.0923\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9660 - loss: 0.0867 - val_accuracy: 0.9638 - val_loss: 0.0894\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9709 - loss: 0.0742 - val_accuracy: 0.9689 - val_loss: 0.0952\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9771 - loss: 0.0665 - val_accuracy: 0.9815 - val_loss: 0.0677\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9803 - loss: 0.0573 - val_accuracy: 0.9815 - val_loss: 0.0602\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000005.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9814814814814815 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 6\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2806FEC00>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.8855 - loss: 3.6541 - val_accuracy: 0.9212 - val_loss: 0.1830\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9196 - loss: 0.2012 - val_accuracy: 0.9498 - val_loss: 0.1398\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9469 - loss: 0.1428 - val_accuracy: 0.9639 - val_loss: 0.0920\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9552 - loss: 0.1124 - val_accuracy: 0.9697 - val_loss: 0.0860\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9595 - loss: 0.1174 - val_accuracy: 0.9750 - val_loss: 0.0708\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9732 - loss: 0.0768 - val_accuracy: 0.9726 - val_loss: 0.0662\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9742 - loss: 0.0671 - val_accuracy: 0.9799 - val_loss: 0.0599\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9787 - loss: 0.0596 - val_accuracy: 0.9815 - val_loss: 0.0559\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9817 - loss: 0.0548 - val_accuracy: 0.9833 - val_loss: 0.0436\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9743 - loss: 0.0716 - val_accuracy: 0.9846 - val_loss: 0.0445\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000006.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9846335697399528 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 7\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31E31E030>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.8846 - loss: 3.9765 - val_accuracy: 0.9052 - val_loss: 0.2280\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9008 - loss: 0.2352 - val_accuracy: 0.9145 - val_loss: 0.1930\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9123 - loss: 0.2040 - val_accuracy: 0.9141 - val_loss: 0.2068\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9281 - loss: 0.1811 - val_accuracy: 0.9163 - val_loss: 0.2147\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9151 - loss: 0.2217 - val_accuracy: 0.9279 - val_loss: 0.1968\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9266 - loss: 0.1861 - val_accuracy: 0.9320 - val_loss: 0.1746\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9358 - loss: 0.1607 - val_accuracy: 0.9448 - val_loss: 0.1459\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9487 - loss: 0.1289 - val_accuracy: 0.9533 - val_loss: 0.1315\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9558 - loss: 0.1139 - val_accuracy: 0.9537 - val_loss: 0.1256\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9627 - loss: 0.1042 - val_accuracy: 0.9691 - val_loss: 0.0867\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000007.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.969070133963751 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 8\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21EBB5EB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8860 - loss: 3.5023 - val_accuracy: 0.8962 - val_loss: 0.2674\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.8995 - loss: 0.2986 - val_accuracy: 0.8981 - val_loss: 0.2401\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 61ms/step - accuracy: 0.9007 - loss: 0.2952 - val_accuracy: 0.8962 - val_loss: 0.3277\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9022 - loss: 0.3161 - val_accuracy: 0.8962 - val_loss: 0.3297\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9016 - loss: 0.3163 - val_accuracy: 0.8962 - val_loss: 0.3282\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9008 - loss: 0.3177 - val_accuracy: 0.8962 - val_loss: 0.3282\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.8934 - loss: 0.3317 - val_accuracy: 0.8962 - val_loss: 0.3284\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8998 - loss: 0.3213 - val_accuracy: 0.8962 - val_loss: 0.3326\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.8963 - loss: 0.3282 - val_accuracy: 0.8962 - val_loss: 0.3282\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9005 - loss: 0.3198 - val_accuracy: 0.8962 - val_loss: 0.3297\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000008.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8961780929866037 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 9\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21EAFDD60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8894 - loss: 3.4237 - val_accuracy: 0.8995 - val_loss: 0.2194\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9024 - loss: 0.2481 - val_accuracy: 0.9043 - val_loss: 0.2213\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9067 - loss: 0.2321 - val_accuracy: 0.9165 - val_loss: 0.1965\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9174 - loss: 0.1997 - val_accuracy: 0.9370 - val_loss: 0.1660\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9370 - loss: 0.1580 - val_accuracy: 0.9632 - val_loss: 0.1093\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9643 - loss: 0.0969 - val_accuracy: 0.9799 - val_loss: 0.0596\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9765 - loss: 0.0664 - val_accuracy: 0.9791 - val_loss: 0.0532\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9794 - loss: 0.0596 - val_accuracy: 0.9740 - val_loss: 0.0728\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9803 - loss: 0.0512 - val_accuracy: 0.9900 - val_loss: 0.0357\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9894 - loss: 0.0325 - val_accuracy: 0.9896 - val_loss: 0.0337\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000009.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.989558707643814 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 10\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2FC29FCB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8852 - loss: 6.2079 - val_accuracy: 0.9056 - val_loss: 0.2170\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9162 - loss: 0.1958 - val_accuracy: 0.9488 - val_loss: 0.1455\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9362 - loss: 0.1537 - val_accuracy: 0.9547 - val_loss: 0.1408\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9513 - loss: 0.1195 - val_accuracy: 0.9671 - val_loss: 0.0899\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9635 - loss: 0.0982 - val_accuracy: 0.9732 - val_loss: 0.0692\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9718 - loss: 0.0805 - val_accuracy: 0.9405 - val_loss: 0.1497\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9676 - loss: 0.0840 - val_accuracy: 0.9803 - val_loss: 0.0623\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9836 - loss: 0.0444 - val_accuracy: 0.9787 - val_loss: 0.0647\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9823 - loss: 0.0538 - val_accuracy: 0.9852 - val_loss: 0.0475\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9876 - loss: 0.0335 - val_accuracy: 0.9872 - val_loss: 0.0441\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000010.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9871946414499606 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 11\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EAD6A5A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 65ms/step - accuracy: 0.8881 - loss: 3.2147 - val_accuracy: 0.9092 - val_loss: 0.2327\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9098 - loss: 0.2176 - val_accuracy: 0.9165 - val_loss: 0.2064\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9183 - loss: 0.2057 - val_accuracy: 0.9186 - val_loss: 0.1926\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9318 - loss: 0.1706 - val_accuracy: 0.9261 - val_loss: 0.1809\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9376 - loss: 0.1576 - val_accuracy: 0.9429 - val_loss: 0.1603\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9482 - loss: 0.1308 - val_accuracy: 0.9452 - val_loss: 0.1584\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9586 - loss: 0.1105 - val_accuracy: 0.9482 - val_loss: 0.1536\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9569 - loss: 0.1026 - val_accuracy: 0.9468 - val_loss: 0.1856\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9576 - loss: 0.1303 - val_accuracy: 0.9513 - val_loss: 0.1562\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9649 - loss: 0.0847 - val_accuracy: 0.9624 - val_loss: 0.1304\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000011.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9623719464144996 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 12\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2D9F0FF20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.8857 - loss: 2.6988 - val_accuracy: 0.9151 - val_loss: 0.2001\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9118 - loss: 0.2146 - val_accuracy: 0.9245 - val_loss: 0.2029\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9482 - loss: 0.1294 - val_accuracy: 0.9718 - val_loss: 0.0823\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9733 - loss: 0.0738 - val_accuracy: 0.9848 - val_loss: 0.0496\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9778 - loss: 0.0601 - val_accuracy: 0.9801 - val_loss: 0.0653\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9838 - loss: 0.0483 - val_accuracy: 0.9874 - val_loss: 0.0449\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9865 - loss: 0.0404 - val_accuracy: 0.9886 - val_loss: 0.0430\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9872 - loss: 0.0364 - val_accuracy: 0.9888 - val_loss: 0.0439\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9907 - loss: 0.0287 - val_accuracy: 0.9862 - val_loss: 0.0522\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9911 - loss: 0.0226 - val_accuracy: 0.9905 - val_loss: 0.0411\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000012.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 13\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2D9F43CE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.8884 - loss: 2.4774 - val_accuracy: 0.9232 - val_loss: 0.1910\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9265 - loss: 0.1955 - val_accuracy: 0.9362 - val_loss: 0.1483\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9458 - loss: 0.1407 - val_accuracy: 0.9541 - val_loss: 0.1230\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9651 - loss: 0.1024 - val_accuracy: 0.9762 - val_loss: 0.0665\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9723 - loss: 0.0755 - val_accuracy: 0.9809 - val_loss: 0.0598\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9784 - loss: 0.0627 - val_accuracy: 0.9791 - val_loss: 0.0626\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9802 - loss: 0.0504 - val_accuracy: 0.9819 - val_loss: 0.0516\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9835 - loss: 0.0451 - val_accuracy: 0.9864 - val_loss: 0.0450\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9892 - loss: 0.0302 - val_accuracy: 0.9813 - val_loss: 0.0655\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9886 - loss: 0.0328 - val_accuracy: 0.9819 - val_loss: 0.0575\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000013.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9818754925137904 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 14\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DA026270>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8869 - loss: 3.9474 - val_accuracy: 0.8993 - val_loss: 0.4356\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8990 - loss: 0.3292 - val_accuracy: 0.9444 - val_loss: 0.1407\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9327 - loss: 0.1422 - val_accuracy: 0.9567 - val_loss: 0.1240\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9429 - loss: 0.1162 - val_accuracy: 0.9602 - val_loss: 0.1189\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9538 - loss: 0.1062 - val_accuracy: 0.9691 - val_loss: 0.0999\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9637 - loss: 0.0915 - val_accuracy: 0.9647 - val_loss: 0.1026\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9691 - loss: 0.0805 - val_accuracy: 0.9691 - val_loss: 0.1145\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9716 - loss: 0.0763 - val_accuracy: 0.9768 - val_loss: 0.0825\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9734 - loss: 0.0686 - val_accuracy: 0.9793 - val_loss: 0.0827\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9783 - loss: 0.0594 - val_accuracy: 0.9819 - val_loss: 0.0759\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000014.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9818754925137904 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 15\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21D9E2B70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8823 - loss: 4.2901 - val_accuracy: 0.9163 - val_loss: 0.2066\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9025 - loss: 0.2196 - val_accuracy: 0.9539 - val_loss: 0.1328\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9296 - loss: 0.1529 - val_accuracy: 0.9572 - val_loss: 0.1079\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9610 - loss: 0.1001 - val_accuracy: 0.9720 - val_loss: 0.0922\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9693 - loss: 0.0791 - val_accuracy: 0.9880 - val_loss: 0.0527\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9814 - loss: 0.0514 - val_accuracy: 0.9860 - val_loss: 0.0644\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9822 - loss: 0.0518 - val_accuracy: 0.9917 - val_loss: 0.0550\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9768 - loss: 0.0614 - val_accuracy: 0.9835 - val_loss: 0.0596\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9855 - loss: 0.0434 - val_accuracy: 0.9876 - val_loss: 0.0479\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9803 - loss: 0.0533 - val_accuracy: 0.9921 - val_loss: 0.0465\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000015.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9921197793538219 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 16\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21EB4BFB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8904 - loss: 2.0194 - val_accuracy: 0.9005 - val_loss: 0.3787\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9083 - loss: 0.2344 - val_accuracy: 0.9531 - val_loss: 0.1206\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9455 - loss: 0.1325 - val_accuracy: 0.9683 - val_loss: 0.0803\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9613 - loss: 0.0964 - val_accuracy: 0.9374 - val_loss: 0.1730\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9671 - loss: 0.0854 - val_accuracy: 0.9724 - val_loss: 0.0777\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9742 - loss: 0.0688 - val_accuracy: 0.9860 - val_loss: 0.0455\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9778 - loss: 0.0567 - val_accuracy: 0.9736 - val_loss: 0.0674\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9783 - loss: 0.0585 - val_accuracy: 0.9864 - val_loss: 0.0505\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9822 - loss: 0.0482 - val_accuracy: 0.9886 - val_loss: 0.0377\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9823 - loss: 0.0451 - val_accuracy: 0.9878 - val_loss: 0.0397\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000016.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.987785657998424 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 17\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DA0DC9B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8795 - loss: 6.1302 - val_accuracy: 0.8978 - val_loss: 0.4257\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9038 - loss: 0.2797 - val_accuracy: 0.9322 - val_loss: 0.1685\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9364 - loss: 0.1561 - val_accuracy: 0.9545 - val_loss: 0.1199\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9503 - loss: 0.1271 - val_accuracy: 0.9017 - val_loss: 0.2266\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9228 - loss: 0.1813 - val_accuracy: 0.9703 - val_loss: 0.0772\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9663 - loss: 0.0916 - val_accuracy: 0.9722 - val_loss: 0.0739\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9747 - loss: 0.0728 - val_accuracy: 0.9738 - val_loss: 0.0689\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9750 - loss: 0.0693 - val_accuracy: 0.9651 - val_loss: 0.0893\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9821 - loss: 0.0485 - val_accuracy: 0.9777 - val_loss: 0.0608\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9841 - loss: 0.0426 - val_accuracy: 0.9787 - val_loss: 0.0552\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000017.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9787234042553191 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 18\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31F96DE80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8889 - loss: 1.7796 - val_accuracy: 0.8932 - val_loss: 0.2649\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9047 - loss: 0.2318 - val_accuracy: 0.9092 - val_loss: 0.2449\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9253 - loss: 0.1842 - val_accuracy: 0.9346 - val_loss: 0.1658\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9437 - loss: 0.1405 - val_accuracy: 0.9602 - val_loss: 0.1063\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9601 - loss: 0.1076 - val_accuracy: 0.9744 - val_loss: 0.0726\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9708 - loss: 0.0788 - val_accuracy: 0.9726 - val_loss: 0.0721\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9776 - loss: 0.0632 - val_accuracy: 0.9744 - val_loss: 0.0698\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9811 - loss: 0.0537 - val_accuracy: 0.9836 - val_loss: 0.0467\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9843 - loss: 0.0462 - val_accuracy: 0.9823 - val_loss: 0.0484\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9899 - loss: 0.0306 - val_accuracy: 0.9874 - val_loss: 0.0344\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000018.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.987391646966115 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 19\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DD546060>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8822 - loss: 5.1597 - val_accuracy: 0.8987 - val_loss: 0.2187\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8977 - loss: 0.2463 - val_accuracy: 0.9007 - val_loss: 0.2606\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9081 - loss: 0.2183 - val_accuracy: 0.9279 - val_loss: 0.2011\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9207 - loss: 0.1846 - val_accuracy: 0.9277 - val_loss: 0.1698\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9229 - loss: 0.1881 - val_accuracy: 0.9374 - val_loss: 0.1552\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9414 - loss: 0.1463 - val_accuracy: 0.9389 - val_loss: 0.1729\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9289 - loss: 0.1832 - val_accuracy: 0.9454 - val_loss: 0.1390\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9556 - loss: 0.1156 - val_accuracy: 0.9543 - val_loss: 0.1231\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9607 - loss: 0.0969 - val_accuracy: 0.9511 - val_loss: 0.1422\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9674 - loss: 0.0897 - val_accuracy: 0.9663 - val_loss: 0.0959\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000019.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9663120567375887 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 20\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DD9CFD10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8796 - loss: 3.6599 - val_accuracy: 0.8983 - val_loss: 0.1959\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8975 - loss: 0.2088 - val_accuracy: 0.9003 - val_loss: 0.2449\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9072 - loss: 0.2246 - val_accuracy: 0.9537 - val_loss: 0.1296\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9380 - loss: 0.1588 - val_accuracy: 0.9604 - val_loss: 0.0988\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9504 - loss: 0.1237 - val_accuracy: 0.9401 - val_loss: 0.1445\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9579 - loss: 0.1084 - val_accuracy: 0.9730 - val_loss: 0.0711\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9747 - loss: 0.0704 - val_accuracy: 0.9840 - val_loss: 0.0464\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9797 - loss: 0.0583 - val_accuracy: 0.9813 - val_loss: 0.0491\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9843 - loss: 0.0470 - val_accuracy: 0.9874 - val_loss: 0.0420\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9892 - loss: 0.0326 - val_accuracy: 0.9917 - val_loss: 0.0232\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000020.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.991725768321513 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 21\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DE02FDD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8876 - loss: 3.4993 - val_accuracy: 0.9214 - val_loss: 0.1988\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9191 - loss: 0.1997 - val_accuracy: 0.9525 - val_loss: 0.1304\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9316 - loss: 0.1748 - val_accuracy: 0.9697 - val_loss: 0.0820\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9593 - loss: 0.1038 - val_accuracy: 0.9803 - val_loss: 0.0573\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9713 - loss: 0.0676 - val_accuracy: 0.9880 - val_loss: 0.0380\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9806 - loss: 0.0525 - val_accuracy: 0.9856 - val_loss: 0.0401\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9843 - loss: 0.0439 - val_accuracy: 0.9894 - val_loss: 0.0306\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9880 - loss: 0.0381 - val_accuracy: 0.9905 - val_loss: 0.0279\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9875 - loss: 0.0380 - val_accuracy: 0.9886 - val_loss: 0.0302\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9898 - loss: 0.0309 - val_accuracy: 0.9880 - val_loss: 0.0355\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000021.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9879826635145784 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 22\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DE100C50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8857 - loss: 4.0237 - val_accuracy: 0.8980 - val_loss: 0.3230\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9041 - loss: 0.2797 - val_accuracy: 0.9220 - val_loss: 0.1805\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9137 - loss: 0.2406 - val_accuracy: 0.8980 - val_loss: 0.3205\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8962 - loss: 0.3320 - val_accuracy: 0.8980 - val_loss: 0.3181\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9076 - loss: 0.2339 - val_accuracy: 0.9572 - val_loss: 0.1139\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9455 - loss: 0.1355 - val_accuracy: 0.9716 - val_loss: 0.0710\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9546 - loss: 0.1172 - val_accuracy: 0.9056 - val_loss: 0.2372\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9550 - loss: 0.1100 - val_accuracy: 0.9807 - val_loss: 0.0602\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9714 - loss: 0.0685 - val_accuracy: 0.9844 - val_loss: 0.0461\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9787 - loss: 0.0568 - val_accuracy: 0.9352 - val_loss: 0.1630\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000022.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9351851851851852 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 23\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DE6EE9C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.8772 - loss: 3.2717 - val_accuracy: 0.9033 - val_loss: 0.2131\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9080 - loss: 0.2206 - val_accuracy: 0.9267 - val_loss: 0.1834\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9293 - loss: 0.1643 - val_accuracy: 0.9746 - val_loss: 0.0752\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9525 - loss: 0.1202 - val_accuracy: 0.9630 - val_loss: 0.0880\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9684 - loss: 0.0741 - val_accuracy: 0.9876 - val_loss: 0.0398\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9717 - loss: 0.0662 - val_accuracy: 0.9880 - val_loss: 0.0349\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9814 - loss: 0.0456 - val_accuracy: 0.9876 - val_loss: 0.0331\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9842 - loss: 0.0442 - val_accuracy: 0.9913 - val_loss: 0.0333\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9857 - loss: 0.0380 - val_accuracy: 0.9915 - val_loss: 0.0267\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9893 - loss: 0.0288 - val_accuracy: 0.9876 - val_loss: 0.0407\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000023.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9875886524822695 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 24\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21DA59040>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8898 - loss: 4.4264 - val_accuracy: 0.9397 - val_loss: 0.1577\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9361 - loss: 0.1658 - val_accuracy: 0.9590 - val_loss: 0.1190\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9563 - loss: 0.1203 - val_accuracy: 0.9647 - val_loss: 0.1069\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9616 - loss: 0.1002 - val_accuracy: 0.9634 - val_loss: 0.0970\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9697 - loss: 0.0803 - val_accuracy: 0.9586 - val_loss: 0.1109\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9724 - loss: 0.0777 - val_accuracy: 0.9758 - val_loss: 0.0708\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9767 - loss: 0.0627 - val_accuracy: 0.9805 - val_loss: 0.0583\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9842 - loss: 0.0421 - val_accuracy: 0.9703 - val_loss: 0.0856\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9833 - loss: 0.0488 - val_accuracy: 0.9858 - val_loss: 0.0467\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9858 - loss: 0.0388 - val_accuracy: 0.9892 - val_loss: 0.0413\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000024.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9891646966115051 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 25\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21DA59040>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8853 - loss: 4.0411 - val_accuracy: 0.8999 - val_loss: 0.2180\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9054 - loss: 0.2169 - val_accuracy: 0.9531 - val_loss: 0.1058\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9434 - loss: 0.1332 - val_accuracy: 0.9693 - val_loss: 0.0851\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9553 - loss: 0.1063 - val_accuracy: 0.9773 - val_loss: 0.0607\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9681 - loss: 0.0873 - val_accuracy: 0.9819 - val_loss: 0.0479\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9768 - loss: 0.0715 - val_accuracy: 0.9773 - val_loss: 0.0614\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9785 - loss: 0.0604 - val_accuracy: 0.9787 - val_loss: 0.0695\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9830 - loss: 0.0527 - val_accuracy: 0.9827 - val_loss: 0.0468\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9806 - loss: 0.0549 - val_accuracy: 0.9896 - val_loss: 0.0370\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 63ms/step - accuracy: 0.9872 - loss: 0.0361 - val_accuracy: 0.9888 - val_loss: 0.0363\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000025.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9887706855791962 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 26\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DA51E840>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.8850 - loss: 3.7968 - val_accuracy: 0.9027 - val_loss: 0.2433\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9119 - loss: 0.2191 - val_accuracy: 0.9096 - val_loss: 0.2110\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9291 - loss: 0.1713 - val_accuracy: 0.9476 - val_loss: 0.1406\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9544 - loss: 0.1143 - val_accuracy: 0.9638 - val_loss: 0.1025\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9659 - loss: 0.0921 - val_accuracy: 0.9506 - val_loss: 0.1223\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9788 - loss: 0.0567 - val_accuracy: 0.9720 - val_loss: 0.0773\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9834 - loss: 0.0483 - val_accuracy: 0.9726 - val_loss: 0.0887\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9846 - loss: 0.0456 - val_accuracy: 0.9659 - val_loss: 0.0994\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9867 - loss: 0.0370 - val_accuracy: 0.9825 - val_loss: 0.0542\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9870 - loss: 0.0381 - val_accuracy: 0.9884 - val_loss: 0.0465\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000026.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9883766745468873 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 27\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31D6F2390>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8920 - loss: 3.0539 - val_accuracy: 0.9245 - val_loss: 0.1836\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9237 - loss: 0.1902 - val_accuracy: 0.9559 - val_loss: 0.1224\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9491 - loss: 0.1451 - val_accuracy: 0.9681 - val_loss: 0.0978\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9659 - loss: 0.0981 - val_accuracy: 0.9726 - val_loss: 0.0779\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9767 - loss: 0.0665 - val_accuracy: 0.9578 - val_loss: 0.1184\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9768 - loss: 0.0664 - val_accuracy: 0.9726 - val_loss: 0.0794\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9800 - loss: 0.0586 - val_accuracy: 0.9738 - val_loss: 0.0786\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9838 - loss: 0.0459 - val_accuracy: 0.9807 - val_loss: 0.0688\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9814 - loss: 0.0555 - val_accuracy: 0.9781 - val_loss: 0.0796\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9883 - loss: 0.0305 - val_accuracy: 0.9744 - val_loss: 0.0881\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000027.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9743892828999212 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 28\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DD58E990>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8895 - loss: 2.5524 - val_accuracy: 0.9046 - val_loss: 0.2675\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9295 - loss: 0.1788 - val_accuracy: 0.9647 - val_loss: 0.0866\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9571 - loss: 0.1118 - val_accuracy: 0.9844 - val_loss: 0.0451\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9782 - loss: 0.0612 - val_accuracy: 0.9850 - val_loss: 0.0446\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9841 - loss: 0.0452 - val_accuracy: 0.9894 - val_loss: 0.0267\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9887 - loss: 0.0397 - val_accuracy: 0.9840 - val_loss: 0.0383\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9806 - loss: 0.0656 - val_accuracy: 0.9835 - val_loss: 0.0427\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9888 - loss: 0.0367 - val_accuracy: 0.9872 - val_loss: 0.0311\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9829 - loss: 0.0498 - val_accuracy: 0.9927 - val_loss: 0.0243\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9925 - loss: 0.0233 - val_accuracy: 0.9890 - val_loss: 0.0251\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000028.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9889676910953507 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 29\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DD4A1F10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8858 - loss: 5.0964 - val_accuracy: 0.9023 - val_loss: 0.2137\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9067 - loss: 0.2092 - val_accuracy: 0.9267 - val_loss: 0.1920\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9279 - loss: 0.1678 - val_accuracy: 0.9267 - val_loss: 0.1781\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9277 - loss: 0.1853 - val_accuracy: 0.8976 - val_loss: 0.3307\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8936 - loss: 0.3425 - val_accuracy: 0.8976 - val_loss: 0.3309\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8959 - loss: 0.3392 - val_accuracy: 0.8976 - val_loss: 0.3265\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8982 - loss: 0.3269 - val_accuracy: 0.8976 - val_loss: 0.3236\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8948 - loss: 0.3327 - val_accuracy: 0.8976 - val_loss: 0.3240\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8993 - loss: 0.3222 - val_accuracy: 0.8976 - val_loss: 0.3238\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8992 - loss: 0.3237 - val_accuracy: 0.8976 - val_loss: 0.3256\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000029.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8975571315996848 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 30\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DA438080>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8876 - loss: 2.0741 - val_accuracy: 0.9090 - val_loss: 0.1992\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9215 - loss: 0.1891 - val_accuracy: 0.9511 - val_loss: 0.1245\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9529 - loss: 0.1198 - val_accuracy: 0.9683 - val_loss: 0.0893\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9729 - loss: 0.0815 - val_accuracy: 0.9866 - val_loss: 0.0466\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9803 - loss: 0.0575 - val_accuracy: 0.9848 - val_loss: 0.0495\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9880 - loss: 0.0372 - val_accuracy: 0.9896 - val_loss: 0.0407\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0322 - val_accuracy: 0.9896 - val_loss: 0.0394\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9925 - loss: 0.0221 - val_accuracy: 0.9900 - val_loss: 0.0424\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9932 - loss: 0.0225 - val_accuracy: 0.9929 - val_loss: 0.0310\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9916 - loss: 0.0258 - val_accuracy: 0.9923 - val_loss: 0.0332\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000030.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9923167848699763 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 31\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DD1A6600>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8829 - loss: 4.4481 - val_accuracy: 0.9025 - val_loss: 0.2281\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 61ms/step - accuracy: 0.8999 - loss: 0.2346 - val_accuracy: 0.9196 - val_loss: 0.1656\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9208 - loss: 0.1730 - val_accuracy: 0.9545 - val_loss: 0.1136\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9362 - loss: 0.1391 - val_accuracy: 0.9287 - val_loss: 0.1868\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9389 - loss: 0.1363 - val_accuracy: 0.9539 - val_loss: 0.1206\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9414 - loss: 0.1216 - val_accuracy: 0.9710 - val_loss: 0.0810\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9638 - loss: 0.0936 - val_accuracy: 0.9724 - val_loss: 0.0774\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9539 - loss: 0.1125 - val_accuracy: 0.9596 - val_loss: 0.1049\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9622 - loss: 0.1015 - val_accuracy: 0.9712 - val_loss: 0.0863\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9693 - loss: 0.0846 - val_accuracy: 0.9777 - val_loss: 0.0668\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000031.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9777383766745469 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 32\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DD241A60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8898 - loss: 2.0017 - val_accuracy: 0.9289 - val_loss: 0.1852\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9181 - loss: 0.2126 - val_accuracy: 0.9196 - val_loss: 0.2019\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9281 - loss: 0.1867 - val_accuracy: 0.9442 - val_loss: 0.1563\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9520 - loss: 0.1265 - val_accuracy: 0.9401 - val_loss: 0.1513\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9607 - loss: 0.1021 - val_accuracy: 0.9695 - val_loss: 0.0923\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9763 - loss: 0.0640 - val_accuracy: 0.9718 - val_loss: 0.0869\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9772 - loss: 0.0602 - val_accuracy: 0.9703 - val_loss: 0.1012\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9829 - loss: 0.0455 - val_accuracy: 0.9777 - val_loss: 0.1153\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9846 - loss: 0.0416 - val_accuracy: 0.9852 - val_loss: 0.0704\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9863 - loss: 0.0395 - val_accuracy: 0.9803 - val_loss: 0.0959\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000032.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9802994483845547 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 33\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E5D24CE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 65ms/step - accuracy: 0.8796 - loss: 3.6689 - val_accuracy: 0.9078 - val_loss: 0.2136\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9083 - loss: 0.2150 - val_accuracy: 0.9362 - val_loss: 0.1624\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9380 - loss: 0.1506 - val_accuracy: 0.9683 - val_loss: 0.0959\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9577 - loss: 0.1169 - val_accuracy: 0.9663 - val_loss: 0.0977\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9477 - loss: 0.1319 - val_accuracy: 0.9657 - val_loss: 0.1054\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9688 - loss: 0.0799 - val_accuracy: 0.9750 - val_loss: 0.0897\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9744 - loss: 0.0757 - val_accuracy: 0.9809 - val_loss: 0.0584\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9776 - loss: 0.0575 - val_accuracy: 0.9789 - val_loss: 0.0742\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9804 - loss: 0.0545 - val_accuracy: 0.9748 - val_loss: 0.0848\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9841 - loss: 0.0420 - val_accuracy: 0.9840 - val_loss: 0.0551\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000033.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9840425531914894 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 34\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B21ABFCCE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8863 - loss: 3.2236 - val_accuracy: 0.9007 - val_loss: 0.2383\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9114 - loss: 0.2082 - val_accuracy: 0.9687 - val_loss: 0.0891\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9639 - loss: 0.0953 - val_accuracy: 0.9718 - val_loss: 0.0741\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9727 - loss: 0.0762 - val_accuracy: 0.9803 - val_loss: 0.0595\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9800 - loss: 0.0558 - val_accuracy: 0.9710 - val_loss: 0.0852\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9815 - loss: 0.0498 - val_accuracy: 0.9827 - val_loss: 0.0631\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9869 - loss: 0.0372 - val_accuracy: 0.9771 - val_loss: 0.0801\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9837 - loss: 0.0469 - val_accuracy: 0.9898 - val_loss: 0.0428\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9896 - loss: 0.0281 - val_accuracy: 0.9811 - val_loss: 0.0659\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9878 - loss: 0.0342 - val_accuracy: 0.9872 - val_loss: 0.0449\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000034.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9871946414499606 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 35\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E59C1D90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8877 - loss: 6.3402 - val_accuracy: 0.9366 - val_loss: 0.1504\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9240 - loss: 0.1724 - val_accuracy: 0.9588 - val_loss: 0.1002\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9438 - loss: 0.1254 - val_accuracy: 0.9687 - val_loss: 0.0930\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9614 - loss: 0.1012 - val_accuracy: 0.9681 - val_loss: 0.0905\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9642 - loss: 0.0954 - val_accuracy: 0.9762 - val_loss: 0.0714\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9728 - loss: 0.0744 - val_accuracy: 0.9671 - val_loss: 0.0846\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 62ms/step - accuracy: 0.9700 - loss: 0.0875 - val_accuracy: 0.9793 - val_loss: 0.0590\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9649 - loss: 0.0938 - val_accuracy: 0.9787 - val_loss: 0.0593\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9762 - loss: 0.0674 - val_accuracy: 0.9766 - val_loss: 0.0707\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9757 - loss: 0.0676 - val_accuracy: 0.9807 - val_loss: 0.0600\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000035.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9806934594168637 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 36\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E583DB50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.8817 - loss: 4.9002 - val_accuracy: 0.9043 - val_loss: 0.2208\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.9042 - loss: 0.2217 - val_accuracy: 0.9135 - val_loss: 0.2255\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9205 - loss: 0.1915 - val_accuracy: 0.9450 - val_loss: 0.1692\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9426 - loss: 0.1308 - val_accuracy: 0.9718 - val_loss: 0.0985\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9632 - loss: 0.0954 - val_accuracy: 0.9706 - val_loss: 0.1015\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9695 - loss: 0.0813 - val_accuracy: 0.9638 - val_loss: 0.1086\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9724 - loss: 0.0775 - val_accuracy: 0.9657 - val_loss: 0.1186\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9797 - loss: 0.0612 - val_accuracy: 0.9835 - val_loss: 0.0782\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9780 - loss: 0.0645 - val_accuracy: 0.9750 - val_loss: 0.0999\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9769 - loss: 0.0693 - val_accuracy: 0.9835 - val_loss: 0.0742\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000036.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.983451536643026 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 37\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DD58F3B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 63ms/step - accuracy: 0.8935 - loss: 2.4101 - val_accuracy: 0.9017 - val_loss: 0.2193\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9108 - loss: 0.2199 - val_accuracy: 0.9220 - val_loss: 0.1733\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9299 - loss: 0.1625 - val_accuracy: 0.9559 - val_loss: 0.1015\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9520 - loss: 0.1179 - val_accuracy: 0.9527 - val_loss: 0.1105\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 62ms/step - accuracy: 0.9623 - loss: 0.0910 - val_accuracy: 0.9718 - val_loss: 0.0701\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9645 - loss: 0.0874 - val_accuracy: 0.9539 - val_loss: 0.1180\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9698 - loss: 0.0771 - val_accuracy: 0.9576 - val_loss: 0.0991\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9772 - loss: 0.0598 - val_accuracy: 0.9817 - val_loss: 0.0534\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9798 - loss: 0.0560 - val_accuracy: 0.9762 - val_loss: 0.0606\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9856 - loss: 0.0388 - val_accuracy: 0.9864 - val_loss: 0.0395\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000037.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9864066193853428 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 38\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E679BD70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.8846 - loss: 2.1058 - val_accuracy: 0.9147 - val_loss: 0.1903\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9126 - loss: 0.2209 - val_accuracy: 0.8960 - val_loss: 0.3046\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9014 - loss: 0.2613 - val_accuracy: 0.8993 - val_loss: 0.2263\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9070 - loss: 0.2079 - val_accuracy: 0.9251 - val_loss: 0.1634\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9218 - loss: 0.1841 - val_accuracy: 0.9651 - val_loss: 0.0901\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9591 - loss: 0.1154 - val_accuracy: 0.9898 - val_loss: 0.0361\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9807 - loss: 0.0545 - val_accuracy: 0.9693 - val_loss: 0.0785\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9814 - loss: 0.0535 - val_accuracy: 0.9915 - val_loss: 0.0284\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9886 - loss: 0.0331 - val_accuracy: 0.9905 - val_loss: 0.0342\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9896 - loss: 0.0317 - val_accuracy: 0.9929 - val_loss: 0.0216\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000038.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9929078014184397 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 39\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E54BA000>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8782 - loss: 3.9931 - val_accuracy: 0.9068 - val_loss: 0.2215\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9035 - loss: 0.2337 - val_accuracy: 0.9113 - val_loss: 0.2103\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9076 - loss: 0.2199 - val_accuracy: 0.9240 - val_loss: 0.1951\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9179 - loss: 0.1909 - val_accuracy: 0.9405 - val_loss: 0.1473\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9337 - loss: 0.1453 - val_accuracy: 0.9618 - val_loss: 0.1253\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9423 - loss: 0.1244 - val_accuracy: 0.9649 - val_loss: 0.1062\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9569 - loss: 0.1062 - val_accuracy: 0.9760 - val_loss: 0.0890\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9573 - loss: 0.1114 - val_accuracy: 0.9728 - val_loss: 0.0898\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9667 - loss: 0.0919 - val_accuracy: 0.9764 - val_loss: 0.0833\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9688 - loss: 0.0760 - val_accuracy: 0.9681 - val_loss: 0.0995\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000039.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9680851063829787 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 40\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E6DEE480>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8887 - loss: 2.8786 - val_accuracy: 0.9021 - val_loss: 0.2177\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9124 - loss: 0.2124 - val_accuracy: 0.9305 - val_loss: 0.1667\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9260 - loss: 0.1814 - val_accuracy: 0.9456 - val_loss: 0.1271\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9460 - loss: 0.1404 - val_accuracy: 0.9679 - val_loss: 0.0906\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9668 - loss: 0.0902 - val_accuracy: 0.9596 - val_loss: 0.0928\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9728 - loss: 0.0723 - val_accuracy: 0.9728 - val_loss: 0.0708\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9818 - loss: 0.0522 - val_accuracy: 0.9856 - val_loss: 0.0349\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9846 - loss: 0.0424 - val_accuracy: 0.9868 - val_loss: 0.0369\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9864 - loss: 0.0351 - val_accuracy: 0.9913 - val_loss: 0.0290\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9900 - loss: 0.0295 - val_accuracy: 0.9911 - val_loss: 0.0251\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000040.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9911347517730497 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 41\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E6E33050>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8823 - loss: 6.0941 - val_accuracy: 0.8976 - val_loss: 0.2538\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9065 - loss: 0.2209 - val_accuracy: 0.9206 - val_loss: 0.2038\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9196 - loss: 0.1914 - val_accuracy: 0.9346 - val_loss: 0.1618\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9295 - loss: 0.1657 - val_accuracy: 0.9691 - val_loss: 0.0888\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9592 - loss: 0.1084 - val_accuracy: 0.9787 - val_loss: 0.0725\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9714 - loss: 0.0787 - val_accuracy: 0.9641 - val_loss: 0.0939\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9761 - loss: 0.0666 - val_accuracy: 0.9833 - val_loss: 0.0522\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9833 - loss: 0.0456 - val_accuracy: 0.9831 - val_loss: 0.0638\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9814 - loss: 0.0481 - val_accuracy: 0.9039 - val_loss: 0.2362\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9493 - loss: 0.1350 - val_accuracy: 0.9748 - val_loss: 0.0715\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000041.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9747832939322301 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 42\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DFC4BF50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8848 - loss: 3.1400 - val_accuracy: 0.9052 - val_loss: 0.2055\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9102 - loss: 0.2049 - val_accuracy: 0.9287 - val_loss: 0.1730\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9320 - loss: 0.1691 - val_accuracy: 0.9604 - val_loss: 0.0926\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9566 - loss: 0.1203 - val_accuracy: 0.9868 - val_loss: 0.0549\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9781 - loss: 0.0607 - val_accuracy: 0.9836 - val_loss: 0.0517\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9846 - loss: 0.0471 - val_accuracy: 0.9900 - val_loss: 0.0277\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9819 - loss: 0.0495 - val_accuracy: 0.9913 - val_loss: 0.0266\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9892 - loss: 0.0344 - val_accuracy: 0.9840 - val_loss: 0.0458\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9732 - loss: 0.0763 - val_accuracy: 0.9931 - val_loss: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9894 - loss: 0.0316 - val_accuracy: 0.9905 - val_loss: 0.0273\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000042.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 43\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DFDFDD00>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8857 - loss: 3.1910 - val_accuracy: 0.9056 - val_loss: 0.2543\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9093 - loss: 0.2397 - val_accuracy: 0.9129 - val_loss: 0.2063\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9168 - loss: 0.2060 - val_accuracy: 0.9149 - val_loss: 0.2108\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9241 - loss: 0.1863 - val_accuracy: 0.9340 - val_loss: 0.1651\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9299 - loss: 0.1877 - val_accuracy: 0.9405 - val_loss: 0.1424\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9465 - loss: 0.1253 - val_accuracy: 0.9474 - val_loss: 0.1179\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9586 - loss: 0.1032 - val_accuracy: 0.9547 - val_loss: 0.1065\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9641 - loss: 0.0916 - val_accuracy: 0.9610 - val_loss: 0.0960\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9666 - loss: 0.0851 - val_accuracy: 0.9697 - val_loss: 0.0856\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9721 - loss: 0.0765 - val_accuracy: 0.9683 - val_loss: 0.0898\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000043.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9682821118991332 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 44\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DFF9B0E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8881 - loss: 4.7935 - val_accuracy: 0.9236 - val_loss: 0.1870\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9225 - loss: 0.1933 - val_accuracy: 0.9480 - val_loss: 0.1277\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9446 - loss: 0.1417 - val_accuracy: 0.9643 - val_loss: 0.0869\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9593 - loss: 0.1014 - val_accuracy: 0.9710 - val_loss: 0.0752\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9696 - loss: 0.0782 - val_accuracy: 0.9833 - val_loss: 0.0482\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9791 - loss: 0.0598 - val_accuracy: 0.9746 - val_loss: 0.0623\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9420 - loss: 0.2089 - val_accuracy: 0.9762 - val_loss: 0.0642\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9723 - loss: 0.0714 - val_accuracy: 0.9878 - val_loss: 0.0413\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9826 - loss: 0.0467 - val_accuracy: 0.9799 - val_loss: 0.0603\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9827 - loss: 0.0515 - val_accuracy: 0.9892 - val_loss: 0.0445\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000044.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9891646966115051 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 45\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E06673B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8848 - loss: 3.2026 - val_accuracy: 0.9062 - val_loss: 0.2421\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9163 - loss: 0.2047 - val_accuracy: 0.9062 - val_loss: 0.2681\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9179 - loss: 0.2097 - val_accuracy: 0.9405 - val_loss: 0.1685\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9393 - loss: 0.1608 - val_accuracy: 0.9415 - val_loss: 0.1717\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9419 - loss: 0.1515 - val_accuracy: 0.9557 - val_loss: 0.1267\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9579 - loss: 0.1177 - val_accuracy: 0.9535 - val_loss: 0.1268\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9623 - loss: 0.0954 - val_accuracy: 0.9468 - val_loss: 0.1368\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9612 - loss: 0.0948 - val_accuracy: 0.9683 - val_loss: 0.0989\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9656 - loss: 0.0858 - val_accuracy: 0.9649 - val_loss: 0.1003\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9737 - loss: 0.0694 - val_accuracy: 0.9738 - val_loss: 0.0862\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000045.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9737982663514578 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 46\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DFD2ED80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8813 - loss: 4.0737 - val_accuracy: 0.8995 - val_loss: 0.2580\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9021 - loss: 0.2585 - val_accuracy: 0.8985 - val_loss: 0.3217\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8968 - loss: 0.3308 - val_accuracy: 0.8985 - val_loss: 0.3229\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8992 - loss: 0.3224 - val_accuracy: 0.8985 - val_loss: 0.3233\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8988 - loss: 0.3212 - val_accuracy: 0.8985 - val_loss: 0.3224\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8985 - loss: 0.3248 - val_accuracy: 0.8985 - val_loss: 0.3225\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8997 - loss: 0.3222 - val_accuracy: 0.8985 - val_loss: 0.3231\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8963 - loss: 0.3274 - val_accuracy: 0.8985 - val_loss: 0.3226\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8984 - loss: 0.3240 - val_accuracy: 0.8985 - val_loss: 0.3226\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9007 - loss: 0.3171 - val_accuracy: 0.8985 - val_loss: 0.3228\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000046.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8985421591804571 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 47\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2FC29FE90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.8851 - loss: 3.5957 - val_accuracy: 0.9155 - val_loss: 0.2154\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9133 - loss: 0.2247 - val_accuracy: 0.9358 - val_loss: 0.1625\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9297 - loss: 0.1802 - val_accuracy: 0.9490 - val_loss: 0.1368\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9404 - loss: 0.1535 - val_accuracy: 0.9563 - val_loss: 0.1246\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9496 - loss: 0.1352 - val_accuracy: 0.9515 - val_loss: 0.1301\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9482 - loss: 0.1385 - val_accuracy: 0.9543 - val_loss: 0.1282\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9609 - loss: 0.1038 - val_accuracy: 0.9616 - val_loss: 0.1077\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9667 - loss: 0.0906 - val_accuracy: 0.9752 - val_loss: 0.0814\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9711 - loss: 0.0755 - val_accuracy: 0.9726 - val_loss: 0.0877\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9704 - loss: 0.0792 - val_accuracy: 0.9854 - val_loss: 0.0608\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000047.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9854215918045706 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 48\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E54E0D10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.8891 - loss: 2.1129 - val_accuracy: 0.9149 - val_loss: 0.1962\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9237 - loss: 0.1830 - val_accuracy: 0.9139 - val_loss: 0.2681\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9343 - loss: 0.1704 - val_accuracy: 0.9559 - val_loss: 0.1170\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9588 - loss: 0.1141 - val_accuracy: 0.9687 - val_loss: 0.0984\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9668 - loss: 0.0867 - val_accuracy: 0.9750 - val_loss: 0.0779\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9720 - loss: 0.0758 - val_accuracy: 0.9679 - val_loss: 0.0904\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9783 - loss: 0.0663 - val_accuracy: 0.9817 - val_loss: 0.0595\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9819 - loss: 0.0520 - val_accuracy: 0.9868 - val_loss: 0.0452\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9847 - loss: 0.0447 - val_accuracy: 0.9799 - val_loss: 0.0754\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9855 - loss: 0.0424 - val_accuracy: 0.9844 - val_loss: 0.0511\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000048.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9844365642237982 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 49\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E753A8A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8850 - loss: 4.4495 - val_accuracy: 0.9106 - val_loss: 0.2179\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9067 - loss: 0.2282 - val_accuracy: 0.9259 - val_loss: 0.1738\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9237 - loss: 0.1871 - val_accuracy: 0.9403 - val_loss: 0.1457\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9379 - loss: 0.1521 - val_accuracy: 0.9584 - val_loss: 0.1107\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9609 - loss: 0.1019 - val_accuracy: 0.9809 - val_loss: 0.0580\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9749 - loss: 0.0683 - val_accuracy: 0.9779 - val_loss: 0.0624\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9776 - loss: 0.0611 - val_accuracy: 0.9901 - val_loss: 0.0306\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9853 - loss: 0.0444 - val_accuracy: 0.9874 - val_loss: 0.0389\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9848 - loss: 0.0395 - val_accuracy: 0.9913 - val_loss: 0.0340\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9876 - loss: 0.0392 - val_accuracy: 0.9766 - val_loss: 0.0715\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000049.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9765563435776202 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 50\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E0CEF350>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8875 - loss: 2.6093 - val_accuracy: 0.9031 - val_loss: 0.3314\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9222 - loss: 0.1976 - val_accuracy: 0.9773 - val_loss: 0.0699\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9605 - loss: 0.1037 - val_accuracy: 0.9547 - val_loss: 0.1354\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9675 - loss: 0.0848 - val_accuracy: 0.9681 - val_loss: 0.0967\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9751 - loss: 0.0663 - val_accuracy: 0.9886 - val_loss: 0.0383\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9822 - loss: 0.0487 - val_accuracy: 0.9880 - val_loss: 0.0375\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9834 - loss: 0.0495 - val_accuracy: 0.9903 - val_loss: 0.0336\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9850 - loss: 0.0423 - val_accuracy: 0.9915 - val_loss: 0.0254\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9868 - loss: 0.0392 - val_accuracy: 0.9905 - val_loss: 0.0330\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9863 - loss: 0.0394 - val_accuracy: 0.9917 - val_loss: 0.0279\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000050.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.991725768321513 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 51\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E7485BE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8835 - loss: 3.9140 - val_accuracy: 0.8964 - val_loss: 0.2428\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9012 - loss: 0.2277 - val_accuracy: 0.8964 - val_loss: 0.2204\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9155 - loss: 0.1793 - val_accuracy: 0.9511 - val_loss: 0.1117\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9620 - loss: 0.1064 - val_accuracy: 0.9833 - val_loss: 0.0594\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9728 - loss: 0.0846 - val_accuracy: 0.9840 - val_loss: 0.0561\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9788 - loss: 0.0652 - val_accuracy: 0.9876 - val_loss: 0.0415\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9795 - loss: 0.0661 - val_accuracy: 0.9791 - val_loss: 0.0586\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9802 - loss: 0.0601 - val_accuracy: 0.9866 - val_loss: 0.0479\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9840 - loss: 0.0541 - val_accuracy: 0.9848 - val_loss: 0.0505\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9823 - loss: 0.0548 - val_accuracy: 0.9925 - val_loss: 0.0305\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000051.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9925137903861309 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 52\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EA848F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8892 - loss: 3.6931 - val_accuracy: 0.8966 - val_loss: 0.2167\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9147 - loss: 0.1967 - val_accuracy: 0.9052 - val_loss: 0.2571\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9408 - loss: 0.1499 - val_accuracy: 0.9494 - val_loss: 0.1305\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9603 - loss: 0.1033 - val_accuracy: 0.9736 - val_loss: 0.0760\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9742 - loss: 0.0705 - val_accuracy: 0.9746 - val_loss: 0.0790\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9777 - loss: 0.0576 - val_accuracy: 0.9752 - val_loss: 0.0707\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9829 - loss: 0.0474 - val_accuracy: 0.9813 - val_loss: 0.0519\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9859 - loss: 0.0422 - val_accuracy: 0.9803 - val_loss: 0.0616\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9867 - loss: 0.0374 - val_accuracy: 0.9854 - val_loss: 0.0477\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9882 - loss: 0.0330 - val_accuracy: 0.9789 - val_loss: 0.0642\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000052.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9789204097714737 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 53\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E134E150>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8913 - loss: 2.1539 - val_accuracy: 0.9143 - val_loss: 0.2023\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9325 - loss: 0.1568 - val_accuracy: 0.9693 - val_loss: 0.0794\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9572 - loss: 0.1117 - val_accuracy: 0.9498 - val_loss: 0.1524\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9730 - loss: 0.0697 - val_accuracy: 0.9856 - val_loss: 0.0501\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9819 - loss: 0.0493 - val_accuracy: 0.9793 - val_loss: 0.0615\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9865 - loss: 0.0426 - val_accuracy: 0.9852 - val_loss: 0.0506\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9876 - loss: 0.0371 - val_accuracy: 0.9917 - val_loss: 0.0356\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9883 - loss: 0.0361 - val_accuracy: 0.9882 - val_loss: 0.0382\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9891 - loss: 0.0287 - val_accuracy: 0.9900 - val_loss: 0.0440\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9867 - loss: 0.0393 - val_accuracy: 0.9909 - val_loss: 0.0268\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000053.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9909377462568952 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 54\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EA9B28A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8869 - loss: 4.2700 - val_accuracy: 0.9054 - val_loss: 0.2172\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9268 - loss: 0.1899 - val_accuracy: 0.9781 - val_loss: 0.0662\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9488 - loss: 0.1332 - val_accuracy: 0.9783 - val_loss: 0.0619\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9719 - loss: 0.0783 - val_accuracy: 0.9738 - val_loss: 0.0774\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9837 - loss: 0.0468 - val_accuracy: 0.9697 - val_loss: 0.0866\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9853 - loss: 0.0420 - val_accuracy: 0.9856 - val_loss: 0.0435\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9867 - loss: 0.0403 - val_accuracy: 0.9894 - val_loss: 0.0322\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9892 - loss: 0.0309 - val_accuracy: 0.9649 - val_loss: 0.1292\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9773 - loss: 0.0746 - val_accuracy: 0.9901 - val_loss: 0.0303\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9919 - loss: 0.0232 - val_accuracy: 0.9894 - val_loss: 0.0324\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000054.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9893617021276596 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 55\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2ED51A810>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8834 - loss: 2.6748 - val_accuracy: 0.9015 - val_loss: 0.2363\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9063 - loss: 0.2354 - val_accuracy: 0.9058 - val_loss: 0.2670\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9114 - loss: 0.2164 - val_accuracy: 0.9108 - val_loss: 0.2128\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9267 - loss: 0.1852 - val_accuracy: 0.9214 - val_loss: 0.1873\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9345 - loss: 0.1608 - val_accuracy: 0.9277 - val_loss: 0.1781\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9464 - loss: 0.1351 - val_accuracy: 0.9275 - val_loss: 0.1733\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9532 - loss: 0.1200 - val_accuracy: 0.9316 - val_loss: 0.1896\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9586 - loss: 0.1045 - val_accuracy: 0.9271 - val_loss: 0.1732\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9565 - loss: 0.1076 - val_accuracy: 0.9372 - val_loss: 0.1721\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9622 - loss: 0.0971 - val_accuracy: 0.9409 - val_loss: 0.1569\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000055.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9408983451536643 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 56\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EDD78F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8953 - loss: 3.9127 - val_accuracy: 0.9121 - val_loss: 0.1934\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9168 - loss: 0.1959 - val_accuracy: 0.9127 - val_loss: 0.2203\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9211 - loss: 0.2043 - val_accuracy: 0.9462 - val_loss: 0.1272\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9464 - loss: 0.1423 - val_accuracy: 0.9675 - val_loss: 0.0938\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9594 - loss: 0.1034 - val_accuracy: 0.9710 - val_loss: 0.0945\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9764 - loss: 0.0639 - val_accuracy: 0.9730 - val_loss: 0.0817\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9816 - loss: 0.0501 - val_accuracy: 0.9815 - val_loss: 0.0720\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9811 - loss: 0.0547 - val_accuracy: 0.9882 - val_loss: 0.0422\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9846 - loss: 0.0399 - val_accuracy: 0.9892 - val_loss: 0.0458\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9866 - loss: 0.0377 - val_accuracy: 0.9838 - val_loss: 0.0603\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000056.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9838455476753349 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 57\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EDBD3350>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8845 - loss: 2.1113 - val_accuracy: 0.9442 - val_loss: 0.1357\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9440 - loss: 0.1348 - val_accuracy: 0.9750 - val_loss: 0.0784\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9700 - loss: 0.0827 - val_accuracy: 0.9872 - val_loss: 0.0402\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9763 - loss: 0.0619 - val_accuracy: 0.9813 - val_loss: 0.0477\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9848 - loss: 0.0423 - val_accuracy: 0.9732 - val_loss: 0.0715\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9862 - loss: 0.0369 - val_accuracy: 0.9923 - val_loss: 0.0233\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9873 - loss: 0.0326 - val_accuracy: 0.9882 - val_loss: 0.0322\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9880 - loss: 0.0346 - val_accuracy: 0.9949 - val_loss: 0.0196\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9906 - loss: 0.0245 - val_accuracy: 0.9935 - val_loss: 0.0229\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9899 - loss: 0.0263 - val_accuracy: 0.9898 - val_loss: 0.0312\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000057.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9897557131599685 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 58\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2ED8767B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8891 - loss: 5.4759 - val_accuracy: 0.9074 - val_loss: 0.2095\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9283 - loss: 0.1766 - val_accuracy: 0.9498 - val_loss: 0.1231\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9665 - loss: 0.0882 - val_accuracy: 0.9823 - val_loss: 0.0537\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9782 - loss: 0.0584 - val_accuracy: 0.9718 - val_loss: 0.0701\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9869 - loss: 0.0387 - val_accuracy: 0.9900 - val_loss: 0.0298\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9890 - loss: 0.0315 - val_accuracy: 0.9905 - val_loss: 0.0292\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9911 - loss: 0.0260 - val_accuracy: 0.9844 - val_loss: 0.0476\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9907 - loss: 0.0306 - val_accuracy: 0.9939 - val_loss: 0.0178\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9916 - loss: 0.0231 - val_accuracy: 0.9925 - val_loss: 0.0245\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9935 - loss: 0.0212 - val_accuracy: 0.9931 - val_loss: 0.0227\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000058.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9931048069345941 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 59\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2F64AA240>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8892 - loss: 3.7932 - val_accuracy: 0.9129 - val_loss: 0.2111\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9029 - loss: 0.2469 - val_accuracy: 0.9041 - val_loss: 0.2689\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8997 - loss: 0.2637 - val_accuracy: 0.9035 - val_loss: 0.2494\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9015 - loss: 0.2311 - val_accuracy: 0.9078 - val_loss: 0.2203\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9092 - loss: 0.2231 - val_accuracy: 0.9145 - val_loss: 0.1970\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9203 - loss: 0.1937 - val_accuracy: 0.9293 - val_loss: 0.1754\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9320 - loss: 0.1728 - val_accuracy: 0.9310 - val_loss: 0.1711\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9446 - loss: 0.1366 - val_accuracy: 0.9543 - val_loss: 0.1175\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9637 - loss: 0.0931 - val_accuracy: 0.9638 - val_loss: 0.1001\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9715 - loss: 0.0737 - val_accuracy: 0.9732 - val_loss: 0.0749\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000059.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9732072498029944 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 60\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2DCB7FF80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8866 - loss: 2.2058 - val_accuracy: 0.9088 - val_loss: 0.2304\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9190 - loss: 0.2041 - val_accuracy: 0.9021 - val_loss: 0.2471\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9185 - loss: 0.2099 - val_accuracy: 0.9212 - val_loss: 0.1951\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9447 - loss: 0.1394 - val_accuracy: 0.9663 - val_loss: 0.1052\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9688 - loss: 0.0853 - val_accuracy: 0.9695 - val_loss: 0.1006\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9773 - loss: 0.0633 - val_accuracy: 0.9756 - val_loss: 0.0783\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9808 - loss: 0.0514 - val_accuracy: 0.9732 - val_loss: 0.0876\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9884 - loss: 0.0351 - val_accuracy: 0.9785 - val_loss: 0.0898\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9851 - loss: 0.0401 - val_accuracy: 0.9661 - val_loss: 0.1334\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9902 - loss: 0.0307 - val_accuracy: 0.9789 - val_loss: 0.1055\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000060.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9789204097714737 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 61\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E06491F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8862 - loss: 3.2058 - val_accuracy: 0.9368 - val_loss: 0.1464\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9443 - loss: 0.1390 - val_accuracy: 0.9817 - val_loss: 0.0587\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9709 - loss: 0.0784 - val_accuracy: 0.9815 - val_loss: 0.0528\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9761 - loss: 0.0621 - val_accuracy: 0.9789 - val_loss: 0.0574\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9858 - loss: 0.0439 - val_accuracy: 0.9888 - val_loss: 0.0336\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9870 - loss: 0.0376 - val_accuracy: 0.9917 - val_loss: 0.0259\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9816 - loss: 0.0596 - val_accuracy: 0.9858 - val_loss: 0.0382\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9891 - loss: 0.0362 - val_accuracy: 0.9915 - val_loss: 0.0292\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9914 - loss: 0.0256 - val_accuracy: 0.9911 - val_loss: 0.0303\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9912 - loss: 0.0256 - val_accuracy: 0.9917 - val_loss: 0.0270\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000061.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.991725768321513 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 62\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EAADC1A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8834 - loss: 2.3210 - val_accuracy: 0.9088 - val_loss: 0.2302\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8997 - loss: 0.2807 - val_accuracy: 0.9056 - val_loss: 0.2811\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9013 - loss: 0.2471 - val_accuracy: 0.9157 - val_loss: 0.2043\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9099 - loss: 0.2207 - val_accuracy: 0.9056 - val_loss: 0.4715\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.8962 - loss: 0.4113 - val_accuracy: 0.9056 - val_loss: 0.3187\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8967 - loss: 0.3349 - val_accuracy: 0.9056 - val_loss: 0.3135\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.8952 - loss: 0.3358 - val_accuracy: 0.9056 - val_loss: 0.3129\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8942 - loss: 0.3396 - val_accuracy: 0.9056 - val_loss: 0.3071\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9010 - loss: 0.2726 - val_accuracy: 0.9584 - val_loss: 0.1171\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9414 - loss: 0.1473 - val_accuracy: 0.9697 - val_loss: 0.0860\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000062.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9696611505122144 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 63\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E745CAD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8868 - loss: 3.4591 - val_accuracy: 0.9011 - val_loss: 0.2235\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9014 - loss: 0.2404 - val_accuracy: 0.9009 - val_loss: 0.3193\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9016 - loss: 0.3193 - val_accuracy: 0.9009 - val_loss: 0.3135\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8993 - loss: 0.2915 - val_accuracy: 0.9340 - val_loss: 0.1347\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9308 - loss: 0.1499 - val_accuracy: 0.9643 - val_loss: 0.0984\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9442 - loss: 0.1243 - val_accuracy: 0.9547 - val_loss: 0.1062\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9535 - loss: 0.1026 - val_accuracy: 0.9714 - val_loss: 0.0750\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9597 - loss: 0.0896 - val_accuracy: 0.9777 - val_loss: 0.0650\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9673 - loss: 0.0848 - val_accuracy: 0.9738 - val_loss: 0.0694\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9677 - loss: 0.0836 - val_accuracy: 0.9740 - val_loss: 0.0706\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000063.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9739952718676123 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 64\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2FBB0D520>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8855 - loss: 5.6231 - val_accuracy: 0.9054 - val_loss: 0.1775\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9263 - loss: 0.1697 - val_accuracy: 0.9519 - val_loss: 0.1181\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9561 - loss: 0.1075 - val_accuracy: 0.9663 - val_loss: 0.0895\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9625 - loss: 0.0961 - val_accuracy: 0.9827 - val_loss: 0.0537\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9718 - loss: 0.0778 - val_accuracy: 0.9825 - val_loss: 0.0535\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9699 - loss: 0.0771 - val_accuracy: 0.9858 - val_loss: 0.0490\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9727 - loss: 0.0744 - val_accuracy: 0.9884 - val_loss: 0.0395\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9831 - loss: 0.0500 - val_accuracy: 0.9874 - val_loss: 0.0395\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9829 - loss: 0.0496 - val_accuracy: 0.9898 - val_loss: 0.0276\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9709 - loss: 0.0829 - val_accuracy: 0.9882 - val_loss: 0.0333\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000064.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9881796690307328 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 65\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2FBB0D520>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8865 - loss: 3.4755 - val_accuracy: 0.9171 - val_loss: 0.1874\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9264 - loss: 0.1912 - val_accuracy: 0.9310 - val_loss: 0.1733\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9411 - loss: 0.1606 - val_accuracy: 0.9192 - val_loss: 0.2098\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9397 - loss: 0.1591 - val_accuracy: 0.9569 - val_loss: 0.1119\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9631 - loss: 0.1077 - val_accuracy: 0.9539 - val_loss: 0.1218\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9716 - loss: 0.0850 - val_accuracy: 0.9783 - val_loss: 0.0605\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9781 - loss: 0.0632 - val_accuracy: 0.9756 - val_loss: 0.0682\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9840 - loss: 0.0457 - val_accuracy: 0.9813 - val_loss: 0.0490\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9865 - loss: 0.0398 - val_accuracy: 0.9706 - val_loss: 0.0826\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9887 - loss: 0.0348 - val_accuracy: 0.9701 - val_loss: 0.0834\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000065.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9700551615445232 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 66\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EDE1ED20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8925 - loss: 5.9371 - val_accuracy: 0.9058 - val_loss: 0.2270\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9123 - loss: 0.2125 - val_accuracy: 0.9117 - val_loss: 0.2403\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9265 - loss: 0.1795 - val_accuracy: 0.9287 - val_loss: 0.1626\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9340 - loss: 0.1628 - val_accuracy: 0.9728 - val_loss: 0.0799\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9588 - loss: 0.1044 - val_accuracy: 0.9748 - val_loss: 0.0659\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9707 - loss: 0.0765 - val_accuracy: 0.9850 - val_loss: 0.0459\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9755 - loss: 0.0685 - val_accuracy: 0.9768 - val_loss: 0.0637\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9808 - loss: 0.0520 - val_accuracy: 0.9917 - val_loss: 0.0341\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9803 - loss: 0.0592 - val_accuracy: 0.9913 - val_loss: 0.0287\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9857 - loss: 0.0380 - val_accuracy: 0.9888 - val_loss: 0.0381\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000066.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9887706855791962 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 67\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EDF3EC60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8888 - loss: 4.4634 - val_accuracy: 0.9240 - val_loss: 0.1926\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9216 - loss: 0.2169 - val_accuracy: 0.9494 - val_loss: 0.1330\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9473 - loss: 0.1336 - val_accuracy: 0.9539 - val_loss: 0.1128\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9506 - loss: 0.1344 - val_accuracy: 0.9598 - val_loss: 0.1094\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9559 - loss: 0.1140 - val_accuracy: 0.9681 - val_loss: 0.0843\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9686 - loss: 0.0844 - val_accuracy: 0.9775 - val_loss: 0.0670\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9760 - loss: 0.0647 - val_accuracy: 0.9750 - val_loss: 0.0761\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9690 - loss: 0.0889 - val_accuracy: 0.9750 - val_loss: 0.0729\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 64ms/step - accuracy: 0.9865 - loss: 0.0423 - val_accuracy: 0.9775 - val_loss: 0.0833\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9847 - loss: 0.0458 - val_accuracy: 0.9825 - val_loss: 0.0526\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000067.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9824665090622537 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 68\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B319624980>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8910 - loss: 1.5670 - val_accuracy: 0.9086 - val_loss: 0.2014\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9176 - loss: 0.2091 - val_accuracy: 0.9135 - val_loss: 0.1931\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9286 - loss: 0.1710 - val_accuracy: 0.9417 - val_loss: 0.1326\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9461 - loss: 0.1325 - val_accuracy: 0.9486 - val_loss: 0.1249\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9626 - loss: 0.1014 - val_accuracy: 0.9679 - val_loss: 0.0826\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9693 - loss: 0.0807 - val_accuracy: 0.9773 - val_loss: 0.0605\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9789 - loss: 0.0585 - val_accuracy: 0.9801 - val_loss: 0.0539\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9756 - loss: 0.0678 - val_accuracy: 0.9817 - val_loss: 0.0479\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9886 - loss: 0.0329 - val_accuracy: 0.9862 - val_loss: 0.0403\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9903 - loss: 0.0283 - val_accuracy: 0.9842 - val_loss: 0.0439\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000068.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9842395587076438 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 69\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2FBFCEDB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8862 - loss: 2.7648 - val_accuracy: 0.9456 - val_loss: 0.1571\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9277 - loss: 0.1712 - val_accuracy: 0.9669 - val_loss: 0.1005\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9567 - loss: 0.1172 - val_accuracy: 0.9582 - val_loss: 0.1123\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9548 - loss: 0.1195 - val_accuracy: 0.9681 - val_loss: 0.0837\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9691 - loss: 0.0809 - val_accuracy: 0.9815 - val_loss: 0.0504\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9798 - loss: 0.0564 - val_accuracy: 0.9866 - val_loss: 0.0370\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9887 - loss: 0.0350 - val_accuracy: 0.9768 - val_loss: 0.0736\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9829 - loss: 0.0525 - val_accuracy: 0.9923 - val_loss: 0.0242\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9903 - loss: 0.0276 - val_accuracy: 0.9931 - val_loss: 0.0233\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9907 - loss: 0.0271 - val_accuracy: 0.9941 - val_loss: 0.0193\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000069.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9940898345153665 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 70\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31CBD6240>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8905 - loss: 2.6049 - val_accuracy: 0.9121 - val_loss: 0.2067\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9032 - loss: 0.2348 - val_accuracy: 0.9001 - val_loss: 0.4122\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.8983 - loss: 0.3179 - val_accuracy: 0.9074 - val_loss: 0.2333\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9161 - loss: 0.2103 - val_accuracy: 0.9415 - val_loss: 0.1629\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9322 - loss: 0.1644 - val_accuracy: 0.9429 - val_loss: 0.1595\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9554 - loss: 0.1172 - val_accuracy: 0.9594 - val_loss: 0.1092\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9676 - loss: 0.0849 - val_accuracy: 0.9768 - val_loss: 0.0733\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9753 - loss: 0.0684 - val_accuracy: 0.9781 - val_loss: 0.0726\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9796 - loss: 0.0569 - val_accuracy: 0.9766 - val_loss: 0.0762\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9833 - loss: 0.0457 - val_accuracy: 0.9764 - val_loss: 0.0768\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000070.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9763593380614657 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 71\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31BAFE720>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.8929 - loss: 3.1328 - val_accuracy: 0.8846 - val_loss: 0.3914\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9134 - loss: 0.2293 - val_accuracy: 0.9115 - val_loss: 0.1863\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9454 - loss: 0.1363 - val_accuracy: 0.9687 - val_loss: 0.0929\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9728 - loss: 0.0776 - val_accuracy: 0.9602 - val_loss: 0.1251\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9787 - loss: 0.0627 - val_accuracy: 0.9801 - val_loss: 0.0615\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9863 - loss: 0.0409 - val_accuracy: 0.9789 - val_loss: 0.0605\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9883 - loss: 0.0337 - val_accuracy: 0.9888 - val_loss: 0.0405\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9885 - loss: 0.0319 - val_accuracy: 0.9907 - val_loss: 0.0351\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9896 - loss: 0.0330 - val_accuracy: 0.9844 - val_loss: 0.0511\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9913 - loss: 0.0233 - val_accuracy: 0.9894 - val_loss: 0.0341\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000071.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9893617021276596 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 72\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3174F4530>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8819 - loss: 4.6447 - val_accuracy: 0.8997 - val_loss: 0.2413\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9045 - loss: 0.2585 - val_accuracy: 0.8964 - val_loss: 0.2540\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9066 - loss: 0.2432 - val_accuracy: 0.9137 - val_loss: 0.2112\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9268 - loss: 0.1883 - val_accuracy: 0.9444 - val_loss: 0.1504\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9463 - loss: 0.1408 - val_accuracy: 0.9559 - val_loss: 0.1202\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9592 - loss: 0.1063 - val_accuracy: 0.9673 - val_loss: 0.0926\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9699 - loss: 0.0888 - val_accuracy: 0.9728 - val_loss: 0.0746\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9753 - loss: 0.0689 - val_accuracy: 0.9768 - val_loss: 0.0664\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9815 - loss: 0.0528 - val_accuracy: 0.9681 - val_loss: 0.0865\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9857 - loss: 0.0475 - val_accuracy: 0.9750 - val_loss: 0.0665\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000072.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9749802994483846 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 73\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31D1826F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8890 - loss: 4.1184 - val_accuracy: 0.9058 - val_loss: 0.2182\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9103 - loss: 0.2214 - val_accuracy: 0.8926 - val_loss: 0.3369\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9010 - loss: 0.3176 - val_accuracy: 0.8926 - val_loss: 0.3333\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9000 - loss: 0.2997 - val_accuracy: 0.8926 - val_loss: 0.3331\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.8970 - loss: 0.3061 - val_accuracy: 0.9023 - val_loss: 0.2465\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9067 - loss: 0.2363 - val_accuracy: 0.9295 - val_loss: 0.1683\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9322 - loss: 0.1696 - val_accuracy: 0.9588 - val_loss: 0.1153\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9565 - loss: 0.1197 - val_accuracy: 0.9701 - val_loss: 0.0840\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9677 - loss: 0.0882 - val_accuracy: 0.9639 - val_loss: 0.0980\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9768 - loss: 0.0706 - val_accuracy: 0.9809 - val_loss: 0.0565\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000073.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9808904649330181 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 74\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31A8849B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 67ms/step - accuracy: 0.8890 - loss: 3.1322 - val_accuracy: 0.9350 - val_loss: 0.1735\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9200 - loss: 0.2107 - val_accuracy: 0.9553 - val_loss: 0.1232\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9559 - loss: 0.1148 - val_accuracy: 0.9750 - val_loss: 0.0666\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9718 - loss: 0.0802 - val_accuracy: 0.9606 - val_loss: 0.1112\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9747 - loss: 0.0674 - val_accuracy: 0.9854 - val_loss: 0.0445\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9846 - loss: 0.0445 - val_accuracy: 0.9852 - val_loss: 0.0373\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9840 - loss: 0.0448 - val_accuracy: 0.9842 - val_loss: 0.0463\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9880 - loss: 0.0337 - val_accuracy: 0.9882 - val_loss: 0.0356\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9900 - loss: 0.0303 - val_accuracy: 0.9838 - val_loss: 0.0470\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9929 - loss: 0.0211 - val_accuracy: 0.9911 - val_loss: 0.0297\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000074.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9911347517730497 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 75\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31D234A10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8859 - loss: 4.2606 - val_accuracy: 0.9102 - val_loss: 0.2223\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9083 - loss: 0.2214 - val_accuracy: 0.9177 - val_loss: 0.1965\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9205 - loss: 0.1889 - val_accuracy: 0.9242 - val_loss: 0.1853\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9311 - loss: 0.1707 - val_accuracy: 0.9407 - val_loss: 0.1511\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9410 - loss: 0.1345 - val_accuracy: 0.9488 - val_loss: 0.1242\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9527 - loss: 0.1098 - val_accuracy: 0.9496 - val_loss: 0.1267\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9594 - loss: 0.0958 - val_accuracy: 0.9647 - val_loss: 0.1048\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9724 - loss: 0.0709 - val_accuracy: 0.9714 - val_loss: 0.0909\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9733 - loss: 0.0678 - val_accuracy: 0.9742 - val_loss: 0.0807\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9788 - loss: 0.0611 - val_accuracy: 0.9764 - val_loss: 0.0744\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000075.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9763593380614657 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 76\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E12AAA50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8911 - loss: 2.6778 - val_accuracy: 0.9283 - val_loss: 0.1838\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9229 - loss: 0.1936 - val_accuracy: 0.9456 - val_loss: 0.1423\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9522 - loss: 0.1331 - val_accuracy: 0.9507 - val_loss: 0.1337\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9597 - loss: 0.1037 - val_accuracy: 0.9665 - val_loss: 0.0870\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9715 - loss: 0.0774 - val_accuracy: 0.9726 - val_loss: 0.0777\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9789 - loss: 0.0610 - val_accuracy: 0.9671 - val_loss: 0.0899\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9811 - loss: 0.0525 - val_accuracy: 0.9868 - val_loss: 0.0436\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9848 - loss: 0.0432 - val_accuracy: 0.9819 - val_loss: 0.0551\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9874 - loss: 0.0363 - val_accuracy: 0.9770 - val_loss: 0.0674\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9691 - loss: 0.0837 - val_accuracy: 0.9829 - val_loss: 0.0511\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000076.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9828605200945626 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 77\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2FBE580E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8882 - loss: 2.9727 - val_accuracy: 0.9060 - val_loss: 0.2297\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9026 - loss: 0.2280 - val_accuracy: 0.9180 - val_loss: 0.1979\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9108 - loss: 0.2013 - val_accuracy: 0.9309 - val_loss: 0.1730\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9260 - loss: 0.1827 - val_accuracy: 0.9346 - val_loss: 0.1598\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9339 - loss: 0.1535 - val_accuracy: 0.9340 - val_loss: 0.1650\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9481 - loss: 0.1317 - val_accuracy: 0.9663 - val_loss: 0.1019\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9624 - loss: 0.0983 - val_accuracy: 0.9762 - val_loss: 0.0769\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9720 - loss: 0.0780 - val_accuracy: 0.9833 - val_loss: 0.0640\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9802 - loss: 0.0513 - val_accuracy: 0.9866 - val_loss: 0.0497\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9828 - loss: 0.0458 - val_accuracy: 0.9872 - val_loss: 0.0462\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000077.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9871946414499606 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 78\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EDBD1880>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 79ms/step - accuracy: 0.8876 - loss: 2.9352 - val_accuracy: 0.9027 - val_loss: 0.2799\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9125 - loss: 0.2173 - val_accuracy: 0.9210 - val_loss: 0.1965\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9239 - loss: 0.1886 - val_accuracy: 0.9328 - val_loss: 0.1806\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9314 - loss: 0.1731 - val_accuracy: 0.9387 - val_loss: 0.1616\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9411 - loss: 0.1457 - val_accuracy: 0.9496 - val_loss: 0.1325\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9539 - loss: 0.1202 - val_accuracy: 0.9703 - val_loss: 0.0883\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9663 - loss: 0.0895 - val_accuracy: 0.9775 - val_loss: 0.0759\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9745 - loss: 0.0704 - val_accuracy: 0.9720 - val_loss: 0.0893\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9743 - loss: 0.0688 - val_accuracy: 0.9771 - val_loss: 0.0742\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9778 - loss: 0.0588 - val_accuracy: 0.9874 - val_loss: 0.0426\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000078.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.987391646966115 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 79\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3197C4380>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.8857 - loss: 3.0491 - val_accuracy: 0.9446 - val_loss: 0.1574\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9433 - loss: 0.1415 - val_accuracy: 0.9758 - val_loss: 0.0729\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9651 - loss: 0.0938 - val_accuracy: 0.9815 - val_loss: 0.0636\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9742 - loss: 0.0722 - val_accuracy: 0.9825 - val_loss: 0.0559\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9789 - loss: 0.0589 - val_accuracy: 0.9829 - val_loss: 0.0492\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9819 - loss: 0.0509 - val_accuracy: 0.9884 - val_loss: 0.0384\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9837 - loss: 0.0476 - val_accuracy: 0.9905 - val_loss: 0.0369\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9888 - loss: 0.0352 - val_accuracy: 0.9844 - val_loss: 0.0492\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9848 - loss: 0.0433 - val_accuracy: 0.9886 - val_loss: 0.0388\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9878 - loss: 0.0338 - val_accuracy: 0.9911 - val_loss: 0.0299\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000079.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9911347517730497 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 80\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E0CEDE80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8839 - loss: 4.3042 - val_accuracy: 0.9033 - val_loss: 0.2237\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9075 - loss: 0.1976 - val_accuracy: 0.9236 - val_loss: 0.1524\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9491 - loss: 0.1299 - val_accuracy: 0.9734 - val_loss: 0.0883\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9643 - loss: 0.0968 - val_accuracy: 0.9704 - val_loss: 0.0844\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9731 - loss: 0.0779 - val_accuracy: 0.9842 - val_loss: 0.0569\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9724 - loss: 0.0745 - val_accuracy: 0.9874 - val_loss: 0.0435\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9834 - loss: 0.0519 - val_accuracy: 0.9835 - val_loss: 0.0630\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9807 - loss: 0.0548 - val_accuracy: 0.9900 - val_loss: 0.0427\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9814 - loss: 0.0519 - val_accuracy: 0.9836 - val_loss: 0.0613\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9840 - loss: 0.0467 - val_accuracy: 0.9905 - val_loss: 0.0374\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000080.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 81\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B32E218AD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8851 - loss: 4.2426 - val_accuracy: 0.9062 - val_loss: 0.2435\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9053 - loss: 0.2223 - val_accuracy: 0.9312 - val_loss: 0.1583\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9215 - loss: 0.1663 - val_accuracy: 0.9496 - val_loss: 0.1230\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9448 - loss: 0.1253 - val_accuracy: 0.9649 - val_loss: 0.0979\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9602 - loss: 0.0996 - val_accuracy: 0.9728 - val_loss: 0.0855\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9703 - loss: 0.0787 - val_accuracy: 0.9748 - val_loss: 0.0974\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9760 - loss: 0.0663 - val_accuracy: 0.9754 - val_loss: 0.0745\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9766 - loss: 0.0636 - val_accuracy: 0.9803 - val_loss: 0.0689\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9812 - loss: 0.0512 - val_accuracy: 0.9748 - val_loss: 0.0793\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9843 - loss: 0.0466 - val_accuracy: 0.9836 - val_loss: 0.0547\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000081.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9836485421591804 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 82\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B32E350DA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8921 - loss: 1.2741 - val_accuracy: 0.9210 - val_loss: 0.2076\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9279 - loss: 0.1863 - val_accuracy: 0.9442 - val_loss: 0.1354\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9565 - loss: 0.1175 - val_accuracy: 0.9655 - val_loss: 0.0891\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9689 - loss: 0.0861 - val_accuracy: 0.9724 - val_loss: 0.0764\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9824 - loss: 0.0521 - val_accuracy: 0.9789 - val_loss: 0.0563\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9834 - loss: 0.0485 - val_accuracy: 0.9441 - val_loss: 0.1245\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9818 - loss: 0.0514 - val_accuracy: 0.9750 - val_loss: 0.0622\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9905 - loss: 0.0291 - val_accuracy: 0.9870 - val_loss: 0.0409\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9907 - loss: 0.0261 - val_accuracy: 0.9809 - val_loss: 0.0656\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9924 - loss: 0.0210 - val_accuracy: 0.9854 - val_loss: 0.0486\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000082.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9854215918045706 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 83\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B32E2C6150>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8886 - loss: 5.0882 - val_accuracy: 0.9529 - val_loss: 0.1142\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9547 - loss: 0.1129 - val_accuracy: 0.9771 - val_loss: 0.0669\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9753 - loss: 0.0747 - val_accuracy: 0.9770 - val_loss: 0.0671\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9760 - loss: 0.0724 - val_accuracy: 0.9738 - val_loss: 0.0766\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9843 - loss: 0.0446 - val_accuracy: 0.9894 - val_loss: 0.0357\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9864 - loss: 0.0364 - val_accuracy: 0.9884 - val_loss: 0.0379\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9908 - loss: 0.0275 - val_accuracy: 0.9590 - val_loss: 0.1202\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9851 - loss: 0.0431 - val_accuracy: 0.9888 - val_loss: 0.0378\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9899 - loss: 0.0285 - val_accuracy: 0.9898 - val_loss: 0.0401\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9932 - loss: 0.0196 - val_accuracy: 0.9884 - val_loss: 0.0464\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000083.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9883766745468873 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 84\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B32E350CE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8870 - loss: 5.1647 - val_accuracy: 0.9106 - val_loss: 0.1960\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9502 - loss: 0.1261 - val_accuracy: 0.9663 - val_loss: 0.0868\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9769 - loss: 0.0627 - val_accuracy: 0.9874 - val_loss: 0.0453\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9829 - loss: 0.0526 - val_accuracy: 0.9890 - val_loss: 0.0330\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9879 - loss: 0.0399 - val_accuracy: 0.9890 - val_loss: 0.0384\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9867 - loss: 0.0426 - val_accuracy: 0.9890 - val_loss: 0.0299\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9889 - loss: 0.0313 - val_accuracy: 0.9911 - val_loss: 0.0271\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9814 - loss: 0.0539 - val_accuracy: 0.9880 - val_loss: 0.0352\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9854 - loss: 0.0444 - val_accuracy: 0.9929 - val_loss: 0.0278\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9916 - loss: 0.0262 - val_accuracy: 0.9919 - val_loss: 0.0258\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000084.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9919227738376675 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 85\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B32CBBF530>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8780 - loss: 5.9989 - val_accuracy: 0.9334 - val_loss: 0.1588\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9437 - loss: 0.1391 - val_accuracy: 0.9829 - val_loss: 0.0583\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9760 - loss: 0.0674 - val_accuracy: 0.9831 - val_loss: 0.0521\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9834 - loss: 0.0434 - val_accuracy: 0.9791 - val_loss: 0.0594\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9800 - loss: 0.0589 - val_accuracy: 0.9795 - val_loss: 0.0518\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9844 - loss: 0.0440 - val_accuracy: 0.9639 - val_loss: 0.0985\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9833 - loss: 0.0457 - val_accuracy: 0.9898 - val_loss: 0.0369\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9864 - loss: 0.0384 - val_accuracy: 0.9907 - val_loss: 0.0303\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9865 - loss: 0.0381 - val_accuracy: 0.9925 - val_loss: 0.0262\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9923 - loss: 0.0266 - val_accuracy: 0.9933 - val_loss: 0.0263\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000085.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9933018124507487 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 86\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B336ACE600>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8844 - loss: 3.0036 - val_accuracy: 0.8976 - val_loss: 0.1963\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9070 - loss: 0.2033 - val_accuracy: 0.9817 - val_loss: 0.0717\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9511 - loss: 0.1244 - val_accuracy: 0.9852 - val_loss: 0.0540\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9722 - loss: 0.0815 - val_accuracy: 0.9882 - val_loss: 0.0379\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9810 - loss: 0.0566 - val_accuracy: 0.9894 - val_loss: 0.0327\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9795 - loss: 0.0616 - val_accuracy: 0.9862 - val_loss: 0.0335\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9820 - loss: 0.0541 - val_accuracy: 0.9785 - val_loss: 0.0602\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9808 - loss: 0.0502 - val_accuracy: 0.9937 - val_loss: 0.0181\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9874 - loss: 0.0366 - val_accuracy: 0.9921 - val_loss: 0.0287\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9898 - loss: 0.0291 - val_accuracy: 0.9649 - val_loss: 0.0878\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000086.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9649330181245075 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 87\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3417E2E70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8908 - loss: 2.7260 - val_accuracy: 0.9153 - val_loss: 0.2054\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9172 - loss: 0.2057 - val_accuracy: 0.9464 - val_loss: 0.1378\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9481 - loss: 0.1368 - val_accuracy: 0.9610 - val_loss: 0.1094\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9613 - loss: 0.1032 - val_accuracy: 0.9728 - val_loss: 0.0788\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9703 - loss: 0.0771 - val_accuracy: 0.9655 - val_loss: 0.0991\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9755 - loss: 0.0671 - val_accuracy: 0.9734 - val_loss: 0.0859\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9775 - loss: 0.0630 - val_accuracy: 0.9754 - val_loss: 0.0726\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9805 - loss: 0.0582 - val_accuracy: 0.9732 - val_loss: 0.0930\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9836 - loss: 0.0476 - val_accuracy: 0.9789 - val_loss: 0.0820\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9855 - loss: 0.0461 - val_accuracy: 0.9846 - val_loss: 0.0573\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000087.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9846335697399528 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 88\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B33B0F5700>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8913 - loss: 1.3141 - val_accuracy: 0.9074 - val_loss: 0.2142\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9130 - loss: 0.2155 - val_accuracy: 0.9192 - val_loss: 0.1920\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9271 - loss: 0.1814 - val_accuracy: 0.9234 - val_loss: 0.1787\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9352 - loss: 0.1566 - val_accuracy: 0.9454 - val_loss: 0.1425\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9430 - loss: 0.1497 - val_accuracy: 0.9506 - val_loss: 0.1222\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9565 - loss: 0.1076 - val_accuracy: 0.9559 - val_loss: 0.1144\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9660 - loss: 0.0879 - val_accuracy: 0.9624 - val_loss: 0.0995\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9706 - loss: 0.0781 - val_accuracy: 0.9699 - val_loss: 0.0834\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9756 - loss: 0.0671 - val_accuracy: 0.9718 - val_loss: 0.0797\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9804 - loss: 0.0527 - val_accuracy: 0.9614 - val_loss: 0.1090\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000088.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9613869188337274 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 89\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B343C88380>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.8779 - loss: 3.3564 - val_accuracy: 0.9245 - val_loss: 0.1864\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9135 - loss: 0.2037 - val_accuracy: 0.9555 - val_loss: 0.1190\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9431 - loss: 0.1388 - val_accuracy: 0.9754 - val_loss: 0.0671\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9584 - loss: 0.1020 - val_accuracy: 0.9720 - val_loss: 0.0702\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9714 - loss: 0.0743 - val_accuracy: 0.9870 - val_loss: 0.0424\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9823 - loss: 0.0488 - val_accuracy: 0.9917 - val_loss: 0.0324\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9828 - loss: 0.0526 - val_accuracy: 0.9905 - val_loss: 0.0312\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9889 - loss: 0.0349 - val_accuracy: 0.9880 - val_loss: 0.0379\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9894 - loss: 0.0307 - val_accuracy: 0.9931 - val_loss: 0.0240\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9937 - loss: 0.0208 - val_accuracy: 0.9921 - val_loss: 0.0229\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000089.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9921197793538219 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 90\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E0992BA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8797 - loss: 5.3509 - val_accuracy: 0.9108 - val_loss: 0.2550\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9108 - loss: 0.2219 - val_accuracy: 0.9167 - val_loss: 0.2161\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9166 - loss: 0.1987 - val_accuracy: 0.9368 - val_loss: 0.1659\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9404 - loss: 0.1457 - val_accuracy: 0.9480 - val_loss: 0.1785\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9553 - loss: 0.1161 - val_accuracy: 0.9547 - val_loss: 0.1229\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9622 - loss: 0.0970 - val_accuracy: 0.9649 - val_loss: 0.1081\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9671 - loss: 0.0869 - val_accuracy: 0.9663 - val_loss: 0.1088\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9758 - loss: 0.0698 - val_accuracy: 0.9701 - val_loss: 0.1053\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9751 - loss: 0.0675 - val_accuracy: 0.9730 - val_loss: 0.1206\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9800 - loss: 0.0546 - val_accuracy: 0.9760 - val_loss: 0.0827\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000090.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9759653270291568 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 91\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3475AEAB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8893 - loss: 1.8258 - val_accuracy: 0.9281 - val_loss: 0.1666\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9301 - loss: 0.1620 - val_accuracy: 0.9614 - val_loss: 0.1005\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9344 - loss: 0.1868 - val_accuracy: 0.9494 - val_loss: 0.1253\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9603 - loss: 0.1004 - val_accuracy: 0.9777 - val_loss: 0.0606\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9707 - loss: 0.0798 - val_accuracy: 0.9720 - val_loss: 0.0675\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9736 - loss: 0.0686 - val_accuracy: 0.9848 - val_loss: 0.0425\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9842 - loss: 0.0474 - val_accuracy: 0.9783 - val_loss: 0.0647\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9827 - loss: 0.0474 - val_accuracy: 0.9764 - val_loss: 0.0674\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9823 - loss: 0.0508 - val_accuracy: 0.9878 - val_loss: 0.0337\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9900 - loss: 0.0301 - val_accuracy: 0.9913 - val_loss: 0.0334\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000091.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9913317572892041 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 92\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3799B1220>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8805 - loss: 4.8787 - val_accuracy: 0.9206 - val_loss: 0.1727\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9127 - loss: 0.2174 - val_accuracy: 0.9062 - val_loss: 0.2933\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9207 - loss: 0.1956 - val_accuracy: 0.9521 - val_loss: 0.1053\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9478 - loss: 0.1228 - val_accuracy: 0.9596 - val_loss: 0.0903\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9579 - loss: 0.1020 - val_accuracy: 0.9703 - val_loss: 0.0724\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9594 - loss: 0.1035 - val_accuracy: 0.9663 - val_loss: 0.0775\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9690 - loss: 0.0743 - val_accuracy: 0.9572 - val_loss: 0.1022\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9691 - loss: 0.0761 - val_accuracy: 0.9813 - val_loss: 0.0532\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9734 - loss: 0.0731 - val_accuracy: 0.9829 - val_loss: 0.0429\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9825 - loss: 0.0495 - val_accuracy: 0.9854 - val_loss: 0.0450\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000092.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9854215918045706 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 93\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B343AF2270>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8821 - loss: 2.1297 - val_accuracy: 0.9232 - val_loss: 0.1970\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9148 - loss: 0.2194 - val_accuracy: 0.9356 - val_loss: 0.1767\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9351 - loss: 0.1637 - val_accuracy: 0.9466 - val_loss: 0.1424\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9468 - loss: 0.1285 - val_accuracy: 0.9606 - val_loss: 0.1142\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9678 - loss: 0.0878 - val_accuracy: 0.9624 - val_loss: 0.1008\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9726 - loss: 0.0667 - val_accuracy: 0.9819 - val_loss: 0.0599\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9848 - loss: 0.0408 - val_accuracy: 0.9846 - val_loss: 0.0565\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9878 - loss: 0.0379 - val_accuracy: 0.9831 - val_loss: 0.0575\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9866 - loss: 0.0403 - val_accuracy: 0.9880 - val_loss: 0.0458\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9893 - loss: 0.0311 - val_accuracy: 0.9797 - val_loss: 0.0692\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000093.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9797084318360915 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 94\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2E100F3B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8888 - loss: 1.6898 - val_accuracy: 0.8983 - val_loss: 0.2837\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9062 - loss: 0.2368 - val_accuracy: 0.9113 - val_loss: 0.2233\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9141 - loss: 0.1965 - val_accuracy: 0.9375 - val_loss: 0.1502\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9379 - loss: 0.1407 - val_accuracy: 0.9462 - val_loss: 0.1286\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9456 - loss: 0.1285 - val_accuracy: 0.9592 - val_loss: 0.1110\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9580 - loss: 0.1015 - val_accuracy: 0.9726 - val_loss: 0.0856\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9665 - loss: 0.0830 - val_accuracy: 0.9785 - val_loss: 0.0652\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9746 - loss: 0.0665 - val_accuracy: 0.9827 - val_loss: 0.0564\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9771 - loss: 0.0587 - val_accuracy: 0.9773 - val_loss: 0.0648\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9825 - loss: 0.0493 - val_accuracy: 0.9842 - val_loss: 0.0514\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000094.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9842395587076438 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 95\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B336ACE060>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.8840 - loss: 5.4887 - val_accuracy: 0.9011 - val_loss: 0.2579\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9050 - loss: 0.2339 - val_accuracy: 0.9090 - val_loss: 0.2119\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9236 - loss: 0.1775 - val_accuracy: 0.9433 - val_loss: 0.1360\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9422 - loss: 0.1288 - val_accuracy: 0.9638 - val_loss: 0.1061\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9490 - loss: 0.1328 - val_accuracy: 0.9681 - val_loss: 0.0924\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9640 - loss: 0.0917 - val_accuracy: 0.9730 - val_loss: 0.0895\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9723 - loss: 0.0688 - val_accuracy: 0.9795 - val_loss: 0.0730\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9744 - loss: 0.0676 - val_accuracy: 0.9809 - val_loss: 0.0773\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9796 - loss: 0.0539 - val_accuracy: 0.9779 - val_loss: 0.0764\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9787 - loss: 0.0551 - val_accuracy: 0.9835 - val_loss: 0.0575\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000095.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.983451536643026 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 96\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2EDF3DA90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8794 - loss: 4.6679 - val_accuracy: 0.9098 - val_loss: 0.2120\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9102 - loss: 0.2101 - val_accuracy: 0.9171 - val_loss: 0.1942\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9309 - loss: 0.1692 - val_accuracy: 0.9604 - val_loss: 0.1080\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9647 - loss: 0.0967 - val_accuracy: 0.9718 - val_loss: 0.0903\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9735 - loss: 0.0715 - val_accuracy: 0.9771 - val_loss: 0.0611\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9800 - loss: 0.0566 - val_accuracy: 0.9722 - val_loss: 0.0859\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9764 - loss: 0.0700 - val_accuracy: 0.9823 - val_loss: 0.0505\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9859 - loss: 0.0430 - val_accuracy: 0.9856 - val_loss: 0.0457\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9890 - loss: 0.0340 - val_accuracy: 0.9872 - val_loss: 0.0495\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9884 - loss: 0.0343 - val_accuracy: 0.9856 - val_loss: 0.0455\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000096.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.985618597320725 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 97\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3778D8D40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8875 - loss: 3.8149 - val_accuracy: 0.8918 - val_loss: 0.2425\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9029 - loss: 0.2318 - val_accuracy: 0.9019 - val_loss: 0.2292\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9093 - loss: 0.2165 - val_accuracy: 0.9082 - val_loss: 0.2125\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9096 - loss: 0.2055 - val_accuracy: 0.8918 - val_loss: 0.3668\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8997 - loss: 0.3276 - val_accuracy: 0.8918 - val_loss: 0.3352\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.8953 - loss: 0.3334 - val_accuracy: 0.8918 - val_loss: 0.3349\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8995 - loss: 0.3225 - val_accuracy: 0.8918 - val_loss: 0.3357\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8998 - loss: 0.3208 - val_accuracy: 0.8918 - val_loss: 0.3370\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9028 - loss: 0.3142 - val_accuracy: 0.8918 - val_loss: 0.3350\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.8995 - loss: 0.3216 - val_accuracy: 0.8918 - val_loss: 0.3350\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000097.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8918439716312057 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 98\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B345EFC050>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8837 - loss: 4.4816 - val_accuracy: 0.9247 - val_loss: 0.1784\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9319 - loss: 0.1662 - val_accuracy: 0.9472 - val_loss: 0.1421\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9514 - loss: 0.1269 - val_accuracy: 0.9616 - val_loss: 0.1065\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9540 - loss: 0.1190 - val_accuracy: 0.9718 - val_loss: 0.0893\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9696 - loss: 0.0838 - val_accuracy: 0.9653 - val_loss: 0.0949\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9748 - loss: 0.0676 - val_accuracy: 0.9825 - val_loss: 0.0635\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9809 - loss: 0.0560 - val_accuracy: 0.9836 - val_loss: 0.0529\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9839 - loss: 0.0456 - val_accuracy: 0.9836 - val_loss: 0.0562\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9872 - loss: 0.0370 - val_accuracy: 0.9898 - val_loss: 0.0418\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9873 - loss: 0.0394 - val_accuracy: 0.9864 - val_loss: 0.0493\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000098.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9864066193853428 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 99\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3418B4B30>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8867 - loss: 2.3976 - val_accuracy: 0.8962 - val_loss: 0.2496\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.8987 - loss: 0.2554 - val_accuracy: 0.9121 - val_loss: 0.2176\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9128 - loss: 0.2229 - val_accuracy: 0.9279 - val_loss: 0.1862\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9226 - loss: 0.1915 - val_accuracy: 0.9273 - val_loss: 0.1785\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9292 - loss: 0.1769 - val_accuracy: 0.9506 - val_loss: 0.1364\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9474 - loss: 0.1313 - val_accuracy: 0.9714 - val_loss: 0.0941\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9449 - loss: 0.1488 - val_accuracy: 0.9659 - val_loss: 0.0974\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9719 - loss: 0.0815 - val_accuracy: 0.9618 - val_loss: 0.0941\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9764 - loss: 0.0645 - val_accuracy: 0.9701 - val_loss: 0.0926\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9835 - loss: 0.0472 - val_accuracy: 0.9866 - val_loss: 0.0456\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000099.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9866036249014972 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 100\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B336B37C50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.8909 - loss: 3.9374 - val_accuracy: 0.9253 - val_loss: 0.1986\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8991 - loss: 0.3447 - val_accuracy: 0.8972 - val_loss: 0.3247\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.8953 - loss: 0.3322 - val_accuracy: 0.8972 - val_loss: 0.3244\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.8979 - loss: 0.3249 - val_accuracy: 0.8972 - val_loss: 0.3239\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.8972 - loss: 0.3265 - val_accuracy: 0.8972 - val_loss: 0.3244\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9010 - loss: 0.3187 - val_accuracy: 0.8972 - val_loss: 0.3239\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.8982 - loss: 0.3257 - val_accuracy: 0.8972 - val_loss: 0.3238\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.8979 - loss: 0.3268 - val_accuracy: 0.8972 - val_loss: 0.3238\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9005 - loss: 0.3195 - val_accuracy: 0.8972 - val_loss: 0.3238\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.8953 - loss: 0.3313 - val_accuracy: 0.8972 - val_loss: 0.3240\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000100.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8971631205673759 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 101\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3196EB8C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 72ms/step - accuracy: 0.8822 - loss: 6.2369 - val_accuracy: 0.9525 - val_loss: 0.1164\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9348 - loss: 0.1618 - val_accuracy: 0.9586 - val_loss: 0.0934\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9612 - loss: 0.1005 - val_accuracy: 0.9766 - val_loss: 0.0748\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9676 - loss: 0.0862 - val_accuracy: 0.9584 - val_loss: 0.1129\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9746 - loss: 0.0708 - val_accuracy: 0.9870 - val_loss: 0.0469\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9760 - loss: 0.0689 - val_accuracy: 0.9850 - val_loss: 0.0487\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 70ms/step - accuracy: 0.9774 - loss: 0.0629 - val_accuracy: 0.9823 - val_loss: 0.0538\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9830 - loss: 0.0500 - val_accuracy: 0.9827 - val_loss: 0.0498\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9827 - loss: 0.0482 - val_accuracy: 0.9901 - val_loss: 0.0371\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9859 - loss: 0.0420 - val_accuracy: 0.9894 - val_loss: 0.0355\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000101.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9893617021276596 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 102\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3DFD111F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8872 - loss: 1.7237 - val_accuracy: 0.9287 - val_loss: 0.1948\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9164 - loss: 0.2081 - val_accuracy: 0.9389 - val_loss: 0.1672\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9453 - loss: 0.1416 - val_accuracy: 0.9643 - val_loss: 0.1073\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9625 - loss: 0.0982 - val_accuracy: 0.9661 - val_loss: 0.1054\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9744 - loss: 0.0698 - val_accuracy: 0.9685 - val_loss: 0.1094\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9807 - loss: 0.0496 - val_accuracy: 0.9799 - val_loss: 0.0747\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9836 - loss: 0.0475 - val_accuracy: 0.9724 - val_loss: 0.0897\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9858 - loss: 0.0414 - val_accuracy: 0.9799 - val_loss: 0.0756\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9881 - loss: 0.0356 - val_accuracy: 0.9848 - val_loss: 0.0755\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9899 - loss: 0.0271 - val_accuracy: 0.9799 - val_loss: 0.0824\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000102.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9799054373522459 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 103\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3E86F7890>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8914 - loss: 6.3237 - val_accuracy: 0.9041 - val_loss: 0.2590\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9022 - loss: 0.2402 - val_accuracy: 0.9037 - val_loss: 0.3292\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.8967 - loss: 0.3249 - val_accuracy: 0.9037 - val_loss: 0.3156\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.8947 - loss: 0.3325 - val_accuracy: 0.9037 - val_loss: 0.3134\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8966 - loss: 0.3288 - val_accuracy: 0.9037 - val_loss: 0.3109\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.8973 - loss: 0.3276 - val_accuracy: 0.9037 - val_loss: 0.3124\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8976 - loss: 0.3260 - val_accuracy: 0.9037 - val_loss: 0.3104\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.8980 - loss: 0.3052 - val_accuracy: 0.9037 - val_loss: 0.3098\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8945 - loss: 0.3328 - val_accuracy: 0.9037 - val_loss: 0.3095\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.8984 - loss: 0.3236 - val_accuracy: 0.9037 - val_loss: 0.3096\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000103.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9036643026004728 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 104\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3E1E62630>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 84ms/step - accuracy: 0.8841 - loss: 6.5781 - val_accuracy: 0.8983 - val_loss: 0.2085\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.9083 - loss: 0.2296 - val_accuracy: 0.9439 - val_loss: 0.1698\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9404 - loss: 0.1484 - val_accuracy: 0.9610 - val_loss: 0.1185\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9645 - loss: 0.0930 - val_accuracy: 0.9781 - val_loss: 0.0671\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 78ms/step - accuracy: 0.9793 - loss: 0.0591 - val_accuracy: 0.9620 - val_loss: 0.0962\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9647 - loss: 0.0956 - val_accuracy: 0.9781 - val_loss: 0.0614\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9788 - loss: 0.0630 - val_accuracy: 0.9748 - val_loss: 0.0734\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9818 - loss: 0.0542 - val_accuracy: 0.9773 - val_loss: 0.0659\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9805 - loss: 0.0544 - val_accuracy: 0.9787 - val_loss: 0.0797\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9890 - loss: 0.0312 - val_accuracy: 0.9876 - val_loss: 0.0381\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000104.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9875886524822695 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 105\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3E8775280>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8753 - loss: 11.7157 - val_accuracy: 0.8995 - val_loss: 0.1839\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9128 - loss: 0.1844 - val_accuracy: 0.9474 - val_loss: 0.1284\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9413 - loss: 0.1407 - val_accuracy: 0.9693 - val_loss: 0.0859\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9636 - loss: 0.0961 - val_accuracy: 0.9714 - val_loss: 0.0868\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9643 - loss: 0.0983 - val_accuracy: 0.9872 - val_loss: 0.0406\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9847 - loss: 0.0487 - val_accuracy: 0.9704 - val_loss: 0.0874\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9870 - loss: 0.0421 - val_accuracy: 0.9860 - val_loss: 0.0488\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9879 - loss: 0.0322 - val_accuracy: 0.9876 - val_loss: 0.0399\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9872 - loss: 0.0337 - val_accuracy: 0.9886 - val_loss: 0.0427\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9908 - loss: 0.0262 - val_accuracy: 0.9939 - val_loss: 0.0262\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000105.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9938928289992119 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 106\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3EBCA9F10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8864 - loss: 3.1925 - val_accuracy: 0.9115 - val_loss: 0.1977\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9103 - loss: 0.2176 - val_accuracy: 0.9310 - val_loss: 0.1562\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9339 - loss: 0.1623 - val_accuracy: 0.9381 - val_loss: 0.1473\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9568 - loss: 0.1116 - val_accuracy: 0.9728 - val_loss: 0.0791\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9582 - loss: 0.1098 - val_accuracy: 0.9685 - val_loss: 0.0939\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9684 - loss: 0.0817 - val_accuracy: 0.9775 - val_loss: 0.0640\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9803 - loss: 0.0561 - val_accuracy: 0.9805 - val_loss: 0.0627\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9822 - loss: 0.0471 - val_accuracy: 0.9791 - val_loss: 0.0597\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9862 - loss: 0.0393 - val_accuracy: 0.9823 - val_loss: 0.0577\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9870 - loss: 0.0390 - val_accuracy: 0.9643 - val_loss: 0.1007\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000106.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9643420015760441 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 107\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3EAAAB500>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 78ms/step - accuracy: 0.8929 - loss: 1.9507 - val_accuracy: 0.9301 - val_loss: 0.1776\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9303 - loss: 0.1734 - val_accuracy: 0.9691 - val_loss: 0.0825\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9598 - loss: 0.1052 - val_accuracy: 0.9760 - val_loss: 0.0594\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9790 - loss: 0.0609 - val_accuracy: 0.9882 - val_loss: 0.0369\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9849 - loss: 0.0445 - val_accuracy: 0.9884 - val_loss: 0.0411\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9906 - loss: 0.0295 - val_accuracy: 0.9726 - val_loss: 0.0777\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.9895 - loss: 0.0367 - val_accuracy: 0.9907 - val_loss: 0.0309\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9890 - loss: 0.0314 - val_accuracy: 0.9927 - val_loss: 0.0297\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9900 - loss: 0.0345 - val_accuracy: 0.9919 - val_loss: 0.0286\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9921 - loss: 0.0247 - val_accuracy: 0.9862 - val_loss: 0.0502\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000107.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9862096138691884 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 108\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F0267FB0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 74ms/step - accuracy: 0.8891 - loss: 2.4339 - val_accuracy: 0.8981 - val_loss: 0.2355\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8974 - loss: 0.2334 - val_accuracy: 0.9198 - val_loss: 0.1802\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9179 - loss: 0.1949 - val_accuracy: 0.9712 - val_loss: 0.0764\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9539 - loss: 0.0936 - val_accuracy: 0.9821 - val_loss: 0.0526\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9751 - loss: 0.0712 - val_accuracy: 0.9852 - val_loss: 0.0438\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9783 - loss: 0.0633 - val_accuracy: 0.9846 - val_loss: 0.0420\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9835 - loss: 0.0485 - val_accuracy: 0.9903 - val_loss: 0.0342\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9858 - loss: 0.0484 - val_accuracy: 0.9921 - val_loss: 0.0255\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9849 - loss: 0.0475 - val_accuracy: 0.9866 - val_loss: 0.0366\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 81ms/step - accuracy: 0.9875 - loss: 0.0412 - val_accuracy: 0.9856 - val_loss: 0.0419\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000108.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.985618597320725 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 109\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F255C800>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 74ms/step - accuracy: 0.8820 - loss: 4.7735 - val_accuracy: 0.9372 - val_loss: 0.1566\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9235 - loss: 0.1857 - val_accuracy: 0.9535 - val_loss: 0.1143\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9511 - loss: 0.1155 - val_accuracy: 0.9632 - val_loss: 0.0947\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9669 - loss: 0.0885 - val_accuracy: 0.9777 - val_loss: 0.0722\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9745 - loss: 0.0676 - val_accuracy: 0.9740 - val_loss: 0.0796\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9771 - loss: 0.0574 - val_accuracy: 0.9744 - val_loss: 0.0750\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9783 - loss: 0.0590 - val_accuracy: 0.9647 - val_loss: 0.0961\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9783 - loss: 0.0563 - val_accuracy: 0.9831 - val_loss: 0.0627\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9831 - loss: 0.0449 - val_accuracy: 0.9884 - val_loss: 0.0451\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9829 - loss: 0.0475 - val_accuracy: 0.9764 - val_loss: 0.0812\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000109.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9763593380614657 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 110\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F485EF00>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8802 - loss: 5.5517 - val_accuracy: 0.9021 - val_loss: 0.2322\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9020 - loss: 0.2415 - val_accuracy: 0.9080 - val_loss: 0.2193\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9174 - loss: 0.2031 - val_accuracy: 0.9391 - val_loss: 0.1606\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9387 - loss: 0.1497 - val_accuracy: 0.9484 - val_loss: 0.1402\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9522 - loss: 0.1235 - val_accuracy: 0.9502 - val_loss: 0.1350\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9592 - loss: 0.1044 - val_accuracy: 0.9628 - val_loss: 0.1061\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9661 - loss: 0.0882 - val_accuracy: 0.9716 - val_loss: 0.0856\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9750 - loss: 0.0729 - val_accuracy: 0.9730 - val_loss: 0.0761\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9795 - loss: 0.0521 - val_accuracy: 0.9740 - val_loss: 0.0803\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9813 - loss: 0.0532 - val_accuracy: 0.9775 - val_loss: 0.0748\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000110.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9775413711583925 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 111\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F4837EF0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8856 - loss: 6.1729 - val_accuracy: 0.9080 - val_loss: 0.2173\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9230 - loss: 0.1869 - val_accuracy: 0.9466 - val_loss: 0.1355\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9476 - loss: 0.1286 - val_accuracy: 0.9667 - val_loss: 0.0816\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9723 - loss: 0.0747 - val_accuracy: 0.9785 - val_loss: 0.0578\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9814 - loss: 0.0497 - val_accuracy: 0.9732 - val_loss: 0.0719\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9848 - loss: 0.0419 - val_accuracy: 0.9783 - val_loss: 0.0495\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9876 - loss: 0.0346 - val_accuracy: 0.9850 - val_loss: 0.0418\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9877 - loss: 0.0329 - val_accuracy: 0.9868 - val_loss: 0.0352\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9897 - loss: 0.0301 - val_accuracy: 0.9898 - val_loss: 0.0281\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9914 - loss: 0.0244 - val_accuracy: 0.9726 - val_loss: 0.1526\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000111.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9726162332545312 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 112\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F7A81730>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 71ms/step - accuracy: 0.8872 - loss: 2.9729 - val_accuracy: 0.8952 - val_loss: 0.2635\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8987 - loss: 0.2462 - val_accuracy: 0.9190 - val_loss: 0.1919\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9271 - loss: 0.1856 - val_accuracy: 0.9517 - val_loss: 0.1237\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9558 - loss: 0.1198 - val_accuracy: 0.9653 - val_loss: 0.0922\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9626 - loss: 0.1086 - val_accuracy: 0.9708 - val_loss: 0.0831\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9750 - loss: 0.0682 - val_accuracy: 0.9616 - val_loss: 0.0965\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9800 - loss: 0.0586 - val_accuracy: 0.9766 - val_loss: 0.0714\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9863 - loss: 0.0403 - val_accuracy: 0.9783 - val_loss: 0.0640\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9886 - loss: 0.0351 - val_accuracy: 0.9860 - val_loss: 0.0545\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9867 - loss: 0.0401 - val_accuracy: 0.9876 - val_loss: 0.0492\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000112.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9875886524822695 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 113\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F7B50AA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8810 - loss: 4.8474 - val_accuracy: 0.9003 - val_loss: 0.2626\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.8957 - loss: 0.2630 - val_accuracy: 0.9003 - val_loss: 0.1920\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9001 - loss: 0.1716 - val_accuracy: 0.9450 - val_loss: 0.1240\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9415 - loss: 0.1403 - val_accuracy: 0.9710 - val_loss: 0.0931\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9598 - loss: 0.1130 - val_accuracy: 0.9803 - val_loss: 0.0696\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9673 - loss: 0.0887 - val_accuracy: 0.9797 - val_loss: 0.0634\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9710 - loss: 0.0859 - val_accuracy: 0.9797 - val_loss: 0.0664\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9786 - loss: 0.0631 - val_accuracy: 0.9874 - val_loss: 0.0438\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9832 - loss: 0.0469 - val_accuracy: 0.9708 - val_loss: 0.0758\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9761 - loss: 0.0647 - val_accuracy: 0.9898 - val_loss: 0.0439\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000113.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9897557131599685 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 114\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3FA56DBE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 67ms/step - accuracy: 0.8857 - loss: 6.7865 - val_accuracy: 0.8960 - val_loss: 0.3416\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9038 - loss: 0.2809 - val_accuracy: 0.9035 - val_loss: 0.2460\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9052 - loss: 0.2422 - val_accuracy: 0.9062 - val_loss: 0.2122\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9158 - loss: 0.2052 - val_accuracy: 0.9184 - val_loss: 0.1870\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 66ms/step - accuracy: 0.9297 - loss: 0.1754 - val_accuracy: 0.8960 - val_loss: 0.3333\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9247 - loss: 0.2039 - val_accuracy: 0.9484 - val_loss: 0.1339\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9466 - loss: 0.1378 - val_accuracy: 0.9590 - val_loss: 0.1139\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9480 - loss: 0.1533 - val_accuracy: 0.9555 - val_loss: 0.1147\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9599 - loss: 0.1033 - val_accuracy: 0.9655 - val_loss: 0.1009\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9676 - loss: 0.0892 - val_accuracy: 0.9563 - val_loss: 0.1109\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000114.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9562647754137116 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 115\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F01C8F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8890 - loss: 3.3265 - val_accuracy: 0.8952 - val_loss: 0.2304\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9061 - loss: 0.2207 - val_accuracy: 0.9504 - val_loss: 0.1280\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9368 - loss: 0.1479 - val_accuracy: 0.9618 - val_loss: 0.1111\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9579 - loss: 0.1002 - val_accuracy: 0.9827 - val_loss: 0.0621\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9691 - loss: 0.0861 - val_accuracy: 0.9825 - val_loss: 0.0514\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9745 - loss: 0.0763 - val_accuracy: 0.9762 - val_loss: 0.0695\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 83ms/step - accuracy: 0.9826 - loss: 0.0554 - val_accuracy: 0.9754 - val_loss: 0.0707\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9812 - loss: 0.0580 - val_accuracy: 0.9872 - val_loss: 0.0386\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9820 - loss: 0.0533 - val_accuracy: 0.9844 - val_loss: 0.0470\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 69ms/step - accuracy: 0.9800 - loss: 0.0564 - val_accuracy: 0.9915 - val_loss: 0.0340\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000115.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9915287628053585 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 116\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B2F8700D10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 71ms/step - accuracy: 0.8819 - loss: 4.8467 - val_accuracy: 0.9084 - val_loss: 0.2152\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9022 - loss: 0.2542 - val_accuracy: 0.9060 - val_loss: 0.3068\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.8982 - loss: 0.3255 - val_accuracy: 0.9060 - val_loss: 0.3063\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.8965 - loss: 0.3301 - val_accuracy: 0.9060 - val_loss: 0.3076\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.8984 - loss: 0.3329 - val_accuracy: 0.9060 - val_loss: 0.3063\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.8935 - loss: 0.3341 - val_accuracy: 0.9060 - val_loss: 0.3052\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.8972 - loss: 0.3265 - val_accuracy: 0.9060 - val_loss: 0.3078\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8972 - loss: 0.3275 - val_accuracy: 0.9060 - val_loss: 0.3058\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.8940 - loss: 0.3324 - val_accuracy: 0.9060 - val_loss: 0.3063\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8969 - loss: 0.3268 - val_accuracy: 0.9060 - val_loss: 0.3061\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000116.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9060283687943262 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 117\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3EE0754C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8865 - loss: 4.5173 - val_accuracy: 0.9137 - val_loss: 0.2138\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9163 - loss: 0.2069 - val_accuracy: 0.9567 - val_loss: 0.1217\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9173 - loss: 0.2511 - val_accuracy: 0.9344 - val_loss: 0.1742\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9457 - loss: 0.1428 - val_accuracy: 0.9622 - val_loss: 0.0952\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9566 - loss: 0.1218 - val_accuracy: 0.9791 - val_loss: 0.0665\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9719 - loss: 0.0780 - val_accuracy: 0.9783 - val_loss: 0.0632\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9768 - loss: 0.0613 - val_accuracy: 0.9846 - val_loss: 0.0457\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9752 - loss: 0.0656 - val_accuracy: 0.9846 - val_loss: 0.0486\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9795 - loss: 0.0523 - val_accuracy: 0.9870 - val_loss: 0.0425\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9814 - loss: 0.0512 - val_accuracy: 0.9888 - val_loss: 0.0399\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000117.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9887706855791962 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 118\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B379DBB500>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8899 - loss: 2.9823 - val_accuracy: 0.9370 - val_loss: 0.1678\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9354 - loss: 0.1673 - val_accuracy: 0.9588 - val_loss: 0.1156\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9627 - loss: 0.1014 - val_accuracy: 0.9622 - val_loss: 0.1051\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9698 - loss: 0.0828 - val_accuracy: 0.9604 - val_loss: 0.1068\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9757 - loss: 0.0693 - val_accuracy: 0.9641 - val_loss: 0.1230\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9869 - loss: 0.0389 - val_accuracy: 0.9777 - val_loss: 0.0635\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9893 - loss: 0.0321 - val_accuracy: 0.9842 - val_loss: 0.0512\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9855 - loss: 0.0432 - val_accuracy: 0.9789 - val_loss: 0.0586\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9408 - loss: 0.1830 - val_accuracy: 0.9764 - val_loss: 0.0672\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9879 - loss: 0.0326 - val_accuracy: 0.9858 - val_loss: 0.0410\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000118.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9858156028368794 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 119\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B33F615880>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.8815 - loss: 2.6606 - val_accuracy: 0.9572 - val_loss: 0.1135\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9417 - loss: 0.1504 - val_accuracy: 0.9659 - val_loss: 0.0919\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9653 - loss: 0.0911 - val_accuracy: 0.9665 - val_loss: 0.0934\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9734 - loss: 0.0731 - val_accuracy: 0.9864 - val_loss: 0.0489\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9827 - loss: 0.0481 - val_accuracy: 0.9833 - val_loss: 0.0568\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9853 - loss: 0.0452 - val_accuracy: 0.9911 - val_loss: 0.0409\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9769 - loss: 0.0644 - val_accuracy: 0.9870 - val_loss: 0.0502\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9882 - loss: 0.0334 - val_accuracy: 0.9909 - val_loss: 0.0456\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9889 - loss: 0.0307 - val_accuracy: 0.9917 - val_loss: 0.0424\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9924 - loss: 0.0206 - val_accuracy: 0.9915 - val_loss: 0.0396\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000119.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9915287628053585 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 120\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F25750D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8872 - loss: 2.8891 - val_accuracy: 0.8989 - val_loss: 0.2203\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8956 - loss: 0.2307 - val_accuracy: 0.9163 - val_loss: 0.2109\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9230 - loss: 0.1902 - val_accuracy: 0.9425 - val_loss: 0.1740\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9482 - loss: 0.1301 - val_accuracy: 0.9506 - val_loss: 0.1409\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9555 - loss: 0.1130 - val_accuracy: 0.9697 - val_loss: 0.0957\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9719 - loss: 0.0788 - val_accuracy: 0.9712 - val_loss: 0.0841\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9681 - loss: 0.0906 - val_accuracy: 0.9708 - val_loss: 0.0923\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9761 - loss: 0.0683 - val_accuracy: 0.9762 - val_loss: 0.0773\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9840 - loss: 0.0463 - val_accuracy: 0.9783 - val_loss: 0.0789\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9869 - loss: 0.0408 - val_accuracy: 0.9801 - val_loss: 0.0780\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000120.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9801024428684003 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 121\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3F485E720>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8801 - loss: 4.0496 - val_accuracy: 0.8997 - val_loss: 0.2221\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9040 - loss: 0.2284 - val_accuracy: 0.9135 - val_loss: 0.1984\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9290 - loss: 0.1650 - val_accuracy: 0.9699 - val_loss: 0.0792\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9634 - loss: 0.0978 - val_accuracy: 0.9673 - val_loss: 0.0788\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9734 - loss: 0.0712 - val_accuracy: 0.9807 - val_loss: 0.0540\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9831 - loss: 0.0510 - val_accuracy: 0.9876 - val_loss: 0.0424\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9838 - loss: 0.0460 - val_accuracy: 0.9868 - val_loss: 0.0396\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9854 - loss: 0.0419 - val_accuracy: 0.9896 - val_loss: 0.0484\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9885 - loss: 0.0364 - val_accuracy: 0.9894 - val_loss: 0.0359\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9852 - loss: 0.0398 - val_accuracy: 0.9915 - val_loss: 0.0293\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000121.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9915287628053585 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 122\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3E8821250>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8940 - loss: 3.5633 - val_accuracy: 0.9431 - val_loss: 0.1599\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9230 - loss: 0.1941 - val_accuracy: 0.8964 - val_loss: 0.2681\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9217 - loss: 0.2104 - val_accuracy: 0.9529 - val_loss: 0.1202\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9635 - loss: 0.1011 - val_accuracy: 0.9638 - val_loss: 0.0983\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9711 - loss: 0.0767 - val_accuracy: 0.9781 - val_loss: 0.0633\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9809 - loss: 0.0519 - val_accuracy: 0.9697 - val_loss: 0.0749\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9847 - loss: 0.0413 - val_accuracy: 0.9787 - val_loss: 0.0602\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9864 - loss: 0.0377 - val_accuracy: 0.9819 - val_loss: 0.0791\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9882 - loss: 0.0384 - val_accuracy: 0.9492 - val_loss: 0.1292\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9847 - loss: 0.0386 - val_accuracy: 0.9880 - val_loss: 0.0427\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000122.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9879826635145784 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 123\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B41A1E91C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8916 - loss: 2.9914 - val_accuracy: 0.9078 - val_loss: 0.1958\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9216 - loss: 0.1890 - val_accuracy: 0.9553 - val_loss: 0.1110\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9532 - loss: 0.1230 - val_accuracy: 0.9689 - val_loss: 0.0881\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9606 - loss: 0.1116 - val_accuracy: 0.9557 - val_loss: 0.1105\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9718 - loss: 0.0756 - val_accuracy: 0.9809 - val_loss: 0.0589\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9799 - loss: 0.0535 - val_accuracy: 0.9831 - val_loss: 0.0503\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9815 - loss: 0.0504 - val_accuracy: 0.9811 - val_loss: 0.0567\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9853 - loss: 0.0456 - val_accuracy: 0.9856 - val_loss: 0.0522\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9873 - loss: 0.0364 - val_accuracy: 0.9848 - val_loss: 0.0481\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9906 - loss: 0.0278 - val_accuracy: 0.9886 - val_loss: 0.0403\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000123.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9885736800630418 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 124\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B41A25E3C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 68ms/step - accuracy: 0.8837 - loss: 3.1400 - val_accuracy: 0.9257 - val_loss: 0.2006\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9311 - loss: 0.1711 - val_accuracy: 0.9535 - val_loss: 0.1232\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9528 - loss: 0.1214 - val_accuracy: 0.9620 - val_loss: 0.1092\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9702 - loss: 0.0792 - val_accuracy: 0.9519 - val_loss: 0.1250\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9791 - loss: 0.0631 - val_accuracy: 0.9766 - val_loss: 0.0767\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9799 - loss: 0.0547 - val_accuracy: 0.9750 - val_loss: 0.0783\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9889 - loss: 0.0342 - val_accuracy: 0.9762 - val_loss: 0.0681\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9901 - loss: 0.0288 - val_accuracy: 0.9809 - val_loss: 0.0627\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9897 - loss: 0.0329 - val_accuracy: 0.9890 - val_loss: 0.0471\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9918 - loss: 0.0207 - val_accuracy: 0.9848 - val_loss: 0.0561\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000124.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9848305752561072 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 125\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B46B27D550>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 64ms/step - accuracy: 0.8836 - loss: 3.6686 - val_accuracy: 0.9131 - val_loss: 0.1857\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9168 - loss: 0.1970 - val_accuracy: 0.9602 - val_loss: 0.1018\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9555 - loss: 0.1151 - val_accuracy: 0.9708 - val_loss: 0.0782\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9683 - loss: 0.0804 - val_accuracy: 0.9789 - val_loss: 0.0572\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9755 - loss: 0.0627 - val_accuracy: 0.9888 - val_loss: 0.0436\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9824 - loss: 0.0504 - val_accuracy: 0.9892 - val_loss: 0.0389\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9831 - loss: 0.0477 - val_accuracy: 0.9872 - val_loss: 0.0432\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9818 - loss: 0.0481 - val_accuracy: 0.9669 - val_loss: 0.0902\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9813 - loss: 0.0532 - val_accuracy: 0.9842 - val_loss: 0.0540\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9891 - loss: 0.0305 - val_accuracy: 0.9927 - val_loss: 0.0317\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000125.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9927107959022853 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 126\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B41A25E0C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8840 - loss: 2.4084 - val_accuracy: 0.9003 - val_loss: 0.2447\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9089 - loss: 0.2307 - val_accuracy: 0.9157 - val_loss: 0.2212\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9214 - loss: 0.1986 - val_accuracy: 0.9232 - val_loss: 0.1813\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9338 - loss: 0.1616 - val_accuracy: 0.9356 - val_loss: 0.1769\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9418 - loss: 0.1366 - val_accuracy: 0.9468 - val_loss: 0.1429\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9533 - loss: 0.1206 - val_accuracy: 0.9529 - val_loss: 0.1334\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9604 - loss: 0.1031 - val_accuracy: 0.9500 - val_loss: 0.1384\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9595 - loss: 0.0966 - val_accuracy: 0.9598 - val_loss: 0.1210\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9650 - loss: 0.0868 - val_accuracy: 0.9600 - val_loss: 0.1241\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9684 - loss: 0.0832 - val_accuracy: 0.9588 - val_loss: 0.1126\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000126.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9588258471237194 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 127\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B31753EB10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8868 - loss: 1.2585 - val_accuracy: 0.9194 - val_loss: 0.1973\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9312 - loss: 0.1814 - val_accuracy: 0.9517 - val_loss: 0.1305\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9526 - loss: 0.1182 - val_accuracy: 0.9659 - val_loss: 0.0971\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9684 - loss: 0.0835 - val_accuracy: 0.9722 - val_loss: 0.0780\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9760 - loss: 0.0683 - val_accuracy: 0.9722 - val_loss: 0.0788\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9814 - loss: 0.0528 - val_accuracy: 0.9819 - val_loss: 0.0610\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9860 - loss: 0.0403 - val_accuracy: 0.9862 - val_loss: 0.0460\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9883 - loss: 0.0312 - val_accuracy: 0.9870 - val_loss: 0.0441\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9893 - loss: 0.0312 - val_accuracy: 0.9797 - val_loss: 0.0594\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9882 - loss: 0.0365 - val_accuracy: 0.9821 - val_loss: 0.0547\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000127.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9820724980299448 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 128\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B460D3F740>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8836 - loss: 3.4772 - val_accuracy: 0.9019 - val_loss: 0.2521\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.8992 - loss: 0.2320 - val_accuracy: 0.9214 - val_loss: 0.1847\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9246 - loss: 0.1895 - val_accuracy: 0.9275 - val_loss: 0.1701\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9123 - loss: 0.2272 - val_accuracy: 0.9736 - val_loss: 0.0955\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9549 - loss: 0.1091 - val_accuracy: 0.9795 - val_loss: 0.0761\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9692 - loss: 0.0773 - val_accuracy: 0.9864 - val_loss: 0.0492\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9776 - loss: 0.0599 - val_accuracy: 0.9913 - val_loss: 0.0411\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9849 - loss: 0.0406 - val_accuracy: 0.9888 - val_loss: 0.0445\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9831 - loss: 0.0456 - val_accuracy: 0.9917 - val_loss: 0.0417\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9869 - loss: 0.0372 - val_accuracy: 0.9815 - val_loss: 0.0823\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000128.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9814814814814815 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 129\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B4650E0770>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8843 - loss: 1.4932 - val_accuracy: 0.9273 - val_loss: 0.2027\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9259 - loss: 0.1921 - val_accuracy: 0.9444 - val_loss: 0.1477\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9483 - loss: 0.1365 - val_accuracy: 0.9588 - val_loss: 0.1090\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9654 - loss: 0.0924 - val_accuracy: 0.9760 - val_loss: 0.0699\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9662 - loss: 0.1004 - val_accuracy: 0.9746 - val_loss: 0.0723\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9776 - loss: 0.0652 - val_accuracy: 0.9795 - val_loss: 0.0634\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9814 - loss: 0.0494 - val_accuracy: 0.9799 - val_loss: 0.0566\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9866 - loss: 0.0413 - val_accuracy: 0.9718 - val_loss: 0.0916\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9870 - loss: 0.0403 - val_accuracy: 0.9827 - val_loss: 0.0511\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9898 - loss: 0.0267 - val_accuracy: 0.9827 - val_loss: 0.0570\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000129.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9826635145784082 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 130\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B45EA857C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8810 - loss: 2.2546 - val_accuracy: 0.9080 - val_loss: 0.2234\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9082 - loss: 0.2291 - val_accuracy: 0.9121 - val_loss: 0.2150\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9166 - loss: 0.1966 - val_accuracy: 0.9411 - val_loss: 0.1561\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9426 - loss: 0.1375 - val_accuracy: 0.9458 - val_loss: 0.1511\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9525 - loss: 0.1210 - val_accuracy: 0.9598 - val_loss: 0.1139\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9658 - loss: 0.0836 - val_accuracy: 0.9703 - val_loss: 0.0862\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9764 - loss: 0.0613 - val_accuracy: 0.9768 - val_loss: 0.0862\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9797 - loss: 0.0526 - val_accuracy: 0.9795 - val_loss: 0.0822\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9837 - loss: 0.0482 - val_accuracy: 0.9805 - val_loss: 0.0928\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9824 - loss: 0.0479 - val_accuracy: 0.9703 - val_loss: 0.1398\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000130.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9702521670606777 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 131\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B462F6A7B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8838 - loss: 2.6612 - val_accuracy: 0.9161 - val_loss: 0.2040\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 66ms/step - accuracy: 0.9095 - loss: 0.2159 - val_accuracy: 0.9385 - val_loss: 0.1631\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9289 - loss: 0.1686 - val_accuracy: 0.9539 - val_loss: 0.1303\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9428 - loss: 0.1354 - val_accuracy: 0.9663 - val_loss: 0.0970\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9671 - loss: 0.0816 - val_accuracy: 0.9850 - val_loss: 0.0571\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9754 - loss: 0.0687 - val_accuracy: 0.9854 - val_loss: 0.0547\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9825 - loss: 0.0505 - val_accuracy: 0.9866 - val_loss: 0.0523\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9832 - loss: 0.0458 - val_accuracy: 0.9882 - val_loss: 0.0579\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9862 - loss: 0.0383 - val_accuracy: 0.9888 - val_loss: 0.0508\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9875 - loss: 0.0324 - val_accuracy: 0.9935 - val_loss: 0.0474\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000131.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9934988179669031 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 132\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B461D47EC0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8852 - loss: 5.8057 - val_accuracy: 0.9072 - val_loss: 0.2106\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9021 - loss: 0.2275 - val_accuracy: 0.9220 - val_loss: 0.1880\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9167 - loss: 0.2009 - val_accuracy: 0.9307 - val_loss: 0.1699\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9293 - loss: 0.1747 - val_accuracy: 0.9559 - val_loss: 0.1166\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9473 - loss: 0.1311 - val_accuracy: 0.9023 - val_loss: 0.2969\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9033 - loss: 0.2796 - val_accuracy: 0.9578 - val_loss: 0.1029\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9515 - loss: 0.1184 - val_accuracy: 0.9492 - val_loss: 0.1153\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9612 - loss: 0.0965 - val_accuracy: 0.9744 - val_loss: 0.0813\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9422 - loss: 0.1534 - val_accuracy: 0.9791 - val_loss: 0.0594\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9614 - loss: 0.1016 - val_accuracy: 0.9697 - val_loss: 0.0809\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000132.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9696611505122144 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 133\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B46AB5EA50>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8807 - loss: 4.4059 - val_accuracy: 0.9149 - val_loss: 0.1982\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9160 - loss: 0.1992 - val_accuracy: 0.9460 - val_loss: 0.1293\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9370 - loss: 0.1625 - val_accuracy: 0.9647 - val_loss: 0.0808\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9617 - loss: 0.0989 - val_accuracy: 0.9840 - val_loss: 0.0455\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9642 - loss: 0.0944 - val_accuracy: 0.9844 - val_loss: 0.0394\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9800 - loss: 0.0551 - val_accuracy: 0.9900 - val_loss: 0.0284\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9837 - loss: 0.0480 - val_accuracy: 0.9927 - val_loss: 0.0218\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9852 - loss: 0.0457 - val_accuracy: 0.9929 - val_loss: 0.0207\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9834 - loss: 0.0457 - val_accuracy: 0.9917 - val_loss: 0.0224\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9909 - loss: 0.0296 - val_accuracy: 0.9951 - val_loss: 0.0179\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000133.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9950748620961387 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 134\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B46B134140>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8847 - loss: 3.2327 - val_accuracy: 0.9027 - val_loss: 0.1792\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9228 - loss: 0.1640 - val_accuracy: 0.9673 - val_loss: 0.0937\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9456 - loss: 0.1235 - val_accuracy: 0.9608 - val_loss: 0.0965\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9491 - loss: 0.1266 - val_accuracy: 0.9789 - val_loss: 0.0606\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9590 - loss: 0.1061 - val_accuracy: 0.9750 - val_loss: 0.0701\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9671 - loss: 0.0900 - val_accuracy: 0.9703 - val_loss: 0.0810\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9665 - loss: 0.0915 - val_accuracy: 0.9697 - val_loss: 0.0833\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9712 - loss: 0.0867 - val_accuracy: 0.9742 - val_loss: 0.0638\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9751 - loss: 0.0725 - val_accuracy: 0.9864 - val_loss: 0.0465\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9530 - loss: 0.1318 - val_accuracy: 0.9823 - val_loss: 0.0657\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000134.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9822695035460993 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 135\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B46B054F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8923 - loss: 3.0472 - val_accuracy: 0.9247 - val_loss: 0.2036\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9242 - loss: 0.1849 - val_accuracy: 0.9271 - val_loss: 0.1844\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9470 - loss: 0.1344 - val_accuracy: 0.9746 - val_loss: 0.0727\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9722 - loss: 0.0776 - val_accuracy: 0.9770 - val_loss: 0.0584\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9778 - loss: 0.0610 - val_accuracy: 0.9852 - val_loss: 0.0422\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9821 - loss: 0.0524 - val_accuracy: 0.9777 - val_loss: 0.0624\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9841 - loss: 0.0464 - val_accuracy: 0.9876 - val_loss: 0.0396\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9866 - loss: 0.0356 - val_accuracy: 0.9880 - val_loss: 0.0345\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9888 - loss: 0.0299 - val_accuracy: 0.9872 - val_loss: 0.0368\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9886 - loss: 0.0319 - val_accuracy: 0.9923 - val_loss: 0.0324\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000135.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9923167848699763 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 136\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B46ADFDDC0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8823 - loss: 2.9079 - val_accuracy: 0.8981 - val_loss: 0.2315\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.8988 - loss: 0.2319 - val_accuracy: 0.8981 - val_loss: 0.1996\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.8968 - loss: 0.1861 - val_accuracy: 0.9571 - val_loss: 0.1670\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9546 - loss: 0.1399 - val_accuracy: 0.9596 - val_loss: 0.1217\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9611 - loss: 0.1159 - val_accuracy: 0.9840 - val_loss: 0.0771\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9705 - loss: 0.0972 - val_accuracy: 0.9779 - val_loss: 0.0755\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9717 - loss: 0.0879 - val_accuracy: 0.9821 - val_loss: 0.0671\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9740 - loss: 0.0785 - val_accuracy: 0.9870 - val_loss: 0.0551\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9774 - loss: 0.0709 - val_accuracy: 0.9878 - val_loss: 0.0566\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9794 - loss: 0.0643 - val_accuracy: 0.9838 - val_loss: 0.0567\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000136.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9838455476753349 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 137\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B474835BE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8848 - loss: 5.1471 - val_accuracy: 0.8972 - val_loss: 0.3315\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8982 - loss: 0.3337 - val_accuracy: 0.8972 - val_loss: 0.3313\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9023 - loss: 0.3239 - val_accuracy: 0.8972 - val_loss: 0.3314\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.8982 - loss: 0.3305 - val_accuracy: 0.8972 - val_loss: 0.3253\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9021 - loss: 0.3178 - val_accuracy: 0.8972 - val_loss: 0.3251\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.8974 - loss: 0.3261 - val_accuracy: 0.8972 - val_loss: 0.3253\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9019 - loss: 0.3172 - val_accuracy: 0.8972 - val_loss: 0.3271\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9013 - loss: 0.3174 - val_accuracy: 0.8972 - val_loss: 0.3257\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9001 - loss: 0.3187 - val_accuracy: 0.8972 - val_loss: 0.3251\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.8956 - loss: 0.3306 - val_accuracy: 0.8972 - val_loss: 0.3253\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000137.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8971631205673759 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 138\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B478CD3260>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8875 - loss: 1.7311 - val_accuracy: 0.8926 - val_loss: 0.4157\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9111 - loss: 0.2798 - val_accuracy: 0.9155 - val_loss: 0.2138\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9229 - loss: 0.1999 - val_accuracy: 0.9257 - val_loss: 0.1704\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9347 - loss: 0.1612 - val_accuracy: 0.9163 - val_loss: 0.2110\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9379 - loss: 0.1583 - val_accuracy: 0.9580 - val_loss: 0.1117\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9541 - loss: 0.1207 - val_accuracy: 0.9730 - val_loss: 0.0797\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9674 - loss: 0.0923 - val_accuracy: 0.9783 - val_loss: 0.0629\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9705 - loss: 0.0767 - val_accuracy: 0.9722 - val_loss: 0.0765\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 66ms/step - accuracy: 0.9783 - loss: 0.0570 - val_accuracy: 0.9787 - val_loss: 0.0608\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9787 - loss: 0.0604 - val_accuracy: 0.9829 - val_loss: 0.0513\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000138.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9828605200945626 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 139\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B47BF7B680>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8838 - loss: 3.9818 - val_accuracy: 0.9178 - val_loss: 0.1558\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9309 - loss: 0.1474 - val_accuracy: 0.9726 - val_loss: 0.0842\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9563 - loss: 0.1097 - val_accuracy: 0.9706 - val_loss: 0.0771\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9675 - loss: 0.0898 - val_accuracy: 0.9618 - val_loss: 0.0899\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9715 - loss: 0.0804 - val_accuracy: 0.9829 - val_loss: 0.0506\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9724 - loss: 0.0842 - val_accuracy: 0.9825 - val_loss: 0.0507\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9723 - loss: 0.0725 - val_accuracy: 0.9655 - val_loss: 0.0926\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9814 - loss: 0.0561 - val_accuracy: 0.9659 - val_loss: 0.0774\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9773 - loss: 0.0579 - val_accuracy: 0.9872 - val_loss: 0.0410\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9831 - loss: 0.0484 - val_accuracy: 0.9838 - val_loss: 0.0486\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000139.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9838455476753349 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 140\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B45C61FAD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8836 - loss: 3.8898 - val_accuracy: 0.9173 - val_loss: 0.2040\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9056 - loss: 0.2272 - val_accuracy: 0.9478 - val_loss: 0.1290\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9423 - loss: 0.1409 - val_accuracy: 0.9206 - val_loss: 0.1989\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9503 - loss: 0.1129 - val_accuracy: 0.9760 - val_loss: 0.0664\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9676 - loss: 0.0799 - val_accuracy: 0.9531 - val_loss: 0.0996\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9716 - loss: 0.0752 - val_accuracy: 0.9848 - val_loss: 0.0493\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9741 - loss: 0.0704 - val_accuracy: 0.9817 - val_loss: 0.0554\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9801 - loss: 0.0565 - val_accuracy: 0.9836 - val_loss: 0.0463\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9800 - loss: 0.0486 - val_accuracy: 0.9878 - val_loss: 0.0444\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9797 - loss: 0.0630 - val_accuracy: 0.9860 - val_loss: 0.0432\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000140.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9860126083530338 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 141\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B462EA0F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8863 - loss: 4.8029 - val_accuracy: 0.9060 - val_loss: 0.2194\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9092 - loss: 0.2090 - val_accuracy: 0.9273 - val_loss: 0.1620\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9303 - loss: 0.1626 - val_accuracy: 0.9383 - val_loss: 0.1361\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9511 - loss: 0.1185 - val_accuracy: 0.9710 - val_loss: 0.0867\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9592 - loss: 0.1037 - val_accuracy: 0.9708 - val_loss: 0.0781\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9645 - loss: 0.0921 - val_accuracy: 0.9771 - val_loss: 0.0698\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9670 - loss: 0.0830 - val_accuracy: 0.9710 - val_loss: 0.0771\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9729 - loss: 0.0740 - val_accuracy: 0.9695 - val_loss: 0.0848\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9694 - loss: 0.0819 - val_accuracy: 0.9703 - val_loss: 0.0851\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9774 - loss: 0.0607 - val_accuracy: 0.9817 - val_loss: 0.0622\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000141.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9816784869976359 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 142\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3E4117A40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 67ms/step - accuracy: 0.8926 - loss: 2.7425 - val_accuracy: 0.9084 - val_loss: 0.2118\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9093 - loss: 0.2086 - val_accuracy: 0.9106 - val_loss: 0.2083\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9346 - loss: 0.1531 - val_accuracy: 0.9710 - val_loss: 0.0876\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9642 - loss: 0.0922 - val_accuracy: 0.9245 - val_loss: 0.1596\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9555 - loss: 0.1056 - val_accuracy: 0.9866 - val_loss: 0.0477\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9770 - loss: 0.0649 - val_accuracy: 0.9874 - val_loss: 0.0394\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9775 - loss: 0.0698 - val_accuracy: 0.9827 - val_loss: 0.0489\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9829 - loss: 0.0482 - val_accuracy: 0.9880 - val_loss: 0.0418\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9871 - loss: 0.0366 - val_accuracy: 0.9846 - val_loss: 0.0549\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9782 - loss: 0.0575 - val_accuracy: 0.9192 - val_loss: 0.1840\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000142.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9192277383766746 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 143\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3FA620950>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.8787 - loss: 4.6075 - val_accuracy: 0.9058 - val_loss: 0.2412\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9084 - loss: 0.2274 - val_accuracy: 0.9092 - val_loss: 0.2017\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9108 - loss: 0.2192 - val_accuracy: 0.8989 - val_loss: 0.3217\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9094 - loss: 0.2252 - val_accuracy: 0.9228 - val_loss: 0.2060\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.9413 - loss: 0.1514 - val_accuracy: 0.9630 - val_loss: 0.0892\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9602 - loss: 0.1050 - val_accuracy: 0.9779 - val_loss: 0.0614\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9706 - loss: 0.0734 - val_accuracy: 0.9789 - val_loss: 0.0528\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9756 - loss: 0.0690 - val_accuracy: 0.9819 - val_loss: 0.0526\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9828 - loss: 0.0499 - val_accuracy: 0.9836 - val_loss: 0.0475\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9834 - loss: 0.0469 - val_accuracy: 0.9900 - val_loss: 0.0286\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000143.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9899527186761229 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 144\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B46F2CBB00>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8872 - loss: 2.9501 - val_accuracy: 0.9125 - val_loss: 0.2250\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9073 - loss: 0.2334 - val_accuracy: 0.9407 - val_loss: 0.1567\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9306 - loss: 0.1553 - val_accuracy: 0.9458 - val_loss: 0.1247\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9504 - loss: 0.1140 - val_accuracy: 0.9614 - val_loss: 0.0999\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9638 - loss: 0.0926 - val_accuracy: 0.9756 - val_loss: 0.0722\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9680 - loss: 0.0811 - val_accuracy: 0.9701 - val_loss: 0.0783\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9706 - loss: 0.0754 - val_accuracy: 0.9801 - val_loss: 0.0698\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 63ms/step - accuracy: 0.9805 - loss: 0.0536 - val_accuracy: 0.9791 - val_loss: 0.0622\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9829 - loss: 0.0469 - val_accuracy: 0.9846 - val_loss: 0.0597\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9832 - loss: 0.0452 - val_accuracy: 0.9872 - val_loss: 0.0513\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000144.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9871946414499606 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 145\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B45C78CC20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8801 - loss: 7.5510 - val_accuracy: 0.9045 - val_loss: 0.2221\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9058 - loss: 0.2326 - val_accuracy: 0.9015 - val_loss: 0.2417\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9136 - loss: 0.1977 - val_accuracy: 0.9328 - val_loss: 0.1449\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 67ms/step - accuracy: 0.9380 - loss: 0.1388 - val_accuracy: 0.9572 - val_loss: 0.1015\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9472 - loss: 0.1153 - val_accuracy: 0.9618 - val_loss: 0.0905\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9584 - loss: 0.0965 - val_accuracy: 0.9720 - val_loss: 0.0765\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9662 - loss: 0.0892 - val_accuracy: 0.9724 - val_loss: 0.0750\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9720 - loss: 0.0725 - val_accuracy: 0.9807 - val_loss: 0.0668\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9764 - loss: 0.0687 - val_accuracy: 0.9781 - val_loss: 0.0642\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9781 - loss: 0.0586 - val_accuracy: 0.9791 - val_loss: 0.0709\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000145.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9791174152876281 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 146\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3E41C4CE0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8813 - loss: 3.2362 - val_accuracy: 0.9078 - val_loss: 0.2386\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9085 - loss: 0.2265 - val_accuracy: 0.9330 - val_loss: 0.1753\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9281 - loss: 0.1873 - val_accuracy: 0.9442 - val_loss: 0.1571\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9386 - loss: 0.1540 - val_accuracy: 0.9515 - val_loss: 0.1300\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9460 - loss: 0.1515 - val_accuracy: 0.9565 - val_loss: 0.1303\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9578 - loss: 0.1121 - val_accuracy: 0.9628 - val_loss: 0.1104\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9592 - loss: 0.1107 - val_accuracy: 0.9612 - val_loss: 0.1117\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9611 - loss: 0.1085 - val_accuracy: 0.9673 - val_loss: 0.1022\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9702 - loss: 0.0805 - val_accuracy: 0.9689 - val_loss: 0.0942\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9695 - loss: 0.0755 - val_accuracy: 0.9638 - val_loss: 0.1135\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000146.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9637509850275807 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 147\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B460D3EEA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.8978 - loss: 1.8297 - val_accuracy: 0.9078 - val_loss: 0.2074\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9129 - loss: 0.2056 - val_accuracy: 0.9273 - val_loss: 0.2102\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9336 - loss: 0.1620 - val_accuracy: 0.9618 - val_loss: 0.1000\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9575 - loss: 0.1064 - val_accuracy: 0.9868 - val_loss: 0.0497\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9718 - loss: 0.0747 - val_accuracy: 0.9669 - val_loss: 0.0792\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9805 - loss: 0.0545 - val_accuracy: 0.9919 - val_loss: 0.0295\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9868 - loss: 0.0336 - val_accuracy: 0.9884 - val_loss: 0.0347\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9913 - loss: 0.0241 - val_accuracy: 0.9917 - val_loss: 0.0284\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9916 - loss: 0.0255 - val_accuracy: 0.9945 - val_loss: 0.0222\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9931 - loss: 0.0189 - val_accuracy: 0.9943 - val_loss: 0.0229\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000147.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9942868400315209 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 148\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3FA53B3B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.8839 - loss: 6.2011 - val_accuracy: 0.9084 - val_loss: 0.2143\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9074 - loss: 0.2341 - val_accuracy: 0.9328 - val_loss: 0.1721\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9155 - loss: 0.1996 - val_accuracy: 0.9332 - val_loss: 0.1687\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9356 - loss: 0.1540 - val_accuracy: 0.9594 - val_loss: 0.1077\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9514 - loss: 0.1183 - val_accuracy: 0.9665 - val_loss: 0.0858\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9632 - loss: 0.0955 - val_accuracy: 0.9693 - val_loss: 0.0756\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9687 - loss: 0.0793 - val_accuracy: 0.9817 - val_loss: 0.0573\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9784 - loss: 0.0572 - val_accuracy: 0.9835 - val_loss: 0.0480\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9819 - loss: 0.0504 - val_accuracy: 0.9770 - val_loss: 0.0654\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9842 - loss: 0.0424 - val_accuracy: 0.9840 - val_loss: 0.0432\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000148.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9840425531914894 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 149\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B4878C02C0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8879 - loss: 4.6722 - val_accuracy: 0.9149 - val_loss: 0.1761\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9338 - loss: 0.1503 - val_accuracy: 0.9643 - val_loss: 0.0838\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9600 - loss: 0.1004 - val_accuracy: 0.9840 - val_loss: 0.0579\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9720 - loss: 0.0796 - val_accuracy: 0.9791 - val_loss: 0.0558\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9761 - loss: 0.0654 - val_accuracy: 0.9817 - val_loss: 0.0495\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9787 - loss: 0.0629 - val_accuracy: 0.9793 - val_loss: 0.0598\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9837 - loss: 0.0489 - val_accuracy: 0.9901 - val_loss: 0.0327\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9835 - loss: 0.0486 - val_accuracy: 0.9896 - val_loss: 0.0351\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9858 - loss: 0.0402 - val_accuracy: 0.9917 - val_loss: 0.0269\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9872 - loss: 0.0369 - val_accuracy: 0.9874 - val_loss: 0.0412\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000149.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.987391646966115 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 150\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B488B4BEC0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8811 - loss: 3.9236 - val_accuracy: 0.8972 - val_loss: 0.2764\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9141 - loss: 0.2234 - val_accuracy: 0.9096 - val_loss: 0.2348\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9360 - loss: 0.1677 - val_accuracy: 0.9608 - val_loss: 0.1229\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9574 - loss: 0.1106 - val_accuracy: 0.9474 - val_loss: 0.2284\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9709 - loss: 0.0821 - val_accuracy: 0.9858 - val_loss: 0.0456\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9796 - loss: 0.0541 - val_accuracy: 0.9876 - val_loss: 0.0419\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9901 - loss: 0.0351 - val_accuracy: 0.9919 - val_loss: 0.0324\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9886 - loss: 0.0326 - val_accuracy: 0.9903 - val_loss: 0.0408\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9887 - loss: 0.0348 - val_accuracy: 0.9768 - val_loss: 0.0825\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9901 - loss: 0.0365 - val_accuracy: 0.9825 - val_loss: 0.0599\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000150.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9824665090622537 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 151\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B488C36480>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8817 - loss: 4.3716 - val_accuracy: 0.9005 - val_loss: 0.2437\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9002 - loss: 0.2456 - val_accuracy: 0.9035 - val_loss: 0.2144\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9138 - loss: 0.1968 - val_accuracy: 0.9545 - val_loss: 0.1154\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9279 - loss: 0.1334 - val_accuracy: 0.9614 - val_loss: 0.1046\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9630 - loss: 0.1002 - val_accuracy: 0.9771 - val_loss: 0.0659\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9647 - loss: 0.1006 - val_accuracy: 0.9833 - val_loss: 0.0543\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9690 - loss: 0.0952 - val_accuracy: 0.9850 - val_loss: 0.0515\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9740 - loss: 0.0786 - val_accuracy: 0.9842 - val_loss: 0.0623\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9782 - loss: 0.0668 - val_accuracy: 0.9870 - val_loss: 0.0444\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9819 - loss: 0.0598 - val_accuracy: 0.9854 - val_loss: 0.0538\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000151.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9854215918045706 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 152\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B36887BA10>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8901 - loss: 1.7820 - val_accuracy: 0.9117 - val_loss: 0.2089\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9097 - loss: 0.2228 - val_accuracy: 0.9023 - val_loss: 1.8960\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9271 - loss: 0.1906 - val_accuracy: 0.9273 - val_loss: 0.1776\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9413 - loss: 0.1480 - val_accuracy: 0.9567 - val_loss: 0.1467\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9617 - loss: 0.1029 - val_accuracy: 0.9728 - val_loss: 0.0802\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9793 - loss: 0.0577 - val_accuracy: 0.9748 - val_loss: 0.0675\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9836 - loss: 0.0487 - val_accuracy: 0.9833 - val_loss: 0.0522\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9865 - loss: 0.0358 - val_accuracy: 0.9901 - val_loss: 0.0359\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9910 - loss: 0.0249 - val_accuracy: 0.9896 - val_loss: 0.0309\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 67ms/step - accuracy: 0.9915 - loss: 0.0229 - val_accuracy: 0.9882 - val_loss: 0.0387\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000152.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9881796690307328 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 153\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B36AB8D8E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8846 - loss: 5.2132 - val_accuracy: 0.9163 - val_loss: 0.2173\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9059 - loss: 0.2318 - val_accuracy: 0.9019 - val_loss: 0.2695\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9022 - loss: 0.2476 - val_accuracy: 0.9090 - val_loss: 0.2221\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9155 - loss: 0.2133 - val_accuracy: 0.9214 - val_loss: 0.1932\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9248 - loss: 0.1825 - val_accuracy: 0.9271 - val_loss: 0.1821\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9389 - loss: 0.1532 - val_accuracy: 0.9437 - val_loss: 0.1451\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9518 - loss: 0.1304 - val_accuracy: 0.9342 - val_loss: 0.1704\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9462 - loss: 0.1374 - val_accuracy: 0.9588 - val_loss: 0.1245\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9618 - loss: 0.0975 - val_accuracy: 0.9675 - val_loss: 0.0988\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9718 - loss: 0.0683 - val_accuracy: 0.9736 - val_loss: 0.0873\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000153.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9736012608353034 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 154\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B36AC0B7D0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8847 - loss: 4.4891 - val_accuracy: 0.9147 - val_loss: 0.2188\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9080 - loss: 0.2249 - val_accuracy: 0.9104 - val_loss: 0.1996\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9160 - loss: 0.1926 - val_accuracy: 0.9348 - val_loss: 0.1576\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9342 - loss: 0.1500 - val_accuracy: 0.9555 - val_loss: 0.1175\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9518 - loss: 0.1212 - val_accuracy: 0.9604 - val_loss: 0.0999\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9648 - loss: 0.0940 - val_accuracy: 0.9655 - val_loss: 0.0975\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9692 - loss: 0.0804 - val_accuracy: 0.9697 - val_loss: 0.0870\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9738 - loss: 0.0743 - val_accuracy: 0.9732 - val_loss: 0.0741\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9787 - loss: 0.0544 - val_accuracy: 0.9746 - val_loss: 0.0800\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9791 - loss: 0.0590 - val_accuracy: 0.9783 - val_loss: 0.0636\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000154.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9783293932230103 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 155\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B36D5386B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8854 - loss: 5.5743 - val_accuracy: 0.9052 - val_loss: 0.2338\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9003 - loss: 0.2336 - val_accuracy: 0.9208 - val_loss: 0.1706\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9248 - loss: 0.1750 - val_accuracy: 0.9466 - val_loss: 0.1317\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9432 - loss: 0.1366 - val_accuracy: 0.9620 - val_loss: 0.1039\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9494 - loss: 0.1172 - val_accuracy: 0.9551 - val_loss: 0.1081\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9569 - loss: 0.1015 - val_accuracy: 0.9736 - val_loss: 0.0837\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9627 - loss: 0.0956 - val_accuracy: 0.9770 - val_loss: 0.0742\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9680 - loss: 0.0828 - val_accuracy: 0.9813 - val_loss: 0.0659\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9757 - loss: 0.0664 - val_accuracy: 0.9819 - val_loss: 0.0674\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9792 - loss: 0.0575 - val_accuracy: 0.9632 - val_loss: 0.1039\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000155.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9631599684791174 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 156\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B36D684C20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8883 - loss: 2.4531 - val_accuracy: 0.9137 - val_loss: 0.2022\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9094 - loss: 0.2235 - val_accuracy: 0.9429 - val_loss: 0.1565\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9309 - loss: 0.1668 - val_accuracy: 0.9553 - val_loss: 0.1206\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9476 - loss: 0.1318 - val_accuracy: 0.9598 - val_loss: 0.1133\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9655 - loss: 0.0950 - val_accuracy: 0.9665 - val_loss: 0.0890\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9713 - loss: 0.0798 - val_accuracy: 0.9685 - val_loss: 0.0860\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9773 - loss: 0.0617 - val_accuracy: 0.9783 - val_loss: 0.0719\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9831 - loss: 0.0485 - val_accuracy: 0.9504 - val_loss: 0.1396\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9858 - loss: 0.0411 - val_accuracy: 0.9803 - val_loss: 0.0677\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9901 - loss: 0.0327 - val_accuracy: 0.9831 - val_loss: 0.0606\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000156.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9830575256107171 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 157\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B48ED3DB80>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8849 - loss: 4.6987 - val_accuracy: 0.8997 - val_loss: 0.2553\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9045 - loss: 0.2538 - val_accuracy: 0.9082 - val_loss: 0.2073\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9144 - loss: 0.2247 - val_accuracy: 0.9212 - val_loss: 0.1949\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9173 - loss: 0.2189 - val_accuracy: 0.9303 - val_loss: 0.1752\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9322 - loss: 0.1777 - val_accuracy: 0.9401 - val_loss: 0.1559\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9358 - loss: 0.1624 - val_accuracy: 0.9417 - val_loss: 0.1421\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9502 - loss: 0.1282 - val_accuracy: 0.9444 - val_loss: 0.1493\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9576 - loss: 0.1074 - val_accuracy: 0.9578 - val_loss: 0.1101\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9669 - loss: 0.0851 - val_accuracy: 0.9525 - val_loss: 0.1147\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 68ms/step - accuracy: 0.9736 - loss: 0.0746 - val_accuracy: 0.9639 - val_loss: 0.0941\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000157.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9639479905437353 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 158\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B4991F8F20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8977 - loss: 2.4779 - val_accuracy: 0.9167 - val_loss: 0.2176\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9164 - loss: 0.2033 - val_accuracy: 0.9257 - val_loss: 0.1912\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9461 - loss: 0.1474 - val_accuracy: 0.9561 - val_loss: 0.1286\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9599 - loss: 0.1077 - val_accuracy: 0.9437 - val_loss: 0.1514\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9602 - loss: 0.1049 - val_accuracy: 0.9744 - val_loss: 0.0783\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9758 - loss: 0.0678 - val_accuracy: 0.9770 - val_loss: 0.0705\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9788 - loss: 0.0553 - val_accuracy: 0.9801 - val_loss: 0.0610\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9853 - loss: 0.0444 - val_accuracy: 0.9819 - val_loss: 0.0555\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9886 - loss: 0.0332 - val_accuracy: 0.9695 - val_loss: 0.1015\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9826 - loss: 0.0481 - val_accuracy: 0.9846 - val_loss: 0.0586\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000158.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9846335697399528 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 159\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B499283500>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8898 - loss: 3.4360 - val_accuracy: 0.9247 - val_loss: 0.1756\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9166 - loss: 0.2001 - val_accuracy: 0.9291 - val_loss: 0.1539\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9372 - loss: 0.1510 - val_accuracy: 0.9033 - val_loss: 0.2960\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9033 - loss: 0.2660 - val_accuracy: 0.9559 - val_loss: 0.1126\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9489 - loss: 0.1277 - val_accuracy: 0.9744 - val_loss: 0.0748\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9676 - loss: 0.0834 - val_accuracy: 0.9760 - val_loss: 0.0658\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9723 - loss: 0.0716 - val_accuracy: 0.9657 - val_loss: 0.0881\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9772 - loss: 0.0581 - val_accuracy: 0.9811 - val_loss: 0.0551\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9826 - loss: 0.0484 - val_accuracy: 0.9793 - val_loss: 0.0508\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9847 - loss: 0.0425 - val_accuracy: 0.9844 - val_loss: 0.0445\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000159.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9844365642237982 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 160\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B49D8F1250>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.8889 - loss: 3.3614 - val_accuracy: 0.9133 - val_loss: 0.2114\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9129 - loss: 0.2091 - val_accuracy: 0.9590 - val_loss: 0.1099\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9440 - loss: 0.1367 - val_accuracy: 0.9706 - val_loss: 0.0805\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 69ms/step - accuracy: 0.9609 - loss: 0.0963 - val_accuracy: 0.9764 - val_loss: 0.0673\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9758 - loss: 0.0652 - val_accuracy: 0.9882 - val_loss: 0.0461\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9778 - loss: 0.0604 - val_accuracy: 0.9833 - val_loss: 0.0550\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9777 - loss: 0.0579 - val_accuracy: 0.9858 - val_loss: 0.0426\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9814 - loss: 0.0523 - val_accuracy: 0.9850 - val_loss: 0.0462\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 69ms/step - accuracy: 0.9825 - loss: 0.0465 - val_accuracy: 0.9862 - val_loss: 0.0428\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9830 - loss: 0.0470 - val_accuracy: 0.9900 - val_loss: 0.0407\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000160.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9899527186761229 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 161\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B49DAFEFF0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8897 - loss: 4.1054 - val_accuracy: 0.9224 - val_loss: 0.1354\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9555 - loss: 0.1200 - val_accuracy: 0.9803 - val_loss: 0.0766\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9729 - loss: 0.0827 - val_accuracy: 0.9848 - val_loss: 0.0500\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9830 - loss: 0.0602 - val_accuracy: 0.9854 - val_loss: 0.0423\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9831 - loss: 0.0603 - val_accuracy: 0.9868 - val_loss: 0.0426\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9845 - loss: 0.0542 - val_accuracy: 0.9854 - val_loss: 0.0346\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9865 - loss: 0.0430 - val_accuracy: 0.9937 - val_loss: 0.0227\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9894 - loss: 0.0336 - val_accuracy: 0.9864 - val_loss: 0.0361\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9884 - loss: 0.0353 - val_accuracy: 0.9896 - val_loss: 0.0404\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9847 - loss: 0.0453 - val_accuracy: 0.9917 - val_loss: 0.0284\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000161.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.991725768321513 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 162\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B49DBB9400>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8819 - loss: 5.5888 - val_accuracy: 0.9011 - val_loss: 0.2315\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9091 - loss: 0.2134 - val_accuracy: 0.9023 - val_loss: 0.2234\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9113 - loss: 0.2147 - val_accuracy: 0.9470 - val_loss: 0.1277\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9384 - loss: 0.1457 - val_accuracy: 0.9545 - val_loss: 0.1015\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9568 - loss: 0.1046 - val_accuracy: 0.9722 - val_loss: 0.0745\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9602 - loss: 0.0969 - val_accuracy: 0.9718 - val_loss: 0.0703\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9631 - loss: 0.0878 - val_accuracy: 0.9708 - val_loss: 0.0717\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9703 - loss: 0.0810 - val_accuracy: 0.9785 - val_loss: 0.0601\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9747 - loss: 0.0707 - val_accuracy: 0.9754 - val_loss: 0.0644\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9763 - loss: 0.0628 - val_accuracy: 0.9687 - val_loss: 0.0773\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000162.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9686761229314421 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 163\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B82C2569F0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8824 - loss: 7.5033 - val_accuracy: 0.9074 - val_loss: 0.2269\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9093 - loss: 0.2192 - val_accuracy: 0.9340 - val_loss: 0.2004\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9291 - loss: 0.1783 - val_accuracy: 0.9196 - val_loss: 0.2004\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9440 - loss: 0.1430 - val_accuracy: 0.9598 - val_loss: 0.1184\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9581 - loss: 0.1113 - val_accuracy: 0.9632 - val_loss: 0.1084\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9684 - loss: 0.0865 - val_accuracy: 0.9669 - val_loss: 0.0999\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9715 - loss: 0.0790 - val_accuracy: 0.9699 - val_loss: 0.0991\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9720 - loss: 0.0728 - val_accuracy: 0.9509 - val_loss: 0.1353\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9754 - loss: 0.0702 - val_accuracy: 0.9716 - val_loss: 0.0798\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9809 - loss: 0.0547 - val_accuracy: 0.9716 - val_loss: 0.0845\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000163.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9716312056737588 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 164\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B82E4BDCD0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 75ms/step - accuracy: 0.8984 - loss: 2.5769 - val_accuracy: 0.9502 - val_loss: 0.1364\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9432 - loss: 0.1442 - val_accuracy: 0.9572 - val_loss: 0.0988\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9675 - loss: 0.0900 - val_accuracy: 0.9785 - val_loss: 0.0567\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9809 - loss: 0.0569 - val_accuracy: 0.9748 - val_loss: 0.0699\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9819 - loss: 0.0511 - val_accuracy: 0.9714 - val_loss: 0.0843\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9851 - loss: 0.0434 - val_accuracy: 0.9531 - val_loss: 0.1205\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9767 - loss: 0.0673 - val_accuracy: 0.9868 - val_loss: 0.0354\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9889 - loss: 0.0341 - val_accuracy: 0.9909 - val_loss: 0.0270\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9886 - loss: 0.0375 - val_accuracy: 0.9886 - val_loss: 0.0356\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9915 - loss: 0.0259 - val_accuracy: 0.9892 - val_loss: 0.0391\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000164.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9891646966115051 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 165\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B82E8C2C90>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.8890 - loss: 2.4879 - val_accuracy: 0.9062 - val_loss: 0.2342\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9138 - loss: 0.2113 - val_accuracy: 0.9374 - val_loss: 0.1691\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9307 - loss: 0.1648 - val_accuracy: 0.9541 - val_loss: 0.1310\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9480 - loss: 0.1331 - val_accuracy: 0.9198 - val_loss: 0.2125\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 71ms/step - accuracy: 0.9525 - loss: 0.1243 - val_accuracy: 0.9620 - val_loss: 0.1054\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9680 - loss: 0.0832 - val_accuracy: 0.9819 - val_loss: 0.0653\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9790 - loss: 0.0590 - val_accuracy: 0.9766 - val_loss: 0.0847\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9820 - loss: 0.0526 - val_accuracy: 0.9892 - val_loss: 0.0539\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9853 - loss: 0.0387 - val_accuracy: 0.9878 - val_loss: 0.0492\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9894 - loss: 0.0304 - val_accuracy: 0.9856 - val_loss: 0.0673\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000165.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.985618597320725 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 166\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B842966480>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8933 - loss: 2.9335 - val_accuracy: 0.9117 - val_loss: 0.2790\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9189 - loss: 0.2133 - val_accuracy: 0.9255 - val_loss: 0.1785\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9449 - loss: 0.1416 - val_accuracy: 0.9663 - val_loss: 0.0934\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9681 - loss: 0.0883 - val_accuracy: 0.9838 - val_loss: 0.0473\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9811 - loss: 0.0560 - val_accuracy: 0.9898 - val_loss: 0.0338\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9824 - loss: 0.0531 - val_accuracy: 0.9856 - val_loss: 0.0469\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9860 - loss: 0.0378 - val_accuracy: 0.9892 - val_loss: 0.0348\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9898 - loss: 0.0308 - val_accuracy: 0.9888 - val_loss: 0.0360\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9901 - loss: 0.0310 - val_accuracy: 0.9919 - val_loss: 0.0309\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9878 - loss: 0.0346 - val_accuracy: 0.9929 - val_loss: 0.0293\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000166.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9929078014184397 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 167\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B8429B0110>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 77ms/step - accuracy: 0.8953 - loss: 3.8377 - val_accuracy: 0.9332 - val_loss: 0.1435\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9374 - loss: 0.1497 - val_accuracy: 0.9744 - val_loss: 0.0833\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9617 - loss: 0.0912 - val_accuracy: 0.9598 - val_loss: 0.0951\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9752 - loss: 0.0693 - val_accuracy: 0.9799 - val_loss: 0.0610\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9788 - loss: 0.0599 - val_accuracy: 0.9848 - val_loss: 0.0517\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9828 - loss: 0.0506 - val_accuracy: 0.9886 - val_loss: 0.0401\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9853 - loss: 0.0432 - val_accuracy: 0.9905 - val_loss: 0.0447\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9879 - loss: 0.0382 - val_accuracy: 0.9856 - val_loss: 0.0689\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9866 - loss: 0.0389 - val_accuracy: 0.9896 - val_loss: 0.0438\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9899 - loss: 0.0287 - val_accuracy: 0.9905 - val_loss: 0.0458\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000167.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9905437352245863 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 168\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B835134B30>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 77ms/step - accuracy: 0.8771 - loss: 4.3187 - val_accuracy: 0.9147 - val_loss: 0.2114\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9047 - loss: 0.2358 - val_accuracy: 0.9220 - val_loss: 0.1894\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9097 - loss: 0.2203 - val_accuracy: 0.9255 - val_loss: 0.1825\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9131 - loss: 0.2122 - val_accuracy: 0.9151 - val_loss: 0.2099\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9275 - loss: 0.1635 - val_accuracy: 0.9606 - val_loss: 0.1021\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9439 - loss: 0.1246 - val_accuracy: 0.9586 - val_loss: 0.1030\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9554 - loss: 0.1075 - val_accuracy: 0.9710 - val_loss: 0.0747\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9651 - loss: 0.0869 - val_accuracy: 0.9726 - val_loss: 0.0724\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9708 - loss: 0.0747 - val_accuracy: 0.9785 - val_loss: 0.0606\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9775 - loss: 0.0607 - val_accuracy: 0.9809 - val_loss: 0.0520\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000168.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9808904649330181 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 169\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B831DF2BA0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 77ms/step - accuracy: 0.8813 - loss: 6.6941 - val_accuracy: 0.9072 - val_loss: 0.2040\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9092 - loss: 0.2148 - val_accuracy: 0.9578 - val_loss: 0.1066\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9555 - loss: 0.1142 - val_accuracy: 0.9813 - val_loss: 0.0632\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.9707 - loss: 0.0773 - val_accuracy: 0.9738 - val_loss: 0.0780\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9734 - loss: 0.0722 - val_accuracy: 0.9868 - val_loss: 0.0484\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9827 - loss: 0.0531 - val_accuracy: 0.9870 - val_loss: 0.0465\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9782 - loss: 0.0617 - val_accuracy: 0.9862 - val_loss: 0.0439\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9849 - loss: 0.0446 - val_accuracy: 0.9903 - val_loss: 0.0339\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9861 - loss: 0.0405 - val_accuracy: 0.9884 - val_loss: 0.0409\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9867 - loss: 0.0387 - val_accuracy: 0.9898 - val_loss: 0.0349\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000169.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9897557131599685 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 170\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3E4114050>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.8794 - loss: 3.9661 - val_accuracy: 0.9039 - val_loss: 0.2277\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9019 - loss: 0.2321 - val_accuracy: 0.9127 - val_loss: 0.2056\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9092 - loss: 0.2215 - val_accuracy: 0.9228 - val_loss: 0.1873\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9160 - loss: 0.2015 - val_accuracy: 0.9356 - val_loss: 0.1338\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9167 - loss: 0.2301 - val_accuracy: 0.9108 - val_loss: 0.2420\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9103 - loss: 0.2243 - val_accuracy: 0.9348 - val_loss: 0.1753\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9339 - loss: 0.1660 - val_accuracy: 0.9478 - val_loss: 0.1403\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9476 - loss: 0.1370 - val_accuracy: 0.9444 - val_loss: 0.1440\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9595 - loss: 0.1035 - val_accuracy: 0.9569 - val_loss: 0.1127\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9735 - loss: 0.0731 - val_accuracy: 0.9687 - val_loss: 0.0913\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000170.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9686761229314421 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 171\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B45D8C79E0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.8921 - loss: 1.1390 - val_accuracy: 0.8907 - val_loss: 0.2393\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9008 - loss: 0.2386 - val_accuracy: 0.8964 - val_loss: 0.2189\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9127 - loss: 0.2016 - val_accuracy: 0.9113 - val_loss: 0.1902\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9220 - loss: 0.1936 - val_accuracy: 0.9271 - val_loss: 0.1872\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9383 - loss: 0.1577 - val_accuracy: 0.9212 - val_loss: 0.1678\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9473 - loss: 0.1405 - val_accuracy: 0.9525 - val_loss: 0.1225\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9628 - loss: 0.1012 - val_accuracy: 0.9647 - val_loss: 0.0947\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9683 - loss: 0.0867 - val_accuracy: 0.9708 - val_loss: 0.0858\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9767 - loss: 0.0631 - val_accuracy: 0.9393 - val_loss: 0.1499\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9820 - loss: 0.0467 - val_accuracy: 0.9838 - val_loss: 0.0511\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000171.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9838455476753349 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 172\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B4746BB770>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.8852 - loss: 1.9278 - val_accuracy: 0.9186 - val_loss: 0.1936\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9150 - loss: 0.2122 - val_accuracy: 0.9226 - val_loss: 0.1774\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9322 - loss: 0.1727 - val_accuracy: 0.9716 - val_loss: 0.0868\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9650 - loss: 0.0880 - val_accuracy: 0.9892 - val_loss: 0.0400\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9748 - loss: 0.0712 - val_accuracy: 0.9864 - val_loss: 0.0480\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9758 - loss: 0.0676 - val_accuracy: 0.9872 - val_loss: 0.0445\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9852 - loss: 0.0441 - val_accuracy: 0.9913 - val_loss: 0.0338\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9892 - loss: 0.0322 - val_accuracy: 0.9939 - val_loss: 0.0251\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9890 - loss: 0.0320 - val_accuracy: 0.9829 - val_loss: 0.0561\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9892 - loss: 0.0318 - val_accuracy: 0.9915 - val_loss: 0.0333\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000172.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9915287628053585 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 173\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B82A0CA8A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8890 - loss: 3.6077 - val_accuracy: 0.8976 - val_loss: 0.2429\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9000 - loss: 0.2414 - val_accuracy: 0.9088 - val_loss: 0.2127\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9105 - loss: 0.2138 - val_accuracy: 0.9196 - val_loss: 0.1997\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9193 - loss: 0.1900 - val_accuracy: 0.9273 - val_loss: 0.1788\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9317 - loss: 0.1675 - val_accuracy: 0.9263 - val_loss: 0.1848\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9398 - loss: 0.1503 - val_accuracy: 0.9348 - val_loss: 0.1719\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9428 - loss: 0.1355 - val_accuracy: 0.9379 - val_loss: 0.1552\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9508 - loss: 0.1203 - val_accuracy: 0.9456 - val_loss: 0.1405\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9580 - loss: 0.1040 - val_accuracy: 0.9454 - val_loss: 0.1525\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9601 - loss: 0.0998 - val_accuracy: 0.9486 - val_loss: 0.1348\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000173.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9485815602836879 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 174\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B49D723770>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8864 - loss: 2.0070 - val_accuracy: 0.8993 - val_loss: 0.2206\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9228 - loss: 0.1921 - val_accuracy: 0.9592 - val_loss: 0.1109\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9634 - loss: 0.0940 - val_accuracy: 0.9545 - val_loss: 0.1064\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9786 - loss: 0.0657 - val_accuracy: 0.9860 - val_loss: 0.0445\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9851 - loss: 0.0408 - val_accuracy: 0.9864 - val_loss: 0.0438\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9845 - loss: 0.0438 - val_accuracy: 0.9923 - val_loss: 0.0366\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9918 - loss: 0.0281 - val_accuracy: 0.9921 - val_loss: 0.0281\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9906 - loss: 0.0261 - val_accuracy: 0.9699 - val_loss: 0.0914\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9899 - loss: 0.0300 - val_accuracy: 0.9854 - val_loss: 0.0521\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9911 - loss: 0.0274 - val_accuracy: 0.9852 - val_loss: 0.0476\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000174.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.985224586288416 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 175\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B36D8EE420>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8862 - loss: 2.6205 - val_accuracy: 0.8942 - val_loss: 0.1936\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9157 - loss: 0.1665 - val_accuracy: 0.9655 - val_loss: 0.1102\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9575 - loss: 0.1258 - val_accuracy: 0.9795 - val_loss: 0.0783\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 68ms/step - accuracy: 0.9669 - loss: 0.0904 - val_accuracy: 0.9815 - val_loss: 0.0782\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.9728 - loss: 0.0783 - val_accuracy: 0.9858 - val_loss: 0.0602\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9811 - loss: 0.0628 - val_accuracy: 0.9892 - val_loss: 0.0484\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9830 - loss: 0.0569 - val_accuracy: 0.9892 - val_loss: 0.0513\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.9838 - loss: 0.0501 - val_accuracy: 0.9825 - val_loss: 0.0663\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 70ms/step - accuracy: 0.9829 - loss: 0.0522 - val_accuracy: 0.9758 - val_loss: 0.0982\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9835 - loss: 0.0562 - val_accuracy: 0.9909 - val_loss: 0.0467\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000175.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9909377462568952 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 176\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B832EF2630>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.8831 - loss: 2.5316 - val_accuracy: 0.9255 - val_loss: 0.1845\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9235 - loss: 0.1929 - val_accuracy: 0.9362 - val_loss: 0.1704\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9436 - loss: 0.1416 - val_accuracy: 0.9720 - val_loss: 0.0765\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9668 - loss: 0.0874 - val_accuracy: 0.9762 - val_loss: 0.0654\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9813 - loss: 0.0561 - val_accuracy: 0.9811 - val_loss: 0.0533\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9845 - loss: 0.0470 - val_accuracy: 0.9854 - val_loss: 0.0376\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9871 - loss: 0.0362 - val_accuracy: 0.9860 - val_loss: 0.0401\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9888 - loss: 0.0298 - val_accuracy: 0.9913 - val_loss: 0.0237\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9925 - loss: 0.0225 - val_accuracy: 0.9898 - val_loss: 0.0314\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9935 - loss: 0.0216 - val_accuracy: 0.9896 - val_loss: 0.0303\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000176.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.989558707643814 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 177\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B46AFE9940>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 70ms/step - accuracy: 0.8832 - loss: 6.8129 - val_accuracy: 0.9019 - val_loss: 0.2433\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9046 - loss: 0.2309 - val_accuracy: 0.9023 - val_loss: 0.2187\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9193 - loss: 0.1939 - val_accuracy: 0.9320 - val_loss: 0.1728\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9404 - loss: 0.1451 - val_accuracy: 0.9506 - val_loss: 0.1304\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9487 - loss: 0.1350 - val_accuracy: 0.9673 - val_loss: 0.0969\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9676 - loss: 0.0874 - val_accuracy: 0.9655 - val_loss: 0.1083\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9690 - loss: 0.0812 - val_accuracy: 0.9697 - val_loss: 0.0816\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9746 - loss: 0.0637 - val_accuracy: 0.9330 - val_loss: 0.1589\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9761 - loss: 0.0656 - val_accuracy: 0.9667 - val_loss: 0.1125\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9790 - loss: 0.0550 - val_accuracy: 0.9699 - val_loss: 0.0906\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000177.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9698581560283688 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 178\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B461D47440>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 71ms/step - accuracy: 0.8837 - loss: 2.8429 - val_accuracy: 0.9145 - val_loss: 0.1875\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9190 - loss: 0.2006 - val_accuracy: 0.9226 - val_loss: 0.1917\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9239 - loss: 0.1953 - val_accuracy: 0.9472 - val_loss: 0.1290\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9473 - loss: 0.1370 - val_accuracy: 0.9517 - val_loss: 0.1313\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9635 - loss: 0.0996 - val_accuracy: 0.9758 - val_loss: 0.0703\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9704 - loss: 0.0821 - val_accuracy: 0.9728 - val_loss: 0.0724\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9817 - loss: 0.0553 - val_accuracy: 0.9923 - val_loss: 0.0240\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9875 - loss: 0.0399 - val_accuracy: 0.9789 - val_loss: 0.0615\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9858 - loss: 0.0472 - val_accuracy: 0.9903 - val_loss: 0.0247\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9864 - loss: 0.0412 - val_accuracy: 0.9911 - val_loss: 0.0222\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000178.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9911347517730497 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 179\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B844CCD670>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8854 - loss: 2.0476 - val_accuracy: 0.9275 - val_loss: 0.1887\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9201 - loss: 0.1993 - val_accuracy: 0.9533 - val_loss: 0.1234\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9398 - loss: 0.1509 - val_accuracy: 0.9527 - val_loss: 0.1317\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9526 - loss: 0.1144 - val_accuracy: 0.9795 - val_loss: 0.0602\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9722 - loss: 0.0727 - val_accuracy: 0.9799 - val_loss: 0.0628\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9805 - loss: 0.0501 - val_accuracy: 0.9892 - val_loss: 0.0411\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9859 - loss: 0.0369 - val_accuracy: 0.9819 - val_loss: 0.0577\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9876 - loss: 0.0398 - val_accuracy: 0.9785 - val_loss: 0.0691\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9908 - loss: 0.0288 - val_accuracy: 0.9894 - val_loss: 0.0368\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9911 - loss: 0.0293 - val_accuracy: 0.9907 - val_loss: 0.0332\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000179.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9907407407407407 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 180\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B346016B70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8811 - loss: 5.5228 - val_accuracy: 0.9240 - val_loss: 0.1746\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9150 - loss: 0.2012 - val_accuracy: 0.9098 - val_loss: 0.2196\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9063 - loss: 0.2270 - val_accuracy: 0.9206 - val_loss: 0.1929\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9212 - loss: 0.1909 - val_accuracy: 0.9232 - val_loss: 0.1889\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9230 - loss: 0.1770 - val_accuracy: 0.9543 - val_loss: 0.1338\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9421 - loss: 0.1469 - val_accuracy: 0.9647 - val_loss: 0.1052\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9634 - loss: 0.0894 - val_accuracy: 0.9425 - val_loss: 0.9128\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9722 - loss: 0.0769 - val_accuracy: 0.9594 - val_loss: 0.1131\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9800 - loss: 0.0555 - val_accuracy: 0.9884 - val_loss: 0.0573\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9829 - loss: 0.0485 - val_accuracy: 0.9876 - val_loss: 0.0554\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000180.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9875886524822695 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 181\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B842BDCD70>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.8974 - loss: 2.1840 - val_accuracy: 0.9009 - val_loss: 0.3140\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.8976 - loss: 0.2885 - val_accuracy: 0.9066 - val_loss: 0.2435\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9046 - loss: 0.2339 - val_accuracy: 0.9123 - val_loss: 0.2454\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9193 - loss: 0.2024 - val_accuracy: 0.9372 - val_loss: 0.1700\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9463 - loss: 0.1453 - val_accuracy: 0.9494 - val_loss: 0.1413\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9584 - loss: 0.1122 - val_accuracy: 0.9746 - val_loss: 0.0830\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9675 - loss: 0.0895 - val_accuracy: 0.9787 - val_loss: 0.0714\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9780 - loss: 0.0655 - val_accuracy: 0.9653 - val_loss: 0.1246\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9856 - loss: 0.0405 - val_accuracy: 0.9771 - val_loss: 0.0788\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9878 - loss: 0.0360 - val_accuracy: 0.9856 - val_loss: 0.0600\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000181.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.985618597320725 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 182\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B84974D130>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8809 - loss: 5.3313 - val_accuracy: 0.9045 - val_loss: 0.1828\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9077 - loss: 0.2404 - val_accuracy: 0.9064 - val_loss: 0.2430\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9141 - loss: 0.2169 - val_accuracy: 0.9259 - val_loss: 0.1965\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9293 - loss: 0.1871 - val_accuracy: 0.9064 - val_loss: 0.1995\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9219 - loss: 0.1754 - val_accuracy: 0.9423 - val_loss: 0.1391\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9464 - loss: 0.1294 - val_accuracy: 0.9687 - val_loss: 0.0802\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9623 - loss: 0.0878 - val_accuracy: 0.9639 - val_loss: 0.0866\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9657 - loss: 0.0800 - val_accuracy: 0.9809 - val_loss: 0.0545\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9725 - loss: 0.0611 - val_accuracy: 0.9762 - val_loss: 0.0631\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9820 - loss: 0.0470 - val_accuracy: 0.9836 - val_loss: 0.0432\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000182.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9836485421591804 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 183\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B84B8DB500>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.8866 - loss: 2.2464 - val_accuracy: 0.9102 - val_loss: 0.2054\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9183 - loss: 0.1950 - val_accuracy: 0.9492 - val_loss: 0.1270\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9357 - loss: 0.1618 - val_accuracy: 0.9701 - val_loss: 0.0797\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9567 - loss: 0.1090 - val_accuracy: 0.9783 - val_loss: 0.0599\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9713 - loss: 0.0773 - val_accuracy: 0.9793 - val_loss: 0.0590\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9735 - loss: 0.0732 - val_accuracy: 0.9539 - val_loss: 0.1308\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9732 - loss: 0.0696 - val_accuracy: 0.9807 - val_loss: 0.0519\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9819 - loss: 0.0490 - val_accuracy: 0.9838 - val_loss: 0.0448\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9827 - loss: 0.0442 - val_accuracy: 0.9856 - val_loss: 0.0404\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9847 - loss: 0.0425 - val_accuracy: 0.9876 - val_loss: 0.0342\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000183.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9875886524822695 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 184\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B3461B34A0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8831 - loss: 2.6483 - val_accuracy: 0.9035 - val_loss: 0.2522\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9085 - loss: 0.2248 - val_accuracy: 0.9218 - val_loss: 0.1878\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9207 - loss: 0.1859 - val_accuracy: 0.9417 - val_loss: 0.1467\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9354 - loss: 0.1595 - val_accuracy: 0.9553 - val_loss: 0.1127\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9561 - loss: 0.1060 - val_accuracy: 0.9732 - val_loss: 0.0718\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9692 - loss: 0.0737 - val_accuracy: 0.9805 - val_loss: 0.0638\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9780 - loss: 0.0580 - val_accuracy: 0.9829 - val_loss: 0.0474\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9818 - loss: 0.0507 - val_accuracy: 0.9675 - val_loss: 0.1185\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9844 - loss: 0.0441 - val_accuracy: 0.9758 - val_loss: 0.0649\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9849 - loss: 0.0417 - val_accuracy: 0.9876 - val_loss: 0.0371\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000184.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9875886524822695 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 185\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B852210380>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8906 - loss: 2.0066 - val_accuracy: 0.9139 - val_loss: 0.1888\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9399 - loss: 0.1488 - val_accuracy: 0.9770 - val_loss: 0.0732\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9764 - loss: 0.0663 - val_accuracy: 0.9732 - val_loss: 0.0762\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9808 - loss: 0.0542 - val_accuracy: 0.9777 - val_loss: 0.0680\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9758 - loss: 0.0669 - val_accuracy: 0.9890 - val_loss: 0.0412\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9896 - loss: 0.0330 - val_accuracy: 0.9825 - val_loss: 0.0687\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9897 - loss: 0.0272 - val_accuracy: 0.9917 - val_loss: 0.0323\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 71ms/step - accuracy: 0.9911 - loss: 0.0281 - val_accuracy: 0.9898 - val_loss: 0.0466\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9854 - loss: 0.0436 - val_accuracy: 0.9565 - val_loss: 0.2614\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9836 - loss: 0.0568 - val_accuracy: 0.9931 - val_loss: 0.0318\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000185.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9931048069345941 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 186\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B85119AD20>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.8872 - loss: 1.7234 - val_accuracy: 0.9689 - val_loss: 0.1054\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9328 - loss: 0.1350 - val_accuracy: 0.9823 - val_loss: 0.0580\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9675 - loss: 0.0927 - val_accuracy: 0.9892 - val_loss: 0.0376\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9778 - loss: 0.0714 - val_accuracy: 0.9923 - val_loss: 0.0316\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9853 - loss: 0.0464 - val_accuracy: 0.9890 - val_loss: 0.0340\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9830 - loss: 0.0524 - val_accuracy: 0.9945 - val_loss: 0.0230\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9828 - loss: 0.0513 - val_accuracy: 0.9939 - val_loss: 0.0241\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9867 - loss: 0.0400 - val_accuracy: 0.9901 - val_loss: 0.0391\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9861 - loss: 0.0433 - val_accuracy: 0.9931 - val_loss: 0.0305\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9894 - loss: 0.0307 - val_accuracy: 0.9957 - val_loss: 0.0224\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000186.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9956658786446021 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 187\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B8500E1C40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8900 - loss: 4.2828 - val_accuracy: 0.9381 - val_loss: 0.1368\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9522 - loss: 0.1208 - val_accuracy: 0.9718 - val_loss: 0.0779\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9782 - loss: 0.0616 - val_accuracy: 0.9884 - val_loss: 0.0368\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9803 - loss: 0.0509 - val_accuracy: 0.9892 - val_loss: 0.0378\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9811 - loss: 0.0485 - val_accuracy: 0.9913 - val_loss: 0.0280\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9854 - loss: 0.0379 - val_accuracy: 0.9884 - val_loss: 0.0419\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9854 - loss: 0.0391 - val_accuracy: 0.9740 - val_loss: 0.0710\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9870 - loss: 0.0407 - val_accuracy: 0.9921 - val_loss: 0.0268\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9846 - loss: 0.0461 - val_accuracy: 0.9929 - val_loss: 0.0270\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9904 - loss: 0.0247 - val_accuracy: 0.9890 - val_loss: 0.0378\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000187.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9889676910953507 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 188\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B858A78680>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 76ms/step - accuracy: 0.8845 - loss: 3.7423 - val_accuracy: 0.9173 - val_loss: 0.2176\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9054 - loss: 0.2336 - val_accuracy: 0.9023 - val_loss: 0.3169\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.8958 - loss: 0.3303 - val_accuracy: 0.9023 - val_loss: 0.3140\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 71ms/step - accuracy: 0.8994 - loss: 0.3222 - val_accuracy: 0.9023 - val_loss: 0.3158\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.8967 - loss: 0.3304 - val_accuracy: 0.9023 - val_loss: 0.3139\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.8975 - loss: 0.3273 - val_accuracy: 0.9023 - val_loss: 0.3137\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.8977 - loss: 0.3250 - val_accuracy: 0.9023 - val_loss: 0.3132\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.8968 - loss: 0.3254 - val_accuracy: 0.9023 - val_loss: 0.3136\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.8992 - loss: 0.3221 - val_accuracy: 0.9023 - val_loss: 0.3134\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8960 - loss: 0.3291 - val_accuracy: 0.9023 - val_loss: 0.3149\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000188.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9022852639873916 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 189\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B8589AF350>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8860 - loss: 4.9467 - val_accuracy: 0.8964 - val_loss: 0.3267\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8979 - loss: 0.3275 - val_accuracy: 0.8964 - val_loss: 0.3256\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8946 - loss: 0.3324 - val_accuracy: 0.8964 - val_loss: 0.3277\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8952 - loss: 0.3318 - val_accuracy: 0.8964 - val_loss: 0.3254\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8962 - loss: 0.3319 - val_accuracy: 0.8964 - val_loss: 0.3260\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.8987 - loss: 0.3237 - val_accuracy: 0.8964 - val_loss: 0.3274\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9008 - loss: 0.3180 - val_accuracy: 0.8964 - val_loss: 0.3251\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.8966 - loss: 0.3285 - val_accuracy: 0.8964 - val_loss: 0.3258\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8914 - loss: 0.3387 - val_accuracy: 0.8964 - val_loss: 0.3259\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8994 - loss: 0.3217 - val_accuracy: 0.8964 - val_loss: 0.3263\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000189.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8963750985027581 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 190\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B85AE2F3B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8847 - loss: 2.9148 - val_accuracy: 0.9090 - val_loss: 0.2213\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9105 - loss: 0.2206 - val_accuracy: 0.9267 - val_loss: 0.1701\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9329 - loss: 0.1629 - val_accuracy: 0.9582 - val_loss: 0.1043\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9625 - loss: 0.0953 - val_accuracy: 0.9614 - val_loss: 0.1115\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9753 - loss: 0.0660 - val_accuracy: 0.9827 - val_loss: 0.0460\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9805 - loss: 0.0507 - val_accuracy: 0.9821 - val_loss: 0.0561\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9847 - loss: 0.0417 - val_accuracy: 0.9901 - val_loss: 0.0318\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9889 - loss: 0.0282 - val_accuracy: 0.9856 - val_loss: 0.0432\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9923 - loss: 0.0251 - val_accuracy: 0.9807 - val_loss: 0.0564\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9905 - loss: 0.0277 - val_accuracy: 0.9860 - val_loss: 0.0480\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000190.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9860126083530338 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 191\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B85D0EEB40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.8833 - loss: 4.6728 - val_accuracy: 0.9058 - val_loss: 0.2279\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 71ms/step - accuracy: 0.9100 - loss: 0.2432 - val_accuracy: 0.9165 - val_loss: 0.1966\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9163 - loss: 0.2016 - val_accuracy: 0.9269 - val_loss: 0.2155\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9288 - loss: 0.1667 - val_accuracy: 0.9415 - val_loss: 0.1639\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.9342 - loss: 0.1506 - val_accuracy: 0.9427 - val_loss: 0.1519\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.9385 - loss: 0.1442 - val_accuracy: 0.9413 - val_loss: 0.1556\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9551 - loss: 0.1134 - val_accuracy: 0.9758 - val_loss: 0.0807\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9697 - loss: 0.0771 - val_accuracy: 0.9827 - val_loss: 0.0672\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9744 - loss: 0.0665 - val_accuracy: 0.9819 - val_loss: 0.0560\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9717 - loss: 0.0758 - val_accuracy: 0.9838 - val_loss: 0.0547\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000191.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9838455476753349 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 192\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B85AE898B0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8899 - loss: 3.6717 - val_accuracy: 0.9035 - val_loss: 0.2232\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9131 - loss: 0.2047 - val_accuracy: 0.9279 - val_loss: 0.1879\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9368 - loss: 0.1548 - val_accuracy: 0.9580 - val_loss: 0.1122\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9570 - loss: 0.1207 - val_accuracy: 0.9679 - val_loss: 0.0838\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9630 - loss: 0.0957 - val_accuracy: 0.9687 - val_loss: 0.0990\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9760 - loss: 0.0699 - val_accuracy: 0.9797 - val_loss: 0.0654\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9856 - loss: 0.0432 - val_accuracy: 0.9732 - val_loss: 0.0768\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9792 - loss: 0.0569 - val_accuracy: 0.9917 - val_loss: 0.0375\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9872 - loss: 0.0368 - val_accuracy: 0.9742 - val_loss: 0.0788\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9876 - loss: 0.0354 - val_accuracy: 0.9890 - val_loss: 0.0419\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000192.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9889676910953507 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 193\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B864889C40>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.8860 - loss: 1.5660 - val_accuracy: 0.9198 - val_loss: 0.2432\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9272 - loss: 0.1760 - val_accuracy: 0.9691 - val_loss: 0.0859\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9597 - loss: 0.1079 - val_accuracy: 0.9858 - val_loss: 0.0485\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9739 - loss: 0.0702 - val_accuracy: 0.9901 - val_loss: 0.0290\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9840 - loss: 0.0521 - val_accuracy: 0.9825 - val_loss: 0.0491\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9845 - loss: 0.0462 - val_accuracy: 0.9917 - val_loss: 0.0275\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9899 - loss: 0.0304 - val_accuracy: 0.9945 - val_loss: 0.0221\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9933 - loss: 0.0218 - val_accuracy: 0.9795 - val_loss: 0.0651\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 70ms/step - accuracy: 0.9906 - loss: 0.0259 - val_accuracy: 0.9923 - val_loss: 0.0291\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.9897 - loss: 0.0320 - val_accuracy: 0.9886 - val_loss: 0.0333\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000193.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9885736800630418 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 194\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B85F262390>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.8957 - loss: 1.8872 - val_accuracy: 0.9208 - val_loss: 0.1828\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9476 - loss: 0.1348 - val_accuracy: 0.9758 - val_loss: 0.0725\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 73ms/step - accuracy: 0.9658 - loss: 0.0884 - val_accuracy: 0.9777 - val_loss: 0.0737\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9768 - loss: 0.0659 - val_accuracy: 0.9836 - val_loss: 0.0563\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9822 - loss: 0.0538 - val_accuracy: 0.9799 - val_loss: 0.0600\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9822 - loss: 0.0528 - val_accuracy: 0.9848 - val_loss: 0.0530\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9847 - loss: 0.0482 - val_accuracy: 0.9864 - val_loss: 0.0423\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9899 - loss: 0.0303 - val_accuracy: 0.9752 - val_loss: 0.0744\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 77ms/step - accuracy: 0.9902 - loss: 0.0275 - val_accuracy: 0.9878 - val_loss: 0.0368\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9927 - loss: 0.0239 - val_accuracy: 0.9878 - val_loss: 0.0412\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000194.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.987785657998424 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 195\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B866E60680>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 73ms/step - accuracy: 0.8870 - loss: 5.4419 - val_accuracy: 0.8980 - val_loss: 0.2296\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9029 - loss: 0.2189 - val_accuracy: 0.9035 - val_loss: 0.2098\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9175 - loss: 0.1962 - val_accuracy: 0.9005 - val_loss: 0.2413\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9159 - loss: 0.2087 - val_accuracy: 0.9267 - val_loss: 0.1682\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9284 - loss: 0.1599 - val_accuracy: 0.9318 - val_loss: 0.1574\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9459 - loss: 0.1303 - val_accuracy: 0.9529 - val_loss: 0.1298\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9521 - loss: 0.1232 - val_accuracy: 0.9649 - val_loss: 0.1019\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9673 - loss: 0.0862 - val_accuracy: 0.9742 - val_loss: 0.0819\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9680 - loss: 0.0869 - val_accuracy: 0.8970 - val_loss: 0.3240\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.8967 - loss: 0.3356 - val_accuracy: 0.8970 - val_loss: 0.3248\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000195.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.8969661150512215 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 196\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B866F67470>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8731 - loss: 8.6667 - val_accuracy: 0.9019 - val_loss: 0.2413\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9046 - loss: 0.2516 - val_accuracy: 0.9068 - val_loss: 0.2313\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9109 - loss: 0.2228 - val_accuracy: 0.9090 - val_loss: 0.2174\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9090 - loss: 0.2214 - val_accuracy: 0.9131 - val_loss: 0.2164\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9157 - loss: 0.2095 - val_accuracy: 0.9240 - val_loss: 0.2004\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9275 - loss: 0.1779 - val_accuracy: 0.9216 - val_loss: 0.1965\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9326 - loss: 0.1679 - val_accuracy: 0.9431 - val_loss: 0.1405\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9450 - loss: 0.1349 - val_accuracy: 0.9531 - val_loss: 0.1215\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9534 - loss: 0.1125 - val_accuracy: 0.9665 - val_loss: 0.0954\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9637 - loss: 0.0927 - val_accuracy: 0.9695 - val_loss: 0.0831\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000196.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9694641449960599 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 197\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B867314E60>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8841 - loss: 4.3947 - val_accuracy: 0.9407 - val_loss: 0.1487\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9418 - loss: 0.1318 - val_accuracy: 0.9779 - val_loss: 0.0697\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9694 - loss: 0.0857 - val_accuracy: 0.9825 - val_loss: 0.0566\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9767 - loss: 0.0689 - val_accuracy: 0.9858 - val_loss: 0.0534\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9619 - loss: 0.1028 - val_accuracy: 0.9781 - val_loss: 0.0698\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9829 - loss: 0.0537 - val_accuracy: 0.9919 - val_loss: 0.0401\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9853 - loss: 0.0482 - val_accuracy: 0.9630 - val_loss: 0.0961\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9775 - loss: 0.0640 - val_accuracy: 0.9943 - val_loss: 0.0305\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.9882 - loss: 0.0377 - val_accuracy: 0.9878 - val_loss: 0.0445\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.9866 - loss: 0.0436 - val_accuracy: 0.9921 - val_loss: 0.0414\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000197.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9921197793538219 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 198\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B86F68CEF0>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8873 - loss: 3.6989 - val_accuracy: 0.9370 - val_loss: 0.1709\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9238 - loss: 0.1849 - val_accuracy: 0.9429 - val_loss: 0.1473\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9469 - loss: 0.1281 - val_accuracy: 0.9576 - val_loss: 0.1255\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9660 - loss: 0.0930 - val_accuracy: 0.9736 - val_loss: 0.0804\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9752 - loss: 0.0710 - val_accuracy: 0.9779 - val_loss: 0.0687\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9754 - loss: 0.0640 - val_accuracy: 0.9728 - val_loss: 0.0832\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9765 - loss: 0.0658 - val_accuracy: 0.9773 - val_loss: 0.0760\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9855 - loss: 0.0415 - val_accuracy: 0.9681 - val_loss: 0.0899\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9802 - loss: 0.0575 - val_accuracy: 0.9840 - val_loss: 0.0536\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9873 - loss: 0.0384 - val_accuracy: 0.9655 - val_loss: 0.0932\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000198.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9655240346729709 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 199\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B8718BD130>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8839 - loss: 7.2731 - val_accuracy: 0.9048 - val_loss: 0.2190\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9054 - loss: 0.2249 - val_accuracy: 0.9121 - val_loss: 0.2097\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9115 - loss: 0.2017 - val_accuracy: 0.9383 - val_loss: 0.1370\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9245 - loss: 0.1832 - val_accuracy: 0.9362 - val_loss: 0.1560\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9434 - loss: 0.1227 - val_accuracy: 0.9661 - val_loss: 0.0822\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9607 - loss: 0.0918 - val_accuracy: 0.9704 - val_loss: 0.0806\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9654 - loss: 0.0912 - val_accuracy: 0.9728 - val_loss: 0.0692\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9688 - loss: 0.0765 - val_accuracy: 0.9827 - val_loss: 0.0486\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.9749 - loss: 0.0676 - val_accuracy: 0.9801 - val_loss: 0.0493\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.9771 - loss: 0.0590 - val_accuracy: 0.9844 - val_loss: 0.0445\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000199.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9844365642237982 - Elements: 5076\n",
      "Selecting training and test data - traininSplitRandomState: 200\n",
      "Training using 20304 files.\n",
      "    def buildModel(self) -> Model:\n",
      "        print(f\"__job__: {self.__job__}\")\n",
      "        input_shape = (self.__job__.numMels, self.width, self.channels)\n",
      "        model_input = Input(shape = input_shape)\n",
      "\n",
      "        # Derived from:\n",
      "        # Anagha, R., Arya, A., Narayan, V. H., Abhishek, S., & Anjali, T. (2023).\n",
      "        #   Audio Deepfake Detection Using Deep Learning.\n",
      "        #   2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART), System Modeling & Advancement in Research Trends (SMART), 2023 12th International Conference On, 176–181.\n",
      "        #   https://doi.org/10.1109/SMART59791.2023.10428163\n",
      "        x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(model_input)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
      "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "        x = Flatten()(x)\n",
      "        x = Dense(units=128, activation='relu')(x)\n",
      "        x = Dropout(0.5)(x)\n",
      "\n",
      "        model_output = Dense(self.__job__.numClasses, activation='softmax')(x)\n",
      "\n",
      "        return Model(inputs=model_input, outputs=model_output)\n",
      "\n",
      "Model definition:\n",
      "None\n",
      "__job__: <configuration.configuration.Job object at 0x000001B872E57410>\n",
      "Training the Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.8898 - loss: 1.5547 - val_accuracy: 0.9139 - val_loss: 0.2218\n",
      "Epoch 2/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.9197 - loss: 0.2048 - val_accuracy: 0.9379 - val_loss: 0.1524\n",
      "Epoch 3/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 85ms/step - accuracy: 0.9457 - loss: 0.1397 - val_accuracy: 0.9441 - val_loss: 0.1398\n",
      "Epoch 4/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 81ms/step - accuracy: 0.9552 - loss: 0.1067 - val_accuracy: 0.9295 - val_loss: 0.1751\n",
      "Epoch 5/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 80ms/step - accuracy: 0.9645 - loss: 0.0915 - val_accuracy: 0.9616 - val_loss: 0.1217\n",
      "Epoch 6/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9727 - loss: 0.0752 - val_accuracy: 0.9775 - val_loss: 0.0679\n",
      "Epoch 7/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.9802 - loss: 0.0550 - val_accuracy: 0.9703 - val_loss: 0.0821\n",
      "Epoch 8/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 78ms/step - accuracy: 0.9818 - loss: 0.0499 - val_accuracy: 0.9730 - val_loss: 0.0768\n",
      "Epoch 9/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 81ms/step - accuracy: 0.9850 - loss: 0.0462 - val_accuracy: 0.9728 - val_loss: 0.0800\n",
      "Epoch 10/10\n",
      "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 79ms/step - accuracy: 0.9703 - loss: 0.0989 - val_accuracy: 0.9799 - val_loss: 0.0713\n",
      "Saving model: output/ASVspoof-2019_training_2025-03-25T07-47-11.409349_random_000200.libjob\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "  Batches: 1 - Files: 5076 - Score: 0.9799054373522459 - Elements: 5076\n"
     ]
    }
   ],
   "source": [
    "bulkTrainingProc = BulkModelTrainingProcessor(job,\n",
    "                                              model_cnn_definition.ModelCnnDefinition,\n",
    "                                              BasicModelTrainingProcessor,\n",
    "                                              BasicModelEvaluationProcessor)\n",
    "\n",
    "bulkTrainingProc.process(random_state_lowValue, random_state_highValue, X, y_encoded, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-deepfake-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
